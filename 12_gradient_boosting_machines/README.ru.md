# Усиление вашей торговой стратегии с помощью градиентного бустинга

Эта глава исследует бустинг — ещё один алгоритм ансамблевого обучения, обычно основанный на деревьях решений, который часто даёт даже лучшие результаты, чем [случайные леса](../10_decision_trees_random_forests).

Ключевое отличие заключается в том, что бустинг в своей оригинальной версии AdaBoost модифицирует обучающие данные для каждого дерева на основе накопленных ошибок, сделанных моделью перед добавлением нового дерева. Случайные леса, напротив, используют бэггинг для независимого обучения множества деревьев с использованием рандомизированных версий обучающей выборки. В то время как случайные леса могут обучаться параллельно, бустинг работает последовательно, используя перевзвешенные версии данных. Современные реализации бустинга также применяют стратегии рандомизации случайных лесов.

В этой главе мы увидим, как бустинг эволюционировал в один из самых успешных алгоритмов машинного обучения за последние три десятилетия. На момент написания он стал доминирующим в соревнованиях по машинному обучению для структурированных данных (в отличие от многомерных изображений или речи, например, где связь между входными и выходными данными более сложная, и глубокое обучение превосходит). В частности, в этой главе мы рассмотрим следующие темы:

## Содержание

1. [Начало работы: адаптивный бустинг](#начало-работы-адаптивный-бустинг)
    * [Алгоритм AdaBoost](#алгоритм-adaboost)
    * [Пример кода: AdaBoost с sklearn](#пример-кода-adaboost-с-sklearn)
2. [Градиентный бустинг — ансамбли для большинства задач](#градиентный-бустинг--ансамбли-для-большинства-задач)
    * [Как обучать и настраивать GBM модели](#как-обучать-и-настраивать-gbm-модели)
    * [Пример кода: градиентный бустинг с scikit-learn](#пример-кода-градиентный-бустинг-с-scikit-learn)
3. [Использование XGBoost, LightGBM и CatBoost](#использование-xgboost-lightgbm-и-catboost)
4. [Пример кода: торговая стратегия long-short с градиентным бустингом](#пример-кода-торговая-стратегия-long-short-с-градиентным-бустингом)
    * [Подготовка данных](#подготовка-данных)
    * [Как генерировать сигналы с моделями LightGBM и CatBoost](#как-генерировать-сигналы-с-моделями-lightgbm-и-catboost)
    * [Оценка торговых сигналов](#оценка-торговых-сигналов)
    * [Создание прогнозов вне выборки](#создание-прогнозов-вне-выборки)
    * [Определение и бэктестинг стратегии long-short](#определение-и-бэктестинг-стратегии-long-short)
5. [Заглянуть в чёрный ящик: как интерпретировать результаты GBM](#заглянуть-в-чёрный-ящик-как-интерпретировать-результаты-gbm)
    * [Пример кода: определение важности признаков с LightGBM](#пример-кода-определение-важности-признаков-с-lightgbm)
        - [Важность признаков](#важность-признаков)
        - [Графики частичной зависимости](#графики-частичной-зависимости)
        - [SHAP-значения (SHapley Additive exPlanations)](#shap-значения-shapley-additive-explanations)
6. [Внутридневная стратегия с Algoseek и LightGBM](#внутридневная-стратегия-с-algoseek-и-lightgbm)
    * [Пример кода: создание внутридневных признаков](#пример-кода-создание-внутридневных-признаков)
    * [Пример кода: настройка модели LightGBM и оценка прогнозов](#пример-кода-настройка-модели-lightgbm-и-оценка-прогнозов)
7. [Ресурсы](#ресурсы)
    * [XGBoost](#xgboost)
    * [LightGBM](#lightgbm)
    * [CatBoost](#catboost)


## Начало работы: адаптивный бустинг

Как и бэггинг, бустинг объединяет базовые модели в ансамбль. Бустинг изначально был разработан для задач классификации, но также может использоваться для регрессии и был назван одной из самых мощных идей обучения, представленных за последние 20 лет (как описано в книге [Elements of Statistical Learning](http://web.stanford.edu/~hastie/ElemStatLearn/) Тревора Хасти и др.). Как и бэггинг, это общий метод или метаметод, который может применяться ко многим моделям статистического обучения.

Следующие разделы кратко представляют AdaBoost, а затем фокусируются на модели градиентного бустинга, а также на нескольких современных реализациях этого алгоритма.

### Алгоритм AdaBoost

AdaBoost — это значительное отступление от бэггинга, который строит ансамбли на очень глубоких деревьях для уменьшения смещения. AdaBoost, напротив, выращивает неглубокие деревья как слабые модели, часто достигая превосходной точности с пнями — то есть деревьями, образованными одним разбиением. Алгоритм начинает с равновзвешенной обучающей выборки, а затем последовательно изменяет распределение выборки. После каждой итерации AdaBoost увеличивает веса неправильно классифицированных наблюдений и уменьшает веса правильно предсказанных образцов, чтобы последующие слабые модели больше фокусировались на особенно сложных случаях. После обучения новое дерево решений включается в ансамбль с весом, отражающим его вклад в уменьшение ошибки обучения.

- [A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting](http://www.face-rec.org/algorithms/Boosting-Ensemble/decision-theoretic_generalization.pdf), Y. Freund, R. Schapire, 1997.

### Пример кода: AdaBoost с sklearn

В рамках модуля ensemble sklearn предоставляет реализацию [AdaBoostClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html), поддерживающую два или более классов.

Примеры кода для этого раздела находятся в ноутбуке [gbm_baseline](01_gbm_baseline.ipynb), который сравнивает производительность различных алгоритмов с фиктивным классификатором, всегда предсказывающим наиболее частый класс.

Алгоритмы в этой главе используют датасет, сгенерированный ноутбуком [feature-engineering](../04_alpha_factor_research/00_data/feature_engineering.ipynb) из [Главы 4 об исследовании альфа-факторов](../04_alpha_factor_research), который необходимо выполнить первым.

- Документация `sklearn` AdaBoost [docs](https://scikit-learn.org/stable/modules/ensemble.html#adaboost)

## Градиентный бустинг — ансамбли для большинства задач

Основная идея получившегося алгоритма Gradient Boosting Machines (GBM) заключается в обучении базовых моделей изучать отрицательный градиент текущей функции потерь ансамбля. В результате каждое добавление к ансамблю напрямую способствует уменьшению общей ошибки обучения с учётом ошибок, допущенных предыдущими членами ансамбля. Поскольку каждый новый член представляет собой новую функцию от данных, говорят, что градиентный бустинг оптимизирует функции hm аддитивным образом.

- [Greedy function approximation: A gradient boosting machine](https://statweb.stanford.edu/~jhf/ftp/trebst.pdf), Jerome H. Friedman, 1999

### Как обучать и настраивать GBM модели

Два ключевых фактора производительности градиентного бустинга — это размер ансамбля и сложность составляющих его деревьев решений. Контроль сложности деревьев решений направлен на предотвращение изучения высокоспецифичных правил, которые обычно подразумевают очень малое количество образцов в листовых узлах. Мы рассмотрели наиболее эффективные ограничения, используемые для ограничения способности дерева решений переобучаться на обучающих данных, в [Главе 4 о деревьях решений и случайных лесах](../10_decision_trees_random_forests).

В дополнение к прямому контролю размера ансамбля существуют различные методы регуляризации, такие как сжатие (shrinkage), с которым мы столкнулись в контексте моделей линейной регрессии Ridge и Lasso в [Главе 7, Линейные модели — регрессия и классификация](../07_linear_models). Кроме того, методы рандомизации, используемые в контексте случайных лесов, также обычно применяются к градиентным бустинг-машинам.

### Пример кода: градиентный бустинг с scikit-learn

Модуль ensemble в sklearn содержит реализацию деревьев градиентного бустинга для регрессии и классификации, как бинарной, так и многоклассовой.

Ноутбук [boosting_baseline](./01_boosting_baseline.ipynb) демонстрирует, как запускать кросс-валидацию для [GradientBoostingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html).

Ноутбук [sklearn_gbm_tuning](02_sklearn_gbm_tuning.ipynb) показывает, как использовать [GridSearchCV]() для поиска лучшего набора параметров. Это может быть очень затратным по времени.

Ноутбук [sklearn_gbm_tuning_results](03_sklearn_gbm_tuning_results.ipynb) отображает некоторые результаты, которые могут быть получены.

- Документация `scikit-klearn` Gradient Boosting [docs](https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting)

## Использование XGBoost, LightGBM и CatBoost

За последние несколько лет появилось несколько новых реализаций градиентного бустинга, использующих различные инновации, которые ускоряют обучение, повышают эффективность использования ресурсов и позволяют алгоритму масштабироваться на очень большие наборы данных. Новые реализации и их источники следующие:
- [XGBoost](https://github.com/dmlc/xgboost) (extreme gradient boosting), начат в 2014 году Тяньци Ченом в Университете Вашингтона
- [LightGBM](https://github.com/Microsoft/LightGBM), впервые выпущен в январе 2017 года компанией Microsoft
- [CatBoost](https://tech.yandex.com/catboost/), впервые выпущен в апреле 2017 года компанией Яндекс

В книге рассматриваются многочисленные алгоритмические инновации, появившиеся со временем и впоследствии сконвергировавшие (так что большинство функций доступны во всех реализациях), прежде чем проиллюстрировать их реализацию.

## Пример кода: торговая стратегия long-short с градиентным бустингом

В этом разделе мы разработаем, реализуем и оценим торговую стратегию для американских акций, основанную на ежедневных прогнозах доходности, создаваемых моделями градиентного бустинга.

Как и в предыдущих примерах, мы изложим фреймворк и построим конкретный пример, который вы можете адаптировать для запуска собственных экспериментов. Есть множество аспектов, которые вы можете варьировать, от класса активов и инвестиционной вселенной до более детальных аспектов, таких как признаки, период удержания или торговые правила. См., например, Библиотеку альфа-факторов в Приложении для множества дополнительных признаков.

### Подготовка данных

Мы используем данные Quandl Wiki для создания нескольких простых признаков (см. ноутбук [preparing_the_model_data](04_preparing_the_model_data.ipynb) для деталей) и выбираем модель, используя 2015/16 в качестве периода валидации, и проводим тест вне выборки для 2017 года.

### Как генерировать сигналы с моделями LightGBM и CatBoost

Мы сохраним торговую стратегию простой и будем использовать только один сигнал машинного обучения (ML); реальное приложение, вероятно, будет использовать несколько сигналов из разных источников, таких как дополняющие ML-модели, обученные на разных датасетах или с разными периодами прогнозирования или ретроспективы. Оно также будет использовать сложное управление рисками, от простого стоп-лосса до анализа стоимости под риском (VaR).

XGBoost, LightGBM и CatBoost предлагают интерфейсы для нескольких языков, включая Python, и имеют как sklearn-интерфейс, совместимый с другими функциями sklearn, такими как GridSearchCV, так и собственные методы для обучения и прогнозирования моделей градиентного бустинга. Ноутбук [gbm_baseline](01_gbm_baseline.ipynb) иллюстрирует использование sklearn-интерфейса для каждой реализации. Методы библиотек часто лучше документированы и также просты в использовании, поэтому мы будем использовать их для иллюстрации применения этих моделей.

Процесс включает создание специфичных для библиотеки форматов данных, настройку различных гиперпараметров и оценку результатов, которые мы опишем в следующих разделах.

- Ноутбук [trading_signals_with_lightgbm_and_catboost](05_trading_signals_with_lightgbm_and_catboost.ipynb) выполняет кросс-валидацию диапазона вариантов гиперпараметров для оптимизации прогностической производительности моделей.

### Оценка торговых сигналов

Ноутбук [evaluate_trading_signals](06_evaluate_trading_signals.ipynb) демонстрирует, как оценивать торговые сигналы.

### Создание прогнозов вне выборки

Ноутбук [making_out_of_sample_predictions](08_making_out_of_sample_predictions.ipynb) показывает, как создавать прогнозы для лучших моделей.

### Определение и бэктестинг стратегии long-short

Ноутбук [backtesting_with_zipline](09_backtesting_with_zipline.ipynb) создаёт стратегию на основе прогнозов модели, симулирует её историческую производительность с использованием [Zipline](https://zipline.ml4trading.io/) и оценивает результат с помощью [Pyfolio](https://github.com/quantopian/pyfolio).

## Заглянуть в чёрный ящик: как интерпретировать результаты GBM

Понимание того, почему модель предсказывает определённый результат, очень важно по нескольким причинам, включая доверие, практическую применимость, подотчётность и отладку. Понимание нелинейных отношений между признаками и результатом, обнаруженных моделью, а также взаимодействий между признаками, также ценно, когда цель состоит в том, чтобы узнать больше о базовых драйверах изучаемого явления.

### Пример кода: определение важности признаков с LightGBM

Ноутбук [model_interpretation](06_model_interpretation.ipynb) иллюстрирует следующие методы.

#### Важность признаков

Существует три основных способа вычисления глобальных значений важности признаков:
- Прирост (Gain): Этот классический подход, представленный Лео Брейманом в 1984 году, использует общее снижение потерь или нечистоты, обеспечиваемое всеми разбиениями для данного признака. Мотивация в основном эвристическая, но это широко используемый метод для отбора признаков.
- Количество разбиений (Split count): Это альтернативный подход, который подсчитывает, как часто признак используется для принятия решения о разбиении, на основе выбора признаков для этой цели на основе результирующего информационного прироста.
- Перестановка (Permutation): Этот подход случайным образом переставляет значения признака в тестовом наборе и измеряет, насколько изменяется ошибка модели, предполагая, что важный признак должен вызвать большое увеличение ошибки предсказания. Различные варианты перестановок приводят к альтернативным реализациям этого базового подхода.

#### Графики частичной зависимости

В дополнение к суммарному вкладу отдельных признаков в предсказание модели, графики частичной зависимости визуализируют связь между целевой переменной и набором признаков. Нелинейная природа деревьев градиентного бустинга приводит к тому, что эта связь зависит от значений всех других признаков.

#### SHAP-значения (SHapley Additive exPlanations)

На конференции NIPS 2017 года Скотт Лундберг и Су-Ин Ли из Университета Вашингтона представили новый и более точный подход к объяснению вклада отдельных признаков в выход моделей ансамблей деревьев, называемый [SHapley Additive exPlanations](https://github.com/slundberg/shap), или SHAP-значения.

Этот новый алгоритм исходит из наблюдения, что методы атрибуции признаков для ансамблей деревьев, такие как рассмотренные ранее, непоследовательны — то есть изменение в модели, увеличивающее влияние признака на выход, может снизить значения важности для этого признака.

SHAP-значения объединяют идеи из кооперативной теории игр и локальных объяснений, и было показано, что они теоретически оптимальны, согласованы и локально точны на основе математических ожиданий. Что наиболее важно, Лундберг и Ли разработали алгоритм, который позволяет снизить сложность вычисления этих модель-агностических аддитивных методов атрибуции признаков с O(TLDM) до O(TLD2), где T и M — количество деревьев и признаков соответственно, а D и L — максимальная глубина и количество листьев по деревьям.

Эта важная инновация позволяет объяснять предсказания от ранее невычислимых моделей с тысячами деревьев и признаков за доли секунды. Реализация с открытым исходным кодом стала доступна в конце 2017 года и совместима с XGBoost, LightGBM, CatBoost и моделями деревьев sklearn.

Значения Шепли возникли в теории игр как техника для присвоения ценности каждому игроку в кооперативной игре, которая отражает его вклад в успех команды. SHAP-значения — это адаптация концепции теории игр к моделям на основе деревьев и вычисляются для каждого признака и каждого образца. Они измеряют, как признак способствует выходу модели для данного наблюдения. По этой причине SHAP-значения предоставляют дифференцированное понимание того, как влияние признака варьируется между образцами, что важно учитывая роль эффектов взаимодействия в этих нелинейных моделях.

SHAP-значения обеспечивают детальную атрибуцию признаков на уровне каждого отдельного предсказания и позволяют проводить гораздо более богатый анализ сложных моделей через (интерактивную) визуализацию. Сводный диаграмма рассеяния SHAP, показанная в начале этого раздела, предлагает гораздо более дифференцированное понимание, чем глобальная столбчатая диаграмма важности признаков. Силовые графики отдельных кластеризованных предсказаний позволяют проводить более детальный анализ, в то время как графики зависимости SHAP захватывают эффекты взаимодействия и, как следствие, предоставляют более точные и детальные результаты, чем графики частичной зависимости.

## Внутридневная стратегия с Algoseek и LightGBM

Этот раздел и ноутбуки будут обновлены, когда Algoseek сделает образцы данных доступными.

### Пример кода: создание внутридневных признаков

Ноутбук [intraday_features](10_intraday_features.ipynb) создаёт признаки из данных минутных баров сделок и котировок.

### Пример кода: настройка модели LightGBM и оценка прогнозов

Ноутбук [intraday_model](11_intraday_model.ipynb) оптимизирует модель LightGBM, генерирует прогнозы вне выборки и оценивает результат.

## Ресурсы

- [Сравнение параметров xgboost и LightGBM](https://sites.google.com/view/lauraepp/parameters)
- [Бенчмарки xgboost vs LightGBM](https://sites.google.com/view/lauraepp/new-benchmarks)
- [Рост по глубине vs по листьям](https://datascience.stackexchange.com/questions/26699/decision-trees-leaf-wise-best-first-and-level-wise-tree-traverse)
- [Rashmi Korlakai Vinayak, Ran Gilad-Bachrach. "DART: Dropouts meet Multiple Additive Regression Trees."](http://www.jmlr.org/proceedings/papers/v38/korlakaivinayak15.pdf)

### XGBoost

- [GitHub репозиторий](https://github.com/dmlc/xgboost)
- [Документация](https://xgboost.readthedocs.io)
- [Параметры](https://xgboost.readthedocs.io/en/latest/parameter.html)
- [XGBoost: A Scalable Tree Boosting System](https://arxiv.org/abs/1603.02754)
- [Accelerating the XGBoost algorithm using GPU computing. Mitchell R, Frank E., 2017, PeerJ Computer Science 3:e127](https://peerj.com/articles/cs-127/)
- [XGBoost: Scalable GPU Accelerated Learning, Rory Mitchell, Andrey Adinets, Thejaswi Rao, 2018](http://arxiv.org/abs/1806.11248)
- [Nvidia Parallel Forall: Gradient Boosting, Decision Trees and XGBoost with CUDA, Rory Mitchell, 2017](https://devblogs.nvidia.com/parallelforall/gradient-boosting-decision-trees-xgboost-cuda/)
- [Awesome XGBoost](https://github.com/dmlc/xgboost/tree/master/demo)

### LightGBM

- [GitHub репозиторий](https://github.com/Microsoft/LightGBM)
- [Документация](https://lightgbm.readthedocs.io/en/latest/index.html)
- [Параметры](https://lightgbm.readthedocs.io/en/latest/Parameters.html)
- [Python API](https://lightgbm.readthedocs.io/en/latest/Python-API.html)
- [LightGBM: A Highly Efficient Gradient Boosting Decision Tree](https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf)
- [On Grouping for Maximum Homogeneity](https://amstat.tandfonline.com/doi/abs/10.1080/01621459.1958.10501479#.W8_3pXX24UE)

### CatBoost

- [Python API](https://tech.yandex.com/catboost/doc/dg/concepts/python-quickstart-docpage/)
- [CatBoost: unbiased boosting with categorical features](https://arxiv.org/abs/1706.09516)
- [CatBoost: gradient boosting with categorical features](http://learningsys.org/nips17/assets/papers/paper_11.pdf)
