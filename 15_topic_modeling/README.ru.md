# Тематическое моделирование для анализа отчётов о прибылях и финансовых новостей

Эта глава использует машинное обучение без учителя для извлечения скрытых тем из документов. Эти темы могут автоматически предоставлять детальное понимание большого корпуса документов. Они очень полезны для понимания самого массива данных и позволяют кратко маркировать документы, используя степень связи тем и документов.

Тематические модели позволяют извлекать сложные, интерпретируемые текстовые признаки, которые можно использовать различными способами для извлечения торговых сигналов из больших коллекций документов. Они ускоряют обзор документов, помогают выявлять и кластеризовать похожие документы, и могут быть аннотированы как основа для предиктивного моделирования. Применения включают идентификацию ключевых тем в раскрытиях компаний или стенограммах отчётов о прибылях, отзывах клиентов или контрактах, аннотированных с использованием, например, анализа тональности или прямой маркировки с последующими доходностями активов.

## Содержание

1. [Изучение скрытых тем: цели и подходы](#изучение-скрытых-тем-цели-и-подходы)
2. [Латентное семантическое индексирование (LSI)](#латентное-семантическое-индексирование-lsi)
    * [Пример кода: как реализовать LSI с помощью scikit-learn](#пример-кода-как-реализовать-lsi-с-помощью-scikit-learn)
3. [Вероятностный латентный семантический анализ (pLSA)](#вероятностный-латентный-семантический-анализ-plsa)
    * [Пример кода: как реализовать pLSA с помощью scikit-learn](#пример-кода-как-реализовать-plsa-с-помощью-scikit-learn)
4. [Латентное размещение Дирихле (LDA)](#латентное-размещение-дирихле-lda)
    * [Пример кода: распределение Дирихле](#пример-кода-распределение-дирихле)
    * [Как оценивать темы LDA](#как-оценивать-темы-lda)
    * [Пример кода: как реализовать LDA с помощью scikit-learn](#пример-кода-как-реализовать-lda-с-помощью-scikit-learn)
    * [Как визуализировать результаты LDA с помощью pyLDAvis](#как-визуализировать-результаты-lda-с-помощью-pyldavis)
    * [Пример кода: как реализовать LDA с помощью gensim](#пример-кода-как-реализовать-lda-с-помощью-gensim)
    * [Ссылки](#ссылки)
5. [Пример кода: Моделирование тем, обсуждаемых во время отчётов о прибылях](#пример-кода-моделирование-тем-обсуждаемых-во-время-отчётов-о-прибылях)
6. [Пример кода: Тематическое моделирование финансовых новостей](#пример-кода-тематическое-моделирование-финансовых-новостей)
7. [Ресурсы](#ресурсы)
    * [Применения](#применения)
    * [Библиотеки для тематического моделирования](#библиотеки-для-тематического-моделирования)

## Изучение скрытых тем: цели и подходы

Первые попытки тематических моделей улучшить векторную модель пространства (разработанную в середине 1970-х) применяли линейную алгебру для уменьшения размерности матрицы документ-термин. Этот подход похож на алгоритм, который мы обсуждали как анализ главных компонент (PCA) в главе 12 об обучении без учителя. Хотя этот метод эффективен, трудно оценить результаты таких моделей без эталонной модели.

В ответ на это появились вероятностные модели, которые предполагают явный процесс генерации документов и предоставляют алгоритмы для обратной инженерии этого процесса и восстановления исходных тем.

Таблица ниже показывает ключевые этапы эволюции моделей, которые мы рассмотрим подробнее в следующих разделах.

| Модель                                         | Год  | Описание                                                                                                       |
|------------------------------------------------|------|----------------------------------------------------------------------------------------------------------------|
| Латентное семантическое индексирование (LSI)   | 1988 | Захват семантических отношений документ-термин путём уменьшения размерности пространства слов                  |
| Вероятностный латентный семантический анализ (pLSA) | 1999 | Обратная инженерия генеративного процесса, который предполагает, что слова генерируют тему, а документы — смесь тем |
| Латентное размещение Дирихле (LDA)             | 2003 | Добавляет генеративный процесс для документов: трёхуровневая иерархическая байесовская модель                  |

## Латентное семантическое индексирование (LSI)

Латентный семантический анализ был создан для улучшения результатов запросов, которые пропускали релевантные документы, содержащие синонимы терминов запроса. Его цель — моделировать отношения между документами и терминами, чтобы предсказать, что термин должен быть связан с документом, даже если из-за вариативности использования слов такая связь не наблюдалась.

LSI использует линейную алгебру для нахождения заданного числа k скрытых тем путём декомпозиции матрицы документ-термин (DTM). Более конкретно, он использует сингулярное разложение (SVD) для нахождения лучшего приближения DTM меньшего ранга с использованием k сингулярных значений и векторов. Другими словами, LSI — это применение методов обучения без учителя для уменьшения размерности, которые мы встретили в главе 12 (с некоторыми дополнительными деталями). Авторы экспериментировали с иерархической кластеризацией, но нашли её слишком ограничивающей для явного моделирования отношений документ-тема и тема-термин или захвата связей документов или терминов с несколькими темами.

### Пример кода: как реализовать LSI с помощью scikit-learn

Ноутбук [latent_semantic_indexing](01_latent_semantic_indexing.ipynb) демонстрирует, как применить LSI к новостным статьям BBC, которые мы использовали в прошлой главе.

## Вероятностный латентный семантический анализ (pLSA)

Вероятностный латентный семантический анализ (pLSA) рассматривает LSA со статистической точки зрения и создаёт генеративную модель для решения проблемы отсутствия теоретических основ в LSA.

pLSA явно моделирует вероятность каждого совместного появления документов d и слов w, описанного матрицей DTM, как смесь условно независимых мультиномиальных распределений, включающих темы t. Количество тем является гиперпараметром, выбираемым до обучения, и не извлекается из данных.

### Пример кода: как реализовать pLSA с помощью scikit-learn

Ноутбук [probabilistic_latent_analysis](02_probabilistic_latent_analysis.ipynb) демонстрирует, как применить pLSA к новостным статьям BBC, которые мы использовали в прошлой главе.

- [Связь между pLSA и NMF и последствия](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.70.8839&rep=rep1&type=pdf), Gaussier, Goutte, 2005

## Латентное размещение Дирихле (LDA)

Латентное размещение Дирихле расширяет pLSA, добавляя генеративный процесс для тем.
Это самая популярная тематическая модель, потому что она создаёт осмысленные темы, понятные людям, может назначать темы новым документам и легко расширяется. Варианты моделей LDA могут включать метаданные, такие как авторы, изображения или иерархические темы.

LDA — это иерархическая байесовская модель, которая предполагает, что темы — это распределения вероятностей по словам, а документы — распределения по темам. Более конкретно, модель предполагает, что темы следуют разреженному распределению Дирихле, что означает, что документы охватывают только небольшой набор тем, а темы часто используют только небольшой набор слов.

### Пример кода: распределение Дирихле

Распределение Дирихле производит векторы вероятностей, которые можно использовать с дискретными распределениями. То есть оно случайным образом генерирует заданное количество значений, которые положительны и в сумме дают единицу. У него есть параметр α (альфа) положительных вещественных значений, который контролирует концентрацию вероятностей.

Ноутбук [dirichlet_distribution](03_dirichlet_distribution.ipynb) содержит симуляцию, чтобы вы могли экспериментировать с разными значениями параметров.

### Как оценивать темы LDA

Тематические модели без учителя не гарантируют, что результат будет осмысленным или интерпретируемым, и нет объективной метрики для оценки результата, как в обучении с учителем. Человеческая оценка тем считается «золотым стандартом», но потенциально дорогая и не всегда доступна в масштабе.

Два варианта для более объективной оценки результатов включают перплексию, которая оценивает модель на невиденных документах, и метрики когерентности тем, которые направлены на оценку семантического качества обнаруженных паттернов.

### Пример кода: как реализовать LDA с помощью scikit-learn

Ноутбук [lda_with_sklearn](04_lda_with_sklearn.ipynb) показывает, как применить LDA к новостным статьям BBC. Мы используем [sklearn.decomposition.LatentDirichletAllocation](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html) для обучения модели LDA с пятью темами.

### Как визуализировать результаты LDA с помощью pyLDAvis

Визуализация тем облегчает оценку качества тем с использованием человеческого суждения. pyLDAvis — это Python-порт LDAvis, разработанный на R и D3.js. Мы представим ключевые концепции; каждый ноутбук с реализацией LDA содержит примеры.

pyLDAvis отображает глобальные отношения между темами, а также облегчает их семантическую оценку путём изучения терминов, наиболее тесно связанных с каждой отдельной темой, и наоборот, тем, связанных с каждым термином. Он также решает проблему того, что термины, часто встречающиеся в корпусе, склонны доминировать в мультиномиальном распределении по словам, определяющем тему. LDAVis вводит релевантность r термина w для темы t для создания гибкого ранжирования ключевых терминов с использованием весового параметра 0<=λ<=1.

- [Доклад автора](https://speakerdeck.com/bmabey/visualizing-topic-models) и [Статья (оригинального) автора](http://www.aclweb.org/anthology/W14-3110)
- [Документация](http://pyldavis.readthedocs.io/en/latest/index.html)

### Пример кода: как реализовать LDA с помощью gensim

Gensim — это специализированная NLP-библиотека с быстрой реализацией LDA и множеством дополнительных функций. Мы также будем использовать её в следующей главе для изучения векторов слов (подробности см. в ноутбуке [lda_with_gensim](05_lda_with_gensim.ipynb)).

### Ссылки

- [Домашняя страница David Blei @ Columbia](http://www.cs.columbia.edu/~blei/)
- [Вводная статья](http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf) и [более техническая обзорная статья](http://www.cs.columbia.edu/~blei/papers/BleiLafferty2009.pdf)
- [Blei Lab @ GitHub](https://github.com/Blei-Lab)
- [Исследование когерентности тем для многих моделей и многих тем](https://www.aclweb.org/anthology/D/D12/D12-1087.pdf)
- [Статья о различных методах](http://www.aclweb.org/anthology/N10-1012)
- [Блог-пост — обзор](http://qpleple.com/topic-coherence-to-evaluate-topic-models/)

## Пример кода: Моделирование тем, обсуждаемых во время отчётов о прибылях

В Главе 3 об [Альтернативных данных](../03_alternative_data/02_earnings_calls) мы узнали, как собирать данные о отчётах о прибылях с сайта SeekingAlpha.

В этом разделе мы проиллюстрируем тематическое моделирование с использованием этого источника. Я использую выборку около 700 стенограмм отчётов о прибылях за 2018 и 2019 годы (см. директорию [data](../data)). Это довольно маленькая выборка; для практического применения понадобится больший набор данных.

Ноутбук [lda_earnings_calls](06_lda_earnings_calls.ipynb) содержит подробности о загрузке, исследовании и предобработке данных, а также обучении и оценке различных моделей.

## Пример кода: Тематическое моделирование финансовых новостей

Ноутбук [lda_financial_news](07_lda_financial_news.ipynb) показывает, как обобщить большой корпус финансовых новостных статей от Reuters и других источников (см. [data](../data) для источников) с использованием LDA.

## Ресурсы

### Применения

- [Применения тематических моделей](https://mimno.infosci.cornell.edu/papers/2017_fntir_tm_applications.pdf), Jordan Boyd-Graber, Yuening Hu, David Mimno, 2017
- [Высококачественное извлечение тем из бизнес-новостей объясняет аномальную волатильность финансового рынка](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3675119/pdf/pone.0064846.pdf)
- [Что вы говорите? Использование тем для обнаружения финансовых искажений](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2803733)
- [LDA в браузере — реализация на JavaScript](https://github.com/mimno/jsLDA)
- [David Mimno @ Cornell University](https://mimno.infosci.cornell.edu/)

### Библиотеки для тематического моделирования

- [Список открытого ПО для тематического моделирования от David Blei](http://www.cs.columbia.edu/~blei/topicmodeling_software.html)
- [Mallet (MAchine Learning for LanguagE Toolkit — на Java)](http://mallet.cs.umass.edu/)
