# Глубокое обучение с подкреплением: Создание торгового агента

Обучение с подкреплением (RL) — это вычислительный подход к целенаправленному обучению, выполняемому агентом, который взаимодействует с обычно стохастической средой, о которой агент имеет неполную информацию. RL направлено на автоматизацию принятия решений агентом для достижения долгосрочной цели путём изучения ценности состояний и действий на основе сигнала вознаграждения. Конечная цель — вывести политику, которая кодирует правила поведения и сопоставляет состояния с действиями.

Эта глава показывает, как сформулировать задачу RL и как применять различные методы решения. Она охватывает методы, основанные на модели и без модели, знакомит со средой [OpenAI Gym](https://gym.openai.com/) и объединяет глубокое обучение с RL для обучения агента, который ориентируется в сложной среде. Наконец, мы покажем, как адаптировать RL к алгоритмической торговле, моделируя агента, который взаимодействует с финансовым рынком, пытаясь оптимизировать целевую функцию.

#### Содержание

1. [Ключевые элементы системы обучения с подкреплением](#ключевые-элементы-системы-обучения-с-подкреплением)
    * [Политика: преобразование состояний в действия](#политика-преобразование-состояний-в-действия)
    * [Вознаграждения: обучение на основе действий](#вознаграждения-обучение-на-основе-действий)
    * [Функция ценности: оптимальные решения на долгосрочную перспективу](#функция-ценности-оптимальные-решения-на-долгосрочную-перспективу)
    * [Среда](#среда)
    * [Компоненты интерактивной системы RL](#компоненты-интерактивной-системы-rl)
2. [Как решать задачи RL](#как-решать-задачи-rl)
    * [Пример кода: динамическое программирование – итерация по ценности и политике](#пример-кода-динамическое-программирование--итерация-по-ценности-и-политике)
    * [Пример кода: Q-обучение](#пример-кода-q-обучение)
3. [Глубокое обучение с подкреплением](#глубокое-обучение-с-подкреплением)
    * [Аппроксимация функции ценности с помощью нейронных сетей](#аппроксимация-функции-ценности-с-помощью-нейронных-сетей)
    * [Алгоритм Deep Q-learning и расширения](#алгоритм-deep-q-learning-и-расширения)
    * [OpenAI Gym – среда Lunar Lander](#openai-gym--среда-lunar-lander)
    * [Пример кода: Double Deep Q-Learning с использованием TensorFlow](#пример-кода-double-deep-q-learning-с-использованием-tensorflow)
4. [Пример кода: глубокий RL для торговли с TensorFlow 2 и OpenAI Gym](#пример-кода-глубокий-rl-для-торговли-с-tensorflow-2-и-openai-gym)
    * [Как спроектировать торговую среду OpenAI](#как-спроектировать-торговую-среду-openai)
    * [Как построить агента Deep Q-learning для фондового рынка](#как-построить-агента-deep-q-learning-для-фондового-рынка)
5. [Ресурсы](#ресурсы)
    * [Алгоритмы RL](#алгоритмы-rl)
    * [Применение в инвестициях](#применение-в-инвестициях)

## Ключевые элементы системы обучения с подкреплением

Задачи RL имеют несколько элементов, которые отличают их от ранее рассмотренных нами настроек машинного обучения. Следующие два раздела описывают ключевые особенности, необходимые для определения и решения задачи RL путём изучения политики, которая автоматизирует принятие решений.

Мы будем использовать обозначения и в целом следовать книге [Reinforcement Learning: An Introduction](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf) (Sutton и Barto 2018) и [курсам UCL по RL](https://www.davidsilver.uk/teaching/) Дэвида Сильвера, которые рекомендуются для дальнейшего изучения за пределами краткого обзора, который позволяет объём этой главы.

Задачи RL направлены на оптимизацию решений агента на основе целевой функции относительно среды.

### Политика: преобразование состояний в действия

В любой момент времени политика определяет поведение агента. Она сопоставляет любое состояние, с которым может столкнуться агент, с одним или несколькими действиями. В среде с ограниченным числом состояний и действий политика может быть простой таблицей поиска, заполняемой во время обучения.

### Вознаграждения: обучение на основе действий

Сигнал вознаграждения — это единственное значение, которое среда отправляет агенту на каждом временном шаге. Цель агента обычно состоит в максимизации общего вознаграждения, полученного за время. Вознаграждения также могут быть стохастической функцией от состояния и действий. Обычно они дисконтируются для облегчения сходимости и отражения временного обесценивания ценности.

### Функция ценности: оптимальные решения на долгосрочную перспективу

Вознаграждение обеспечивает немедленную обратную связь о действиях. Однако решение задачи RL требует принятия решений, которые создают ценность в долгосрочной перспективе. Здесь вступает в игру функция ценности: она суммирует полезность состояний или действий в данном состоянии с точки зрения их долгосрочного вознаграждения.

### Среда

Среда предоставляет агенту информацию о своём состоянии, назначает вознаграждения за действия и переводит агента в новые состояния в соответствии с распределениями вероятностей, о которых агент может знать или не знать.

Она может быть полностью или частично наблюдаемой и может также содержать других агентов. Проектирование среды обычно требует значительных предварительных усилий для облегчения целенаправленного обучения агента во время тренировки.

Задачи RL различаются по сложности пространств состояний и действий, которые могут быть дискретными или непрерывными. Последнее требует машинного обучения для аппроксимации функциональной связи между состояниями, действиями и их ценностью. Они также требуют обобщения от подмножества состояний и действий, с которыми агент сталкивается во время обучения.

### Компоненты интерактивной системы RL

Компоненты системы RL обычно включают:

- Наблюдения агентом состояния среды
- Набор действий, доступных агенту
- Политика, которая управляет решениями агента

Кроме того, среда испускает сигнал вознаграждения, который отражает новое состояние, возникающее в результате действия агента. В основе агент обычно изучает функцию ценности, которая формирует его суждение о действиях. У агента есть целевая функция для обработки сигнала вознаграждения и преобразования суждений о ценности в оптимальную политику.

## Как решать задачи RL

Методы RL направлены на обучение на опыте тому, как предпринимать действия для достижения долгосрочной цели. Для этого агент и среда взаимодействуют на протяжении последовательности дискретных временных шагов через интерфейс действий, наблюдений состояния и вознаграждений, который мы описали в предыдущем разделе.

Существует множество подходов к решению задач RL, что подразумевает нахождение правил оптимального поведения агента:

- Методы **динамического программирования** (DP) делают часто нереалистичное предположение о полном знании среды, но являются концептуальной основой для большинства других подходов.
- Методы **Монте-Карло** (MC) узнают о среде и затратах и выгодах различных решений путём выборки целых последовательностей состояние-действие-вознаграждение.
- **Обучение с временной разницей** (TD) значительно улучшает эффективность выборки, обучаясь на более коротких последовательностях. Для этого оно опирается на бутстрэппинг, который определяется как уточнение оценок на основе собственных предыдущих оценок.

Подходы для непрерывных пространств состояний и/или действий часто используют машинное обучение для аппроксимации функции ценности или политики. Следовательно, они интегрируют обучение с учителем и, в частности, методы глубокого обучения, которые мы обсуждали в последних нескольких главах. Однако эти методы сталкиваются с отдельными проблемами в контексте RL:

- Сигнал вознаграждения не отражает напрямую целевую концепцию, такую как размеченный образец
- Распределение наблюдений зависит от действий агента и политики, которая сама является предметом процесса обучения

### Пример кода: динамическое программирование – итерация по ценности и политике

Конечные MDP — это простая, но фундаментальная структура. Этот раздел вводит траектории вознаграждений, которые агент стремится оптимизировать, и определяет функции политики и ценности, которые используются для формулировки задачи оптимизации и уравнений Беллмана, лежащих в основе методов решения.

Ноутбук [gridworld_dynamic_programming](01_gridworld_dynamic_programming.ipynb) применяет итерацию по ценности и политике к игрушечной среде, состоящей из сетки 3 x 4.

### Пример кода: Q-обучение

Q-обучение стало ранним прорывом в RL, когда оно было разработано Крисом Уоткинсом для его [докторской диссертации](http://www.cs.rhul.ac.uk/~chrisw/thesis.html) в 1989 году. Оно вводит инкрементное динамическое программирование для управления MDP без знания или моделирования матриц переходов и вознаграждений, которые мы использовали для итерации по ценности и политике в предыдущем разделе. Доказательство сходимости последовало через три года от [Watkins и Dayan](http://www.gatsby.ucl.ac.uk/~dayan/papers/wd92.html).

Q-обучение напрямую оптимизирует функцию ценности действия, q, для аппроксимации q*. Обучение происходит вне политики, то есть алгоритму не нужно выбирать действия исключительно на основе политики, подразумеваемой функцией ценности. Однако сходимость требует, чтобы все пары состояние-действие продолжали обновляться на протяжении всего процесса обучения. Простой способ обеспечить это — использовать ε-жадную политику.

Алгоритм Q-обучения продолжает улучшать функцию ценности состояние-действие после случайной инициализации для заданного количества эпизодов. На каждом временном шаге он выбирает действие на основе ε-жадной политики и использует скорость обучения, α, для обновления функции ценности на основе вознаграждения и текущей оценки функции ценности для следующего состояния.

Ноутбук [gridworld_q_learning](02_gridworld_q_learning.ipynb) демонстрирует, как построить агента Q-обучения, используя сетку 3 x 4 состояний из предыдущего раздела.

## Глубокое обучение с подкреплением

Этот раздел адаптирует Q-обучение к непрерывным состояниям и действиям, где мы не можем использовать табличное решение, которое просто заполняет массив значениями состояние-действие. Вместо этого мы увидим, как аппроксимировать оптимальную функцию ценности состояния с помощью нейронной сети для построения глубокой Q-сети с различными усовершенствованиями для ускорения сходимости. Затем мы увидим, как можно использовать [OpenAI Gym](http://gym.openai.com/docs/) для применения алгоритма к среде [Lunar Lander](https://gym.openai.com/envs/LunarLander-v2).

### Аппроксимация функции ценности с помощью нейронных сетей

Как и в других областях, глубокие нейронные сети стали популярными для аппроксимации функций ценности. Однако машинное обучение сталкивается с отдельными проблемами в контексте RL, где данные генерируются взаимодействием модели со средой с использованием (возможно, рандомизированной) политики:

- При непрерывных состояниях агент не сможет посетить большинство состояний и, следовательно, должен обобщать.
- Обучение с учителем стремится обобщать от выборки независимых и одинаково распределённых образцов, которые являются репрезентативными и правильно размеченными. В контексте RL есть только один образец на временной шаг, поэтому обучение должно происходить онлайн.
- Образцы могут быть сильно коррелированы, когда последовательные состояния похожи, и распределение поведения по состояниям и действиям не стационарно, а меняется в результате обучения агента.

### Алгоритм Deep Q-learning и расширения

Deep Q-learning оценивает ценность доступных действий для данного состояния с использованием глубокой нейронной сети. Он был представлен в работе DeepMind [Playing Atari with Deep Reinforcement Learning](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf) (2013), где агенты RL учились играть в игры исключительно по пиксельному входу.

Алгоритм Deep Q-learning аппроксимирует функцию ценности действия, q, путём изучения набора весов, θ, многослойной Deep Q Network (DQN), которая сопоставляет состояния с действиями.

Несколько инноваций улучшили точность и скорость сходимости Deep Q-Learning, а именно:

- **Воспроизведение опыта** хранит историю переходов состояние, действие, вознаграждение и следующее состояние и случайным образом выбирает мини-пакеты из этого опыта для обновления весов сети на каждом временном шаге перед тем, как агент выберет ε-жадное действие. Это увеличивает эффективность выборки, уменьшает автокорреляцию образцов и ограничивает обратную связь из-за текущих весов, производящих обучающие образцы, которые могут привести к локальным минимумам или расхождению.
- **Медленно изменяющаяся целевая сеть** ослабляет петлю обратной связи от текущих параметров сети на обновления весов нейронной сети. Также изобретённая DeepMind в [Human-level control through deep reinforcement learning](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf) (2015), она использует медленно изменяющуюся целевую сеть, которая имеет такую же архитектуру, как Q-сеть, но её веса обновляются только периодически. Целевая сеть генерирует прогнозы ценности следующего состояния, используемые для обновления оценки Q-сетью ценности текущего состояния.
- **Double Deep Q-learning** устраняет склонность Deep Q-Learning переоценивать значения действий, потому что он намеренно выбирает наибольшее значение действия. Это смещение может негативно влиять на процесс обучения и результирующую политику, если оно не применяется равномерно, как показал Хадо ван Хассельт в [Deep Reinforcement Learning with Double Q-learning](https://arxiv.org/abs/1509.06461) (2015). Чтобы отделить оценку значений действий от выбора действий, Double Deep Q-Learning (DDQN) использует веса одной сети для выбора лучшего действия для следующего состояния и веса другой сети для предоставления соответствующей оценки значения действия.

### OpenAI Gym – среда Lunar Lander

[OpenAI Gym](https://gym.openai.com/) — это платформа RL, которая предоставляет стандартизированные среды для тестирования и сравнения алгоритмов RL с использованием Python. Также возможно расширить платформу и зарегистрировать пользовательские среды.

Среда [Lunar Lander](https://gym.openai.com/envs/LunarLander-v2) (LL) требует от агента управления своим движением в двух измерениях на основе дискретного пространства действий и низкоразмерных наблюдений состояния, которые включают положение, ориентацию и скорость. На каждом временном шаге среда предоставляет наблюдение нового состояния и положительное или отрицательное вознаграждение. Каждый эпизод состоит максимум из 1000 временных шагов.

### Пример кода: Double Deep Q-Learning с использованием TensorFlow

Ноутбук [lunar_lander_deep_q_learning](03_lunar_lander_deep_q_learning.ipynb) реализует агента DDQN, который использует TensorFlow и среду Lunar Lander из OpenAI Gym.

## Пример кода: глубокий RL для торговли с TensorFlow 2 и OpenAI Gym

Для обучения торгового агента нам нужно создать рыночную среду, которая предоставляет информацию о ценах и другую информацию, предлагает действия, связанные с торговлей, и отслеживает портфель для соответствующего вознаграждения агента.

### Как спроектировать торговую среду OpenAI

OpenAI Gym позволяет проектировать, регистрировать и использовать среды, которые соответствуют его архитектуре, как описано в его [документации](https://github.com/openai/gym/tree/master/gym/envs#how-to-create-new-environments-for-gym).
- Файл [trading_env.py](trading_env.py) реализует пример, который иллюстрирует, как создать класс, реализующий необходимые методы `step()` и `reset()`.

Торговая среда состоит из трёх классов, которые взаимодействуют для облегчения деятельности агента:
1. Класс `DataSource` загружает временной ряд, генерирует несколько признаков и предоставляет последнее наблюдение агенту на каждом временном шаге.
2. `TradingSimulator` отслеживает позиции, сделки и затраты, а также производительность. Он также реализует и записывает результаты эталонной стратегии "купи и держи".
3. Сам `TradingEnvironment` оркестрирует процесс.

### Как построить агента Deep Q-learning для фондового рынка

Ноутбук [q_learning_for_trading](04_q_learning_for_trading.ipynb) демонстрирует, как настроить простую игру с ограниченным набором опций, относительно низкоразмерным состоянием и другими параметрами, которые можно легко изменить и расширить для обучения агента Deep Q-Learning, используемого в [lunar_lander_deep_q_learning](03_lunar_lander_deep_q_learning.ipynb).

<p align="center">
<img src="https://i.imgur.com/lg0ofbZ.png" width="60%">
</p>


## Ресурсы

- [Reinforcement Learning: An Introduction, 2nd edition](http://incompleteideas.net/book/RLbook2018.pdf), Richard S. Sutton и Andrew G. Barto, 2018
- [Курс University College of London по обучению с подкреплением](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html), David Silver, 2015
- [Реализация алгоритмов обучения с подкреплением](https://github.com/dennybritz/reinforcement-learning), Denny Britz
    - Этот репозиторий содержит код, упражнения и решения для популярных алгоритмов обучения с подкреплением. Они предназначены для использования в качестве учебного инструмента в дополнение к теоретическим материалам от Sutton/Barto и Silver (см. выше).

### Алгоритмы RL

- Q-обучение
    - [Learning from Delayed Rewards](http://www.cs.rhul.ac.uk/~chrisw/thesis.html), докторская диссертация, Chris Watkins, 1989
    - [Q-Learning](http://www.gatsby.ucl.ac.uk/~dayan/papers/wd92.html), Machine Learning, 1992
- Deep Q Networks
    - [Playing Atari with Deep Reinforcement Learning](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf), Mnih et al, 2013
    - Мы представляем первую модель глубокого обучения, которая успешно изучает политики управления непосредственно из высокоразмерного сенсорного входа с использованием обучения с подкреплением. Модель представляет собой свёрточную нейронную сеть, обученную с вариантом Q-обучения, вход которой — необработанные пиксели, а выход — функция ценности, оценивающая будущие вознаграждения. Мы применяем наш метод к семи играм Atari 2600 из Arcade Learning Environment без настройки архитектуры или алгоритма обучения. Мы обнаруживаем, что он превосходит все предыдущие подходы в шести играх и превосходит человека-эксперта в трёх из них.
- Asynchronous Advantage Actor-Critic (A2C/A3C)
    - [Asynchronous Methods for Deep Reinforcement Learning](https://arxiv.org/abs/1602.01783), Mnih, V. et al. 2016
    - Мы предлагаем концептуально простую и легковесную структуру для глубокого обучения с подкреплением, которая использует асинхронный градиентный спуск для оптимизации контроллеров глубоких нейронных сетей. Мы представляем асинхронные варианты четырёх стандартных алгоритмов обучения с подкреплением и показываем, что параллельные actor-learner имеют стабилизирующий эффект на обучение, позволяя всем четырём методам успешно обучать контроллеры нейронных сетей. Лучший метод, асинхронный вариант actor-critic, превосходит современный уровень на домене Atari, обучаясь в два раза быстрее на одном многоядерном CPU вместо GPU.
- Proximal Policy Optimization (PPO)
    - [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347), Schulman et al, 2017
    - Мы предлагаем новое семейство методов градиента политики для обучения с подкреплением, которые чередуют выборку данных через взаимодействие со средой и оптимизацию "суррогатной" целевой функции с использованием стохастического градиентного подъёма.

- Trust Region Policy Optimization (TRPO)
    - [Trust Region Policy Optimization](https://arxiv.org/abs/1502.05477), Schulman et al, 2015
    - Мы описываем итеративную процедуру для оптимизации политик с гарантированным монотонным улучшением.

- Deep Deterministic Policy Gradient (DDPG)
    - [Continuous control with deep reinforcement learning](https://arxiv.org/abs/1509.02971), Lillicrap et al, 2015
    - Мы адаптируем идеи, лежащие в основе успеха Deep Q-Learning, к области непрерывных действий.
- Twin Delayed DDPG (TD3)
    - [Addressing Function Approximation Error in Actor-Critic Methods](https://arxiv.org/abs/1802.09477), Fujimoto et al, 2018
    - В методах обучения с подкреплением, основанных на ценности, таких как deep Q-learning, известно, что ошибки аппроксимации функций приводят к переоценённым оценкам ценности и субоптимальным политикам.
- Soft Actor-Critic (SAC)
    - [Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor](https://arxiv.org/abs/1801.01290), Haarnoja et al, 2018
    - Алгоритмы глубокого обучения с подкреплением без модели были продемонстрированы на ряде сложных задач принятия решений и управления.
- Categorical 51-Atom DQN (C51)
    - [A Distributional Perspective on Reinforcement Learning](https://arxiv.org/abs/1707.06887), Bellemare, et al 2017
    - В этой статье мы аргументируем фундаментальную важность распределения ценности: распределения случайного возврата, полученного агентом обучения с подкреплением.

### Применение в инвестициях

- [A Deep Reinforcement Learning Framework for the Financial Portfolio Management Problem](https://arxiv.org/abs/1706.10059), Zhengyao Jiang, Dixing Xu, Jinjun Liang 2017
    - Управление финансовым портфелем — это процесс постоянного перераспределения средств в различные финансовые продукты. В этой статье представлена не зависящая от финансовых моделей структура обучения с подкреплением для предоставления решения глубокого машинного обучения проблемы управления портфелем.
    - [PGPortfolio](https://github.com/ZhengyaoJiang/PGPortfolio); соответствующий GitHub репозиторий
- [Financial Trading as a Game: A Deep Reinforcement Learning Approach](https://arxiv.org/pdf/1807.02787.pdf), Huang, Chien-Yi, 2018
- [Order placement with Reinforcement Learning](https://github.com/mjuchli/ctc-executioner)
    - CTC-Executioner — это инструмент, который предоставляет стратегию исполнения/размещения лимитных ордеров по требованию на рынках криптовалют с использованием методов обучения с подкреплением.

- [Q-Trader](https://github.com/edwardhdlu/q-trader)
    - Реализация Q-обучения, применённого к (краткосрочной) торговле акциями. Модель использует n-дневные окна цен закрытия, чтобы определить, является ли лучшим действием в данный момент покупка, продажа или ожидание.
