# Рекуррентные нейронные сети для трейдинга: многомерные временные ряды и текстовые данные

Главным нововведением рекуррентных нейронных сетей (RNN) является то, что каждый выходной сигнал является функцией как предыдущего выхода, так и новых данных. В результате RNN получают способность учитывать информацию о предыдущих наблюдениях при вычислениях с новым вектором признаков, фактически создавая модель с памятью. Такая рекуррентная формулировка позволяет разделять параметры по значительно более глубокому вычислительному графу, включающему циклы. Известные архитектуры включают долгую краткосрочную память (LSTM) и управляемые рекуррентные блоки (GRU), которые направлены на преодоление проблемы затухающих градиентов, связанной с обучением долгосрочных зависимостей, где ошибки должны распространяться через множество соединений.

RNN успешно применяются к различным задачам, требующим отображения одной или нескольких входных последовательностей в одну или несколько выходных последовательностей, и особенно хорошо подходят для обработки естественного языка. RNN также могут применяться к одномерным и многомерным временным рядам для прогнозирования рыночных или фундаментальных данных. В этой главе рассматривается, как RNN могут моделировать альтернативные текстовые данные, используя векторные представления слов (word embeddings), которые мы рассмотрели в [Главе 16](16_word_embeddings), для классификации тональности, выраженной в документах.

## Содержание

1. [Как работают рекуррентные нейронные сети](#как-работают-рекуррентные-нейронные-сети)
    * [Обратное распространение ошибки во времени](#обратное-распространение-ошибки-во-времени)
    * [Альтернативные архитектуры RNN](#альтернативные-архитектуры-rnn)
        - [Долгая краткосрочная память (LSTM)](#долгая-краткосрочная-память-lstm)
        - [Управляемые рекуррентные блоки (GRU)](#управляемые-рекуррентные-блоки-gru)
2. [RNN для финансовых временных рядов с TensorFlow 2](#rnn-для-финансовых-временных-рядов-с-tensorflow-2)
    * [Пример кода: одномерная регрессия временных рядов: прогнозирование S&P 500](#пример-кода-одномерная-регрессия-временных-рядов-прогнозирование-sp-500)
    * [Пример кода: многослойная LSTM для прогнозирования недельных движений и доходности акций](#пример-кода-многослойная-lstm-для-прогнозирования-недельных-движений-и-доходности-акций)
    * [Пример кода: прогнозирование доходности вместо направления движения цены](#пример-кода-прогнозирование-доходности-вместо-направления-движения-цены)
    * [Пример кода: многомерная регрессия временных рядов для макроэкономических данных](#пример-кода-многомерная-регрессия-временных-рядов-для-макроэкономических-данных)
3. [RNN для текстовых данных: анализ тональности и прогнозирование доходности](#rnn-для-текстовых-данных-анализ-тональности-и-прогнозирование-доходности)
    * [Пример кода: LSTM с пользовательскими векторными представлениями слов для классификации тональности](#пример-кода-lstm-с-пользовательскими-векторными-представлениями-слов-для-классификации-тональности)
    * [Пример кода: анализ тональности с предобученными векторами слов](#пример-кода-анализ-тональности-с-предобученными-векторами-слов)
    * [Пример кода: SEC-отчёты для двунаправленной RNN GRU для прогнозирования недельной доходности](#пример-кода-sec-отчёты-для-двунаправленной-rnn-gru-для-прогнозирования-недельной-доходности)

## Как работают рекуррентные нейронные сети

RNN предполагают, что входные данные были сгенерированы как последовательность, где предыдущие точки данных влияют на текущее наблюдение и важны для прогнозирования последующих значений. Таким образом, они позволяют моделировать более сложные отношения между входом и выходом, чем сети прямого распространения (FFNN) и свёрточные нейронные сети (CNN), которые предназначены для отображения одного входного вектора в один выходной вектор за заданное количество вычислительных шагов.

RNN, напротив, могут моделировать данные для задач, где вход, выход или оба лучше всего представлены как последовательность векторов.

Для подробного обзора см. [главу 10](https://www.deeplearningbook.org/contents/rnn.html) в книге [Deep Learning](https://www.deeplearningbook.org/) авторов Goodfellow, Bengio и Courville (2016).

### Обратное распространение ошибки во времени

RNN называются рекуррентными, потому что они применяют одни и те же преобразования к каждому элементу последовательности таким образом, что выход зависит от результата предыдущих итераций. В результате RNN поддерживают внутреннее состояние, которое захватывает информацию о предыдущих элементах последовательности, подобно памяти.

Алгоритм обратного распространения ошибки, который обновляет весовые параметры на основе градиента функции потерь относительно параметров, включает прямой проход слева направо вдоль развёрнутого вычислительного графа, за которым следует обратный проход в противоположном направлении.

- [Sequence Modeling: Recurrent and Recursive Nets](http://www.deeplearningbook.org/contents/rnn.html), Deep Learning Book, Глава 10, Ian Goodfellow, Yoshua Bengio и Aaron Courville, MIT Press, 2016
- [Supervised Sequence Labelling with Recurrent Neural Networks](https://www.cs.toronto.edu/~graves/preprint.pdf), Alex Graves, 2013
- [Tutorial on LSTM Recurrent Networks](http://people.idsia.ch/~juergen/lstm/sld001.htm), Juergen Schmidhuber, 2003
- [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)

### Альтернативные архитектуры RNN

RNN могут быть спроектированы различными способами для лучшего захвата функциональной связи и динамики между входными и выходными данными. В дополнение к рекуррентным связям между скрытыми состояниями существует несколько альтернативных подходов, включая рекуррентные выходные связи, двунаправленные RNN и архитектуры кодировщик-декодировщик.

#### Долгая краткосрочная память (LSTM)

RNN с архитектурой LSTM имеют более сложные блоки, которые поддерживают внутреннее состояние и содержат вентили (gates) для отслеживания зависимостей между элементами входной последовательности и соответствующего регулирования состояния ячейки. Эти вентили рекуррентно соединяются друг с другом вместо обычных скрытых блоков, которые мы встречали выше. Они направлены на решение проблемы затухающих и взрывающихся градиентов, позволяя градиентам проходить без изменений.

Типичный блок LSTM объединяет четыре параметризованных слоя, которые взаимодействуют друг с другом и с состоянием ячейки путём преобразования и передачи векторов. Эти слои обычно включают входной вентиль, выходной вентиль и вентиль забывания, но существуют вариации с дополнительными вентилями или без некоторых из этих механизмов.

- [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/), Christopher Olah, 2015
- [An Empirical Exploration of Recurrent Network Architectures](http://proceedings.mlr.press/v37/jozefowicz15.pdf), Rafal Jozefowicz, Ilya Sutskever и др., 2015

#### Управляемые рекуррентные блоки (GRU)

Управляемые рекуррентные блоки (GRU) упрощают блоки LSTM, исключая выходной вентиль. Было показано, что они достигают схожей производительности на определённых задачах языкового моделирования, но лучше работают на меньших наборах данных.

- [Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation](https://arxiv.org/pdf/1406.1078.pdf), Kyunghyun Cho, Yoshua Bengio и др., 2014
- [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](https://arxiv.org/abs/1412.3555), Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, Yoshua Bengio, 2014

## RNN для финансовых временных рядов с TensorFlow 2

Мы демонстрируем, как строить RNN с использованием библиотеки Keras для различных сценариев. Первый набор моделей включает регрессию и классификацию одномерных и многомерных временных рядов. Второй набор задач фокусируется на текстовых данных для анализа тональности с использованием текстовых данных, преобразованных в векторные представления слов (см. [Главу 15](../15_word_embeddings)).

- [Recurrent Neural Networks (RNN) with Keras](https://www.tensorflow.org/guide/keras/rnn)
- [Time series forecasting](https://www.tensorflow.org/tutorials/structured_data/time_series)
- [Keras documentation](https://keras.io/getting-started/sequential-model-guide/)
- [LSTM documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM)
- [Working with RNNs](https://keras.io/guides/working_with_rnns/) от Scott Zhu и Francois Chollet

### Пример кода: одномерная регрессия временных рядов: прогнозирование S&P 500

Ноутбук [univariate_time_series_regression](01_univariate_time_series_regression.ipynb) демонстрирует, как подготовить данные в требуемую форму и как прогнозировать значения индекса S&P 500 с помощью рекуррентной нейронной сети.

### Пример кода: многослойная LSTM для прогнозирования недельных движений и доходности акций

Теперь мы построим немного более глубокую модель, объединив два слоя LSTM с использованием данных о ценах акций Quandl. Кроме того, мы включим признаки, которые не являются последовательными по своей природе, а именно индикаторные переменные, идентифицирующие тикер и временные периоды, такие как месяц и год.
- См. ноутбук [stacked_lstm_with_feature_embeddings](02_stacked_lstm_with_feature_embeddings.ipynb) для деталей реализации.

### Пример кода: прогнозирование доходности вместо направления движения цены

Ноутбук [stacked_lstm_with_feature_embeddings_regression](03_stacked_lstm_with_feature_embeddings_regression.ipynb) демонстрирует, как адаптировать модель к задаче регрессии для прогнозирования доходности вместо бинарных изменений цены.

### Пример кода: многомерная регрессия временных рядов для макроэкономических данных

До сих пор мы ограничивали наши усилия по моделированию одиночными временными рядами. RNN естественно хорошо подходят для многомерных временных рядов и представляют собой нелинейную альтернативу моделям векторной авторегрессии (VAR), которые мы рассмотрели в [Главе 9, Модели временных рядов](../09_time_series_models).

Ноутбук [multivariate_timeseries](04_multivariate_timeseries.ipynb) демонстрирует применение RNN для моделирования и прогнозирования нескольких временных рядов с использованием того же набора данных, который мы использовали для [примера VAR](../09_time_series_models/04_vector_autoregressive_model.ipynb), а именно ежемесячных данных о потребительских настроениях и промышленном производстве от службы FRED Федеральной резервной системы.

## RNN для текстовых данных: анализ тональности и прогнозирование доходности

### Пример кода: LSTM с пользовательскими векторными представлениями слов для классификации тональности

RNN обычно применяются к различным задачам обработки естественного языка. Мы уже сталкивались с анализом тональности с использованием текстовых данных в третьей части [этой книги](https://www.amazon.com/Machine-Learning-Algorithmic-Trading-alternative/dp/1839217715?pf_rd_r=VMKJPZC4N36TTZZCWATP&pf_rd_p=c5b6893a-24f2-4a59-9d4b-aff5065c90ec&pd_rd_r=8f331266-0d21-4c76-a3eb-d2e61d23bb31&pd_rd_w=kVGNF&pd_rd_wg=LYLKH&ref_=pd_gw_ci_mcx_mr_hp_d).

Этот пример показывает, как обучить пользовательские векторные представления слов одновременно с обучением RNN на задаче классификации. Это отличается от модели word2vec, которая обучает векторы, оптимизируя предсказания соседних токенов, что приводит к их способности улавливать определённые семантические отношения между словами (см. Главу 16). Обучение векторов слов с целью предсказания тональности означает, что представления будут отражать, как токен связан с результатами, с которыми он ассоциируется.

Ноутбук [sentiment_analysis_imdb](05_sentiment_analysis_imdb.ipynb) демонстрирует, как применить модель RNN к текстовым данным для обнаружения положительной или отрицательной тональности (что можно легко расширить до более детальной шкалы тональности). Мы будем использовать векторные представления слов для представления токенов в документах. Мы рассмотрели векторные представления слов в [Главе 15, Word Embeddings](../15_word_embeddings). Они являются отличной техникой для преобразования текста в непрерывное векторное представление, так что относительное расположение слов в латентном пространстве кодирует полезные семантические аспекты на основе использования слов в контексте.

### Пример кода: анализ тональности с предобученными векторами слов

В [Главе 15, Word Embeddings](../15_word_embeddings) мы показали, как обучить специфичные для предметной области векторные представления слов. Word2vec и связанные алгоритмы обучения создают высококачественные векторы слов, но требуют больших наборов данных. Поэтому часто исследовательские группы делятся векторами слов, обученными на больших наборах данных, подобно весам для предобученных моделей глубокого обучения, которые мы встречали в разделе о трансферном обучении в [предыдущей главе](../17_convolutional_neural_nets).

Ноутбук [sentiment_analysis_pretrained_embeddings](06_sentiment_analysis_pretrained_embeddings.ipynb) демонстрирует, как использовать предобученные глобальные векторы для представления слов (GloVe), предоставленные группой Stanford NLP, с набором данных рецензий IMDB.

- [Large Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/), Stanford AI Group
- [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/), Stanford NLP

### Пример кода: SEC-отчёты для двунаправленной RNN GRU для прогнозирования недельной доходности

В Главе 16 мы обсудили важные различия между отзывами на продукты и финансовыми текстовыми данными. Хотя первые были полезны для иллюстрации важных рабочих процессов, в этом разделе мы рассмотрим более сложные, но и более релевантные финансовые документы.

Более конкретно, мы будем использовать данные SEC-отчётов, представленные в [Главе 16](../16_word_embeddings), для обучения векторных представлений слов, адаптированных для прогнозирования доходности тикера, связанного с раскрытием информации, от момента до публикации до одной недели после.

Ноутбук [sec_filings_return_prediction](07_sec_filings_return_prediction.ipynb) содержит примеры кода для этого приложения.

См. ноутбук [sec_preprocessing](../16_word_embeddings/06_sec_preprocessing.ipynb) в Главе 16 и инструкции в папке data на GitHub о том, как получить данные.
