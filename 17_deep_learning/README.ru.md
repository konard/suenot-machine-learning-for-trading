# Глубокое обучение для трейдинга

Эта глава открывает четвёртую часть книги, посвящённую применению различных методов глубокого обучения (Deep Learning, DL) в инвестировании и трейдинге. Глубокое обучение совершило множество прорывов в самых разных областях — от распознавания изображений и речи до робототехники и интеллектуальных агентов — привлекая широкое внимание и возрождая масштабные исследования в области искусственного интеллекта (ИИ). Ожидания высоки: стремительное развитие продолжится, и появится множество новых решений для сложных практических задач.

В этой главе мы представим нейронные сети прямого распространения (feedforward neural networks), чтобы познакомить вас с ключевыми элементами работы с нейронными сетями, актуальными для различных архитектур глубокого обучения, рассматриваемых в следующих главах. В частности, мы продемонстрируем, как эффективно обучать большие модели с использованием алгоритма обратного распространения ошибки (backpropagation) и управлять рисками переобучения. Мы также покажем, как использовать популярные фреймворки Keras, TensorFlow 2.0 и PyTorch, которые будем применять на протяжении всей четвёртой части.

В последующих главах мы будем опираться на эту основу для разработки различных архитектур, подходящих для разных инвестиционных приложений, с особым акцентом на альтернативные текстовые и графические данные. Они включают рекуррентные нейронные сети (RNN), адаптированные для последовательных данных, таких как временные ряды или естественный язык, и свёрточные нейронные сети (CNN), которые особенно хорошо подходят для работы с изображениями, но могут использоваться и с данными временных рядов. Мы также рассмотрим глубокое обучение без учителя, включая автоэнкодеры и генеративно-состязательные сети (GAN), а также обучение с подкреплением для тренировки агентов, которые интерактивно обучаются на основе взаимодействия с окружающей средой.

## Содержание

1. [Глубокое обучение: в чём отличия и почему это важно](#глубокое-обучение-в-чём-отличия-и-почему-это-важно)
    * [Как иерархические признаки помогают укрощать многомерные данные](#как-иерархические-признаки-помогают-укрощать-многомерные-данные)
    * [Автоматизация извлечения признаков: DL как обучение представлениям](#автоматизация-извлечения-признаков-dl-как-обучение-представлениям)
    * [Как DL связано с машинным обучением и искусственным интеллектом](#как-dl-связано-с-машинным-обучением-и-искусственным-интеллектом)
2. [Пример кода: проектирование нейронной сети](#пример-кода-проектирование-нейронной-сети)
    * [Ключевые проектные решения](#ключевые-проектные-решения)
    * [Как регуляризовать глубокие нейронные сети](#как-регуляризовать-глубокие-нейронные-сети)
    * [Ускорение обучения: оптимизации для глубокого обучения](#ускорение-обучения-оптимизации-для-глубокого-обучения)
3. [Популярные библиотеки глубокого обучения](#популярные-библиотеки-глубокого-обучения)
    * [Как использовать оптимизацию GPU](#как-использовать-оптимизацию-gpu)
    * [Как использовать Tensorboard](#как-использовать-tensorboard)
    * [Пример кода: как использовать PyTorch](#пример-кода-как-использовать-pytorch)
    * [Пример кода: как использовать TensorFlow](#пример-кода-как-использовать-tensorflow)
4. [Пример кода: оптимизация нейронной сети для long-short торговой стратегии](#пример-кода-оптимизация-нейронной-сети-для-long-short-торговой-стратегии)
    * [Оптимизация архитектуры нейронной сети](#оптимизация-архитектуры-нейронной-сети)
    * [Бэктестинг long-short стратегии на основе ансамблированных сигналов](#бэктестинг-long-short-стратегии-на-основе-ансамблированных-сигналов)


## Глубокое обучение: в чём отличия и почему это важно

Алгоритмы машинного обучения (ML), рассмотренные во второй части, хорошо работают на широком спектре важных задач, включая работу с текстовыми данными, как было продемонстрировано в третьей части. Однако они оказались менее успешными в решении центральных задач ИИ, таких как распознавание речи или классификация объектов на изображениях. Эти ограничения послужили стимулом для развития глубокого обучения, и недавние прорывы в DL значительно способствовали возрождению интереса к искусственному интеллекту.

Для всестороннего введения, которое включает и расширяет многие из рассматриваемых здесь тем, см. Goodfellow, Bengio, и Courville (2016), или для более краткой версии — LeCun, Bengio, и Hinton (2015).

- [Deep Learning](https://www.deeplearningbook.org/), Ian Goodfellow, Yoshua Bengio and Aaron Courville, MIT Press, 2016
- [Deep learning](https://www.nature.com/articles/nature14539), Yann LeCun, Yoshua Bengio, and Geoffrey Hinton, Nature 2015
- [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/), Michael A. Nielsen, Determination Press, 2015
- [The Quest for Artificial Intelligence - A History of Ideas and Achievements](https://ai.stanford.edu/~nilsson/QAI/qai.pdf), Nils J. Nilsson, Cambridge University Press, 2010
- [One Hundred Year Study on Artificial Intelligence (AI100)](https://ai100.stanford.edu/)
- [TensorFlow Playground](http://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.71056&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false), интерактивная браузерная платформа для глубокого обучения

### Как иерархические признаки помогают укрощать многомерные данные

Как обсуждалось на протяжении второй части, ключевая задача обучения с учителем — обобщение от обучающих данных к новым образцам. Обобщение становится экспоненциально сложнее по мере увеличения размерности данных. Мы столкнулись с коренными причинами этих трудностей как с «проклятием размерности» в главе 13 «Обучение без учителя: от data-driven факторов риска до иерархической паритетности рисков».

### Автоматизация извлечения признаков: DL как обучение представлениям

Многие задачи ИИ, такие как распознавание изображений или речи, требуют знаний о мире. Одна из ключевых проблем — закодировать эти знания так, чтобы компьютер мог их использовать. На протяжении десятилетий разработка систем машинного обучения требовала значительной экспертизы в предметной области для преобразования сырых данных (таких как пиксели изображений) во внутреннее представление, которое алгоритм обучения мог бы использовать для обнаружения или классификации паттернов.

### Как DL связано с машинным обучением и искусственным интеллектом

ИИ имеет долгую историю, уходящую как минимум в 1950-е годы как академическая дисциплина и гораздо дальше как предмет человеческих исследований, но пережил несколько волн спада и подъёма энтузиазма с тех пор (см. [The Quest for Artificial Intelligence](https://ai.stanford.edu/~nilsson/QAI/qai.pdf), Nilsson, 2009 для подробного обзора).
- ML — важное подполе с долгой историей в смежных дисциплинах, таких как статистика, и стало заметным в 1980-х годах.
- DL — форма обучения представлениям и само по себе является подполем ML.

## Пример кода: проектирование нейронной сети

Чтобы лучше понять, как работают нейронные сети, ноутбук [01_build_and_train_feedforward_nn](01_build_and_train_feedforward_nn.ipynb) формулирует простую архитектуру прямого распространения и вычисления прямого прохода с использованием матричной алгебры и реализует их с помощью NumPy — Python-эквивалента линейной алгебры.

<p align="center">
<img src="https://i.imgur.com/UKCr9zi.png" width="85%">
</p>

### Ключевые проектные решения

Некоторые проектные решения для нейронных сетей похожи на решения для других моделей обучения с учителем. Например, выход определяется типом задачи ML: регрессия, классификация или ранжирование. Исходя из выхода, нам нужно выбрать функцию потерь для измерения успеха и неудачи предсказаний, а также алгоритм, который оптимизирует параметры сети для минимизации потерь.

Специфичные для нейронных сетей решения включают количество слоёв и узлов на слой, связи между узлами разных слоёв и типы функций активации.

### Как регуляризовать глубокие нейронные сети

Обратной стороной способности нейронных сетей аппроксимировать произвольные функции является значительно повышенный риск переобучения. Лучшая защита от переобучения — обучение модели на большем наборе данных. Аугментация данных, например, путём создания слегка изменённых версий изображений, является мощным альтернативным подходом. Генерация синтетических финансовых обучающих данных для этой цели — активная область исследований, которую мы рассмотрим в [главе 21](../21_gans_for_synthetic_time_series).

### Ускорение обучения: оптимизации для глубокого обучения

Обратное распространение (backprop) относится к вычислению градиента функции потерь по отношению к внутренним параметрам, которые мы хотим обновить, и использованию этой информации для обновления значений параметров. Градиент полезен, потому что он указывает направление изменения параметров, которое вызывает максимальное увеличение функции потерь. Следовательно, корректировка параметров в соответствии с отрицательным градиентом производит оптимальное снижение потерь, по крайней мере для области, очень близкой к наблюдаемым образцам. См. Ruder (2016) для отличного обзора ключевых алгоритмов оптимизации градиентного спуска.

- [Gradient Checking & Advanced Optimization](http://ufldl.stanford.edu/wiki/index.php/Gradient_checking_and_advanced_optimization), Unsupervised Feature Learning and Deep Learning, Stanford University
- [An overview of gradient descent optimization algorithms](http://ruder.io/optimizing-gradient-descent/index.html#momentum), Sebastian Ruder, 2016

## Популярные библиотеки глубокого обучения

В настоящее время самыми популярными библиотеками глубокого обучения являются [TensorFlow](https://www.tensorflow.org/) (поддерживается Google) и [PyTorch](https://pytorch.org/) (поддерживается Facebook).

Разработка очень активна: PyTorch версии 1.4 и TensorFlow версии 2.2 (по состоянию на март 2020). TensorFlow 2.0 принял [Keras](https://keras.io/) в качестве своего основного интерфейса, фактически объединив обе библиотеки в одну.

Дополнительные варианты включают:

- [Microsoft Cognitive Toolkit (CNTK)](https://github.com/Microsoft/CNTK)
- [Caffe](http://caffe.berkeleyvision.org/)
- [Theano](http://www.deeplearning.net/software/theano/), разрабатывается в Университете Монреаля с 2007 года
- [Apache MXNet](https://mxnet.apache.org/), используется Amazon
- [Chainer](https://chainer.org/), разработан японской компанией Preferred Networks
- [Torch](http://torch.ch/), использует Lua, основа для PyTorch
- [Deeplearning4J](https://deeplearning4j.org/), использует Java

### Как использовать оптимизацию GPU

Все популярные библиотеки глубокого обучения поддерживают использование GPU, а некоторые также позволяют параллельное обучение на нескольких GPU. Наиболее распространённые типы GPU производятся NVIDIA, и конфигурация требует установки и настройки среды CUDA. Этот процесс продолжает развиваться и может быть довольно сложным в зависимости от вашей вычислительной среды.

Более простой способ использования GPU — через платформу виртуализации Docker. Существует множество образов, которые можно запустить в локальном контейнере, управляемом Docker, что позволяет обойти многие конфликты драйверов и версий, с которыми вы можете столкнуться в противном случае. TensorFlow предоставляет образы Docker на своём веб-сайте, которые также можно использовать с Keras.

- [Which GPU(s) to Get for Deep Learning: My Experience and Advice for Using GPUs in Deep Learning](http://timdettmers.com/2018/11/05/which-gpu-for-deep-learning/), Tim Dettmers
- [A Full Hardware Guide to Deep Learning](http://timdettmers.com/2018/12/16/deep-learning-hardware-guide/), Tim Dettmers

### Как использовать Tensorboard

Tensorboard — отличный инструмент визуализации, поставляемый с TensorFlow. Он включает набор инструментов визуализации для упрощения понимания, отладки и оптимизации нейронных сетей.

Вы можете использовать его для визуализации вычислительного графа, построения различных метрик выполнения и производительности, и даже визуализации данных изображений, обрабатываемых сетью. Он также позволяет сравнивать различные запуски обучения.

Когда вы запускаете ноутбук how_to_use_keras с установленным TensorFlow, вы можете запустить Tensorboard из командной строки:

```python
tensorboard --logdir=/полный_путь_к_вашим_логам ## например ./tensorboard
```

- [TensorBoard: Visualizing Learning](https://www.tensorflow.org/guide/summaries_and_tensorboard)

### Пример кода: как использовать PyTorch

PyTorch был разработан в группе Facebook AI Research под руководством Янна ЛеКуна, и первая альфа-версия была выпущена в сентябре 2016 года. Он обеспечивает глубокую интеграцию с библиотеками Python, такими как NumPy, которые можно использовать для расширения его функциональности, мощное ускорение GPU и автоматическое дифференцирование с помощью системы autograd. Он предоставляет более детальный контроль, чем Keras, через API более низкого уровня и в основном используется как платформа исследований глубокого обучения, но также может заменить NumPy, обеспечивая вычисления на GPU.

Он использует eager execution (немедленное выполнение), в отличие от статических вычислительных графов, используемых, например, Theano или TensorFlow. Вместо того чтобы изначально определять и компилировать сеть для быстрого, но статического выполнения, он полагается на свой пакет autograd для автоматического дифференцирования тензорных операций, то есть он вычисляет градиенты «на лету», так что структуры сети могут быть частично изменены более легко. Это называется define-by-run, что означает, что обратное распространение определяется тем, как выполняется ваш код, что, в свою очередь, подразумевает, что каждая итерация может быть разной. Документация PyTorch предоставляет подробное руководство по этому вопросу.

Ноутбук [how_to_use_pytorch](03_how_to_use_pytorch.ipynb) иллюстрирует использование версии 1.4.

- [PyTorch Documentation](https://pytorch.org/docs)
- [PyTorch Tutorials](https://pytorch.org/tutorials)
- [PyTorch Ecosystem](https://pytorch.org/ecosystem)
    - [AllenNLP](https://allennlp.org/), современная NLP-платформа, разработанная Allen Institute for Artificial Intelligence
    - [Flair](https://github.com/zalandoresearch/flair), простой фреймворк для современного NLP, разработанный в Zalando
    - [fast.ai](http://www.fast.ai/), упрощает обучение нейронных сетей с использованием современных лучших практик; предлагает онлайн-обучение

### Пример кода: как использовать TensorFlow

TensorFlow стал ведущей библиотекой глубокого обучения вскоре после своего выпуска в сентябре 2015 года, за год до PyTorch. TensorFlow 2.0 стремится упростить API, который со временем становился всё более сложным, сделав API Keras, интегрированный в TensorFlow как часть пакета contrib с 2017 года, своим основным интерфейсом и приняв eager execution. Он продолжит фокусироваться на надёжной реализации на множестве платформ, но упростит эксперименты и исследования.

Ноутбук [how_to_use_tensorflow](02_how_to_use_tensorflow.ipynb) иллюстрирует использование версии 2.0.

- [TensorFlow.org](https://www.tensorflow.org/)
- [Standardizing on Keras: Guidance on High-level APIs in TensorFlow 2.0](https://medium.com/tensorflow/standardizing-on-keras-guidance-on-high-level-apis-in-tensorflow-2-0-bad2b04c819a)
- [TensorFlow.js](https://js.tensorflow.org/), JavaScript-библиотека для обучения и развёртывания ML-моделей в браузере и на Node.js

## Пример кода: оптимизация нейронной сети для long-short торговой стратегии

На практике нам нужно исследовать вариации проектных решений для архитектуры нейронной сети и способа её обучения из тех, что мы описали ранее, потому что мы никогда не можем быть уверены заранее, какая конфигурация лучше всего подходит для данных.

Этот пример кода исследует различные архитектуры простой нейронной сети прямого распространения для предсказания ежедневных доходностей акций с использованием набора данных, разработанного в [главе 12](../12_gradient_boosting_machines) (см. ноутбук [preparing_the_model_data](../12_gradient_boosting_machines/04_preparing_the_model_data.ipynb)).

Для этого мы определим функцию, которая возвращает модель TensorFlow на основе нескольких архитектурных входных параметров и кросс-валидируем альтернативные дизайны с использованием MultipleTimeSeriesCV, который мы представили в главе 7. Чтобы оценить качество сигнала предсказаний модели, мы построим простую стратегию long-short на основе ранжирования, основанную на ансамбле моделей, которые лучше всего работают в течение периода кросс-валидации in-sample. Чтобы ограничить риск ложных открытий, мы затем оценим производительность этой стратегии для периода тестирования out-of-sample.

### Оптимизация архитектуры нейронной сети

Ноутбук [how_to_optimize_a_NN_architecture](04_optimizing_a_NN_architecture_for_trading.ipynb) исследует различные варианты построения простой нейронной сети прямого распространения для предсказания доходности активов. Для разработки нашей торговой стратегии мы используем ежедневные доходности акций для 995 американских акций за восьмилетний период с 2010 по 2017 год.

### Бэктестинг long-short стратегии на основе ансамблированных сигналов

Чтобы преобразовать нашу модель нейронной сети в торговую стратегию, мы генерируем предсказания, оцениваем качество их сигнала, создаём правила, определяющие, как торговать на основе этих предсказаний, и проводим бэктестинг производительности стратегии, которая реализует эти правила.

Ноутбук [backtesting_with_zipline](05_backtesting_with_zipline.ipynb) содержит примеры кода для этого раздела.
