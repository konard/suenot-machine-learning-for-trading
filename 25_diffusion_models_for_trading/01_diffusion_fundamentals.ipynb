{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion Model Fundamentals\n",
    "\n",
    "This notebook introduces the core concepts of diffusion models for time series:\n",
    "\n",
    "1. Forward diffusion process (adding noise)\n",
    "2. Reverse diffusion process (denoising)\n",
    "3. Noise schedules (linear, cosine, sigmoid)\n",
    "4. Loss functions and training objectives\n",
    "5. Simple 1D example with financial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Intuition Behind Diffusion Models\n",
    "\n",
    "Diffusion models work by:\n",
    "1. **Forward process**: Gradually add noise to data until it becomes pure Gaussian noise\n",
    "2. **Reverse process**: Learn to undo each noise step, recovering original data\n",
    "\n",
    "The key insight is that it's easier to learn many small denoising steps than to generate data in one shot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample financial time series\n",
    "def generate_sample_returns(n_samples=1000, n_timesteps=100):\n",
    "    \"\"\"Generate synthetic stock returns with realistic properties.\"\"\"\n",
    "    # Parameters\n",
    "    mu = 0.0001  # Daily drift\n",
    "    sigma = 0.02  # Daily volatility\n",
    "    \n",
    "    # Generate returns with volatility clustering (GARCH-like)\n",
    "    returns = np.zeros((n_samples, n_timesteps))\n",
    "    for i in range(n_samples):\n",
    "        vol = sigma\n",
    "        for t in range(n_timesteps):\n",
    "            # Simple volatility clustering\n",
    "            if t > 0:\n",
    "                vol = 0.9 * vol + 0.1 * sigma * (1 + abs(returns[i, t-1]) / sigma)\n",
    "            returns[i, t] = mu + vol * np.random.randn()\n",
    "    \n",
    "    return returns\n",
    "\n",
    "# Generate data\n",
    "returns = generate_sample_returns(n_samples=5000, n_timesteps=50)\n",
    "print(f\"Generated returns shape: {returns.shape}\")\n",
    "\n",
    "# Visualize a few samples\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Sample trajectories\n",
    "for i in range(5):\n",
    "    axes[0].plot(np.cumsum(returns[i]), alpha=0.7, label=f'Sample {i+1}')\n",
    "axes[0].set_title('Sample Cumulative Returns')\n",
    "axes[0].set_xlabel('Time')\n",
    "axes[0].set_ylabel('Cumulative Return')\n",
    "axes[0].legend()\n",
    "\n",
    "# Distribution of returns\n",
    "axes[1].hist(returns.flatten(), bins=100, density=True, alpha=0.7)\n",
    "axes[1].set_title('Distribution of Returns')\n",
    "axes[1].set_xlabel('Return')\n",
    "axes[1].set_ylabel('Density')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Forward Diffusion Process\n",
    "\n",
    "The forward process is defined as:\n",
    "\n",
    "$$q(x_t | x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1-\\beta_t} x_{t-1}, \\beta_t I)$$\n",
    "\n",
    "Key property - we can sample $x_t$ directly from $x_0$:\n",
    "\n",
    "$$q(x_t | x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t} x_0, (1-\\bar{\\alpha}_t) I)$$\n",
    "\n",
    "where $\\bar{\\alpha}_t = \\prod_{s=1}^{t} (1-\\beta_s)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoiseSchedule:\n",
    "    \"\"\"Different noise schedules for diffusion models.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_timesteps=1000, schedule_type='linear'):\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.schedule_type = schedule_type\n",
    "        \n",
    "        # Compute betas based on schedule type\n",
    "        if schedule_type == 'linear':\n",
    "            self.betas = self._linear_schedule()\n",
    "        elif schedule_type == 'cosine':\n",
    "            self.betas = self._cosine_schedule()\n",
    "        elif schedule_type == 'sigmoid':\n",
    "            self.betas = self._sigmoid_schedule()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown schedule: {schedule_type}\")\n",
    "        \n",
    "        # Compute derived quantities\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alphas_cumprod = np.cumprod(self.alphas)\n",
    "        self.sqrt_alphas_cumprod = np.sqrt(self.alphas_cumprod)\n",
    "        self.sqrt_one_minus_alphas_cumprod = np.sqrt(1.0 - self.alphas_cumprod)\n",
    "    \n",
    "    def _linear_schedule(self, beta_start=0.0001, beta_end=0.02):\n",
    "        \"\"\"Linear noise schedule.\"\"\"\n",
    "        return np.linspace(beta_start, beta_end, self.num_timesteps)\n",
    "    \n",
    "    def _cosine_schedule(self, s=0.008):\n",
    "        \"\"\"Cosine noise schedule (better for smaller sequences).\"\"\"\n",
    "        steps = self.num_timesteps + 1\n",
    "        t = np.linspace(0, self.num_timesteps, steps) / self.num_timesteps\n",
    "        alphas_cumprod = np.cos((t + s) / (1 + s) * np.pi / 2) ** 2\n",
    "        alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "        betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "        return np.clip(betas, 0.0001, 0.9999)\n",
    "    \n",
    "    def _sigmoid_schedule(self, beta_start=0.0001, beta_end=0.02):\n",
    "        \"\"\"Sigmoid noise schedule.\"\"\"\n",
    "        t = np.linspace(-6, 6, self.num_timesteps)\n",
    "        sigmoid = 1 / (1 + np.exp(-t))\n",
    "        return sigmoid * (beta_end - beta_start) + beta_start\n",
    "    \n",
    "    def add_noise(self, x_0, t, noise=None):\n",
    "        \"\"\"Add noise to data at timestep t.\"\"\"\n",
    "        if noise is None:\n",
    "            noise = np.random.randn(*x_0.shape)\n",
    "        \n",
    "        sqrt_alpha = self.sqrt_alphas_cumprod[t]\n",
    "        sqrt_one_minus_alpha = self.sqrt_one_minus_alphas_cumprod[t]\n",
    "        \n",
    "        return sqrt_alpha * x_0 + sqrt_one_minus_alpha * noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different noise schedules\n",
    "schedules = {\n",
    "    'linear': NoiseSchedule(1000, 'linear'),\n",
    "    'cosine': NoiseSchedule(1000, 'cosine'),\n",
    "    'sigmoid': NoiseSchedule(1000, 'sigmoid')\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Plot betas\n",
    "for name, schedule in schedules.items():\n",
    "    axes[0].plot(schedule.betas, label=name)\n",
    "axes[0].set_title('Noise Schedule (β_t)')\n",
    "axes[0].set_xlabel('Timestep')\n",
    "axes[0].set_ylabel('β')\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot cumulative alphas\n",
    "for name, schedule in schedules.items():\n",
    "    axes[1].plot(schedule.alphas_cumprod, label=name)\n",
    "axes[1].set_title('Cumulative Alpha (ᾱ_t)')\n",
    "axes[1].set_xlabel('Timestep')\n",
    "axes[1].set_ylabel('ᾱ')\n",
    "axes[1].legend()\n",
    "\n",
    "# Plot signal-to-noise ratio\n",
    "for name, schedule in schedules.items():\n",
    "    snr = schedule.alphas_cumprod / (1 - schedule.alphas_cumprod)\n",
    "    axes[2].plot(np.log(snr), label=name)\n",
    "axes[2].set_title('Log Signal-to-Noise Ratio')\n",
    "axes[2].set_xlabel('Timestep')\n",
    "axes[2].set_ylabel('log(SNR)')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the forward diffusion process\n",
    "schedule = NoiseSchedule(1000, 'cosine')\n",
    "\n",
    "# Take one sample return series\n",
    "x_0 = returns[0]\n",
    "\n",
    "# Show noising at different timesteps\n",
    "timesteps = [0, 100, 250, 500, 750, 999]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, t in enumerate(timesteps):\n",
    "    if t == 0:\n",
    "        x_t = x_0\n",
    "    else:\n",
    "        x_t = schedule.add_noise(x_0, t)\n",
    "    \n",
    "    axes[i].plot(np.cumsum(x_t), color='blue', alpha=0.8)\n",
    "    axes[i].set_title(f't = {t} (SNR = {schedule.alphas_cumprod[t]/(1-schedule.alphas_cumprod[t]+1e-8):.2f})')\n",
    "    axes[i].set_xlabel('Time')\n",
    "    axes[i].set_ylabel('Cumulative Return')\n",
    "    axes[i].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.suptitle('Forward Diffusion Process: Adding Noise', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reverse Diffusion Process (Denoising)\n",
    "\n",
    "The reverse process learns to denoise:\n",
    "\n",
    "$$p_\\theta(x_{t-1} | x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t))$$\n",
    "\n",
    "We train a neural network to predict the noise $\\epsilon_\\theta(x_t, t)$ added at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    \"\"\"Sinusoidal embeddings for timestep encoding.\"\"\"\n",
    "    \n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "    \n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = np.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat([torch.sin(embeddings), torch.cos(embeddings)], dim=-1)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class SimpleDenoiser(nn.Module):\n",
    "    \"\"\"Simple MLP-based denoiser for 1D time series.\"\"\"\n",
    "    \n",
    "    def __init__(self, seq_length, hidden_dim=256, time_emb_dim=64):\n",
    "        super().__init__()\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        # Time embedding\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(time_emb_dim),\n",
    "            nn.Linear(time_emb_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Main network\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(seq_length + hidden_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, seq_length)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        \"\"\"Predict noise given noisy input x and timestep t.\"\"\"\n",
    "        # Get time embedding\n",
    "        t_emb = self.time_mlp(t)\n",
    "        \n",
    "        # Concatenate input with time embedding\n",
    "        x_combined = torch.cat([x, t_emb], dim=-1)\n",
    "        \n",
    "        # Predict noise\n",
    "        return self.net(x_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training the Diffusion Model\n",
    "\n",
    "Training objective (simplified):\n",
    "\n",
    "$$\\mathcal{L} = \\mathbb{E}_{t, x_0, \\epsilon} \\left[ \\| \\epsilon - \\epsilon_\\theta(x_t, t) \\|^2 \\right]$$\n",
    "\n",
    "Algorithm:\n",
    "1. Sample $x_0$ from data\n",
    "2. Sample $t \\sim \\text{Uniform}(1, T)$\n",
    "3. Sample $\\epsilon \\sim \\mathcal{N}(0, I)$\n",
    "4. Compute $x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1-\\bar{\\alpha}_t} \\epsilon$\n",
    "5. Minimize $\\| \\epsilon - \\epsilon_\\theta(x_t, t) \\|^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionTrainer:\n",
    "    \"\"\"Training loop for diffusion models.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, schedule, lr=1e-4):\n",
    "        self.model = model\n",
    "        self.schedule = schedule\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        \n",
    "        # Convert schedule to tensors\n",
    "        self.sqrt_alphas_cumprod = torch.tensor(\n",
    "            schedule.sqrt_alphas_cumprod, dtype=torch.float32\n",
    "        ).to(device)\n",
    "        self.sqrt_one_minus_alphas_cumprod = torch.tensor(\n",
    "            schedule.sqrt_one_minus_alphas_cumprod, dtype=torch.float32\n",
    "        ).to(device)\n",
    "    \n",
    "    def get_noisy_sample(self, x_0, t, noise=None):\n",
    "        \"\"\"Add noise to sample at timestep t.\"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_0)\n",
    "        \n",
    "        sqrt_alpha = self.sqrt_alphas_cumprod[t][:, None]\n",
    "        sqrt_one_minus_alpha = self.sqrt_one_minus_alphas_cumprod[t][:, None]\n",
    "        \n",
    "        return sqrt_alpha * x_0 + sqrt_one_minus_alpha * noise\n",
    "    \n",
    "    def train_step(self, x_0):\n",
    "        \"\"\"Single training step.\"\"\"\n",
    "        batch_size = x_0.shape[0]\n",
    "        \n",
    "        # Sample random timesteps\n",
    "        t = torch.randint(0, self.schedule.num_timesteps, (batch_size,), device=device)\n",
    "        \n",
    "        # Sample noise\n",
    "        noise = torch.randn_like(x_0)\n",
    "        \n",
    "        # Get noisy samples\n",
    "        x_t = self.get_noisy_sample(x_0, t, noise)\n",
    "        \n",
    "        # Predict noise\n",
    "        noise_pred = self.model(x_t, t.float())\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(noise_pred, noise)\n",
    "        \n",
    "        # Backprop\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def train(self, dataloader, epochs=100):\n",
    "        \"\"\"Full training loop.\"\"\"\n",
    "        losses = []\n",
    "        \n",
    "        for epoch in tqdm(range(epochs), desc=\"Training\"):\n",
    "            epoch_losses = []\n",
    "            for batch in dataloader:\n",
    "                x_0 = batch[0].to(device)\n",
    "                loss = self.train_step(x_0)\n",
    "                epoch_losses.append(loss)\n",
    "            \n",
    "            avg_loss = np.mean(epoch_losses)\n",
    "            losses.append(avg_loss)\n",
    "            \n",
    "            if (epoch + 1) % 20 == 0:\n",
    "                print(f\"Epoch {epoch+1}: Loss = {avg_loss:.6f}\")\n",
    "        \n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X = torch.tensor(returns, dtype=torch.float32)\n",
    "dataset = TensorDataset(X)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Initialize model and trainer\n",
    "seq_length = returns.shape[1]\n",
    "model = SimpleDenoiser(seq_length, hidden_dim=256, time_emb_dim=64).to(device)\n",
    "schedule = NoiseSchedule(1000, 'cosine')\n",
    "trainer = DiffusionTrainer(model, schedule, lr=1e-4)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "losses = trainer.train(dataloader, epochs=100)\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sampling from the Diffusion Model\n",
    "\n",
    "To generate new samples, we start from pure noise and iteratively denoise:\n",
    "\n",
    "$$x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}} \\epsilon_\\theta(x_t, t) \\right) + \\sigma_t z$$\n",
    "\n",
    "where $z \\sim \\mathcal{N}(0, I)$ and $\\sigma_t = \\sqrt{\\beta_t}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample(model, schedule, n_samples, seq_length, device):\n",
    "    \"\"\"Generate samples using DDPM sampling.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Convert schedule parameters to tensors\n",
    "    betas = torch.tensor(schedule.betas, dtype=torch.float32, device=device)\n",
    "    alphas = torch.tensor(schedule.alphas, dtype=torch.float32, device=device)\n",
    "    alphas_cumprod = torch.tensor(schedule.alphas_cumprod, dtype=torch.float32, device=device)\n",
    "    \n",
    "    # Start from pure noise\n",
    "    x = torch.randn(n_samples, seq_length, device=device)\n",
    "    \n",
    "    # Iteratively denoise\n",
    "    for t in tqdm(reversed(range(schedule.num_timesteps)), desc=\"Sampling\", total=schedule.num_timesteps):\n",
    "        t_tensor = torch.full((n_samples,), t, device=device, dtype=torch.float32)\n",
    "        \n",
    "        # Predict noise\n",
    "        noise_pred = model(x, t_tensor)\n",
    "        \n",
    "        # Compute coefficients\n",
    "        alpha = alphas[t]\n",
    "        alpha_cumprod = alphas_cumprod[t]\n",
    "        beta = betas[t]\n",
    "        \n",
    "        # Denoise\n",
    "        x = (1 / torch.sqrt(alpha)) * (x - (beta / torch.sqrt(1 - alpha_cumprod)) * noise_pred)\n",
    "        \n",
    "        # Add noise (except for last step)\n",
    "        if t > 0:\n",
    "            noise = torch.randn_like(x)\n",
    "            x = x + torch.sqrt(beta) * noise\n",
    "    \n",
    "    return x.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples\n",
    "generated_samples = sample(model, schedule, n_samples=100, seq_length=seq_length, device=device)\n",
    "print(f\"Generated samples shape: {generated_samples.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare real vs generated samples\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "# Real samples - trajectories\n",
    "for i in range(5):\n",
    "    axes[0, 0].plot(np.cumsum(returns[i]), alpha=0.7)\n",
    "axes[0, 0].set_title('Real: Sample Trajectories')\n",
    "axes[0, 0].set_xlabel('Time')\n",
    "axes[0, 0].set_ylabel('Cumulative Return')\n",
    "\n",
    "# Generated samples - trajectories\n",
    "for i in range(5):\n",
    "    axes[0, 1].plot(np.cumsum(generated_samples[i]), alpha=0.7)\n",
    "axes[0, 1].set_title('Generated: Sample Trajectories')\n",
    "axes[0, 1].set_xlabel('Time')\n",
    "axes[0, 1].set_ylabel('Cumulative Return')\n",
    "\n",
    "# Distribution comparison\n",
    "axes[0, 2].hist(returns.flatten(), bins=100, density=True, alpha=0.5, label='Real')\n",
    "axes[0, 2].hist(generated_samples.flatten(), bins=100, density=True, alpha=0.5, label='Generated')\n",
    "axes[0, 2].set_title('Return Distribution')\n",
    "axes[0, 2].legend()\n",
    "\n",
    "# Autocorrelation comparison\n",
    "def compute_autocorr(data, max_lag=20):\n",
    "    autocorrs = []\n",
    "    for lag in range(max_lag):\n",
    "        if lag == 0:\n",
    "            autocorrs.append(1.0)\n",
    "        else:\n",
    "            corr = np.corrcoef(data[:-lag].flatten(), data[lag:].flatten())[0, 1]\n",
    "            autocorrs.append(corr)\n",
    "    return autocorrs\n",
    "\n",
    "real_acf = compute_autocorr(returns)\n",
    "gen_acf = compute_autocorr(generated_samples)\n",
    "\n",
    "axes[1, 0].bar(range(20), real_acf, alpha=0.5, label='Real')\n",
    "axes[1, 0].bar(range(20), gen_acf, alpha=0.5, label='Generated')\n",
    "axes[1, 0].set_title('Autocorrelation')\n",
    "axes[1, 0].set_xlabel('Lag')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Volatility comparison (rolling std)\n",
    "real_vol = np.std(returns, axis=1)\n",
    "gen_vol = np.std(generated_samples, axis=1)\n",
    "\n",
    "axes[1, 1].hist(real_vol, bins=50, density=True, alpha=0.5, label='Real')\n",
    "axes[1, 1].hist(gen_vol, bins=50, density=True, alpha=0.5, label='Generated')\n",
    "axes[1, 1].set_title('Volatility Distribution')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "# QQ plot\n",
    "from scipy import stats\n",
    "real_sorted = np.sort(returns.flatten())\n",
    "gen_sorted = np.sort(generated_samples.flatten())\n",
    "# Subsample for QQ plot\n",
    "idx = np.linspace(0, len(real_sorted)-1, 1000).astype(int)\n",
    "axes[1, 2].scatter(real_sorted[idx], gen_sorted[idx], alpha=0.3, s=10)\n",
    "axes[1, 2].plot([real_sorted.min(), real_sorted.max()], \n",
    "                [real_sorted.min(), real_sorted.max()], 'r--')\n",
    "axes[1, 2].set_title('Q-Q Plot')\n",
    "axes[1, 2].set_xlabel('Real Quantiles')\n",
    "axes[1, 2].set_ylabel('Generated Quantiles')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Key Takeaways\n",
    "\n",
    "1. **Diffusion models** learn to generate data by reversing a gradual noising process\n",
    "2. **Forward process** adds Gaussian noise according to a schedule (linear, cosine, sigmoid)\n",
    "3. **Reverse process** learns to denoise using a neural network that predicts the added noise\n",
    "4. **Training** minimizes the MSE between predicted and actual noise\n",
    "5. **Sampling** iteratively denoises from pure noise to generate new samples\n",
    "\n",
    "### Advantages over GANs:\n",
    "- More stable training (no adversarial dynamics)\n",
    "- Better mode coverage\n",
    "- Natural uncertainty quantification\n",
    "\n",
    "### Limitations:\n",
    "- Slow sampling (1000 denoising steps)\n",
    "- Computationally expensive\n",
    "\n",
    "### Next Steps:\n",
    "- See notebook 02 for full DDPM implementation with U-Net\n",
    "- See notebook 03 for TimeGrad applied to cryptocurrency forecasting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
