{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSDI: Conditional Score-based Diffusion для Imputation и Forecasting\n",
    "\n",
    "CSDI (Conditional Score-based Diffusion for Imputation) - это non-autoregressive модель, которая может выполнять:\n",
    "\n",
    "1. **Imputation (заполнение пропусков)**: Восстановление недостающих значений во временном ряду\n",
    "2. **Forecasting (прогнозирование)**: Генерация будущих значений как \"заполнение\" будущих timesteps\n",
    "3. **Interpolation**: Восстановление значений между известными точками\n",
    "\n",
    "## Ключевые преимущества CSDI:\n",
    "\n",
    "- **Non-autoregressive**: Генерирует все значения одновременно (нет накопления ошибок)\n",
    "- **Conditional**: Учитывает известные значения как условие\n",
    "- **Гибкость**: Работает с произвольными паттернами пропусков\n",
    "\n",
    "**Статья**: Tashiro et al., \"CSDI: Conditional Score-based Diffusion Models for Probabilistic Time Series Imputation\" (NeurIPS 2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Установка зависимостей\n",
    "!pip install torch numpy pandas matplotlib seaborn scikit-learn tqdm einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, Optional, Dict, List\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange, repeat\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Генерация синтетических данных с пропусками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_crypto_like_data(\n",
    "    n_samples: int = 1000,\n",
    "    n_features: int = 5,\n",
    "    seq_len: int = 100\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Генерация синтетических данных, похожих на криптовалютные.\n",
    "    \n",
    "    Returns:\n",
    "        data: [n_samples, seq_len, n_features]\n",
    "        masks: [n_samples, seq_len, n_features] - 1 где данные есть, 0 где пропуски\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Базовый тренд (GBM)\n",
    "    dt = 1/24  # почасовые данные\n",
    "    mu = 0.0001\n",
    "    sigma = 0.02\n",
    "    \n",
    "    data = np.zeros((n_samples, seq_len, n_features))\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Close price (feature 0)\n",
    "        returns = np.random.normal(mu * dt, sigma * np.sqrt(dt), seq_len)\n",
    "        prices = 100 * np.exp(np.cumsum(returns))\n",
    "        data[i, :, 0] = prices\n",
    "        \n",
    "        # Volume (feature 1) - зависит от волатильности\n",
    "        vol = np.abs(returns) * 100\n",
    "        data[i, :, 1] = np.random.exponential(1 + vol) * 1000\n",
    "        \n",
    "        # RSI-like (feature 2)\n",
    "        gains = np.maximum(returns, 0)\n",
    "        losses = np.maximum(-returns, 0)\n",
    "        avg_gain = pd.Series(gains).rolling(14, min_periods=1).mean().values\n",
    "        avg_loss = pd.Series(losses).rolling(14, min_periods=1).mean().values\n",
    "        rs = avg_gain / (avg_loss + 1e-10)\n",
    "        data[i, :, 2] = 100 - 100 / (1 + rs)\n",
    "        \n",
    "        # Volatility (feature 3)\n",
    "        data[i, :, 3] = pd.Series(returns).rolling(24, min_periods=1).std().values * np.sqrt(24) * 100\n",
    "        \n",
    "        # Momentum (feature 4)\n",
    "        data[i, :, 4] = pd.Series(prices).pct_change(12).fillna(0).values * 100\n",
    "    \n",
    "    # Нормализация\n",
    "    for f in range(n_features):\n",
    "        mean = data[:, :, f].mean()\n",
    "        std = data[:, :, f].std()\n",
    "        data[:, :, f] = (data[:, :, f] - mean) / (std + 1e-10)\n",
    "    \n",
    "    # Создаём маски пропусков\n",
    "    masks = np.ones_like(data)\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Случайные пропуски (10-30% данных)\n",
    "        missing_rate = np.random.uniform(0.1, 0.3)\n",
    "        n_missing = int(seq_len * n_features * missing_rate)\n",
    "        \n",
    "        # Случайные позиции для пропусков\n",
    "        flat_indices = np.random.choice(\n",
    "            seq_len * n_features, \n",
    "            size=n_missing, \n",
    "            replace=False\n",
    "        )\n",
    "        \n",
    "        for idx in flat_indices:\n",
    "            t = idx // n_features\n",
    "            f = idx % n_features\n",
    "            masks[i, t, f] = 0\n",
    "    \n",
    "    return data, masks\n",
    "\n",
    "# Генерируем данные\n",
    "data, masks = generate_crypto_like_data(n_samples=500, n_features=5, seq_len=100)\n",
    "\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "print(f\"Masks shape: {masks.shape}\")\n",
    "print(f\"Средняя доля пропусков: {1 - masks.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация пропусков\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "\n",
    "sample_idx = 0\n",
    "\n",
    "# Маска пропусков\n",
    "ax1 = axes[0, 0]\n",
    "im = ax1.imshow(masks[sample_idx].T, aspect='auto', cmap='RdYlGn', vmin=0, vmax=1)\n",
    "ax1.set_xlabel('Time')\n",
    "ax1.set_ylabel('Feature')\n",
    "ax1.set_title('Missing Data Mask (green=observed, red=missing)')\n",
    "ax1.set_yticks(range(5))\n",
    "ax1.set_yticklabels(['Close', 'Volume', 'RSI', 'Volatility', 'Momentum'])\n",
    "plt.colorbar(im, ax=ax1)\n",
    "\n",
    "# Данные с пропусками (Close price)\n",
    "ax2 = axes[0, 1]\n",
    "observed_data = data[sample_idx, :, 0].copy()\n",
    "observed_data[masks[sample_idx, :, 0] == 0] = np.nan\n",
    "ax2.plot(observed_data, 'b.-', markersize=2)\n",
    "ax2.set_xlabel('Time')\n",
    "ax2.set_ylabel('Normalized Price')\n",
    "ax2.set_title('Close Price with Missing Values')\n",
    "\n",
    "# Статистика пропусков по фичам\n",
    "ax3 = axes[1, 0]\n",
    "missing_by_feature = 1 - masks.mean(axis=(0, 1))\n",
    "features = ['Close', 'Volume', 'RSI', 'Volatility', 'Momentum']\n",
    "ax3.bar(features, missing_by_feature)\n",
    "ax3.set_ylabel('Missing Rate')\n",
    "ax3.set_title('Missing Rate by Feature')\n",
    "\n",
    "# Статистика пропусков по времени\n",
    "ax4 = axes[1, 1]\n",
    "missing_by_time = 1 - masks.mean(axis=(0, 2))\n",
    "ax4.plot(missing_by_time)\n",
    "ax4.set_xlabel('Time')\n",
    "ax4.set_ylabel('Missing Rate')\n",
    "ax4.set_title('Missing Rate Over Time')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CSDI Architecture\n",
    "\n",
    "CSDI использует score-based подход с кондиционированием на наблюдаемые значения.\n",
    "\n",
    "Основные компоненты:\n",
    "1. **Diffusion Embedding**: Кодирование шага диффузии\n",
    "2. **Feature Embedding**: Кодирование признаков\n",
    "3. **Bidirectional Transformer**: Обработка последовательности\n",
    "4. **Conditional Mechanism**: Учёт известных значений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CSDIConfig:\n",
    "    \"\"\"Конфигурация CSDI.\"\"\"\n",
    "    seq_len: int = 100\n",
    "    n_features: int = 5\n",
    "    hidden_dim: int = 64\n",
    "    num_heads: int = 4\n",
    "    num_layers: int = 4\n",
    "    diffusion_steps: int = 50\n",
    "    beta_start: float = 0.0001\n",
    "    beta_end: float = 0.5\n",
    "    schedule: str = 'quad'  # 'linear', 'quad', 'cosine'\n",
    "\n",
    "config = CSDIConfig()\n",
    "print(f\"Config: {config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionEmbedding(nn.Module):\n",
    "    \"\"\"Кодирование шага диффузии.\"\"\"\n",
    "    \n",
    "    def __init__(self, dim: int, max_steps: int = 500):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.max_steps = max_steps\n",
    "        \n",
    "        # Предвычисляем embeddings\n",
    "        self.register_buffer(\n",
    "            'embedding', \n",
    "            self._build_embedding(max_steps, dim)\n",
    "        )\n",
    "        \n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 2),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(dim * 2, dim)\n",
    "        )\n",
    "    \n",
    "    def _build_embedding(self, max_steps: int, dim: int) -> torch.Tensor:\n",
    "        \"\"\"Синусоидальное кодирование.\"\"\"\n",
    "        steps = torch.arange(max_steps).unsqueeze(1)\n",
    "        frequencies = 10000 ** (-torch.arange(0, dim, 2).float() / dim)\n",
    "        \n",
    "        encoding = torch.zeros(max_steps, dim)\n",
    "        encoding[:, 0::2] = torch.sin(steps * frequencies)\n",
    "        encoding[:, 1::2] = torch.cos(steps * frequencies)\n",
    "        \n",
    "        return encoding\n",
    "    \n",
    "    def forward(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            t: [batch] - шаги времени\n",
    "        Returns:\n",
    "            [batch, dim]\n",
    "        \"\"\"\n",
    "        t = t.long().clamp(0, self.max_steps - 1)\n",
    "        emb = self.embedding[t]\n",
    "        return self.projection(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Позиционное кодирование для временного ряда.\"\"\"\n",
    "    \n",
    "    def __init__(self, dim: int, max_len: int = 500):\n",
    "        super().__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_len, dim)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, dim, 2).float() * -(np.log(10000.0) / dim))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch, seq_len, dim]\n",
    "        \"\"\"\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalResidualBlock(nn.Module):\n",
    "    \"\"\"Residual block с кондиционированием на время диффузии.\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim: int, diffusion_dim: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.linear1 = nn.Linear(hidden_dim, hidden_dim * 2)\n",
    "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Projection для условия времени\n",
    "        self.cond_proj = nn.Linear(diffusion_dim, hidden_dim * 2)\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        x: torch.Tensor, \n",
    "        cond: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch, seq_len, hidden_dim]\n",
    "            cond: [batch, diffusion_dim]\n",
    "        \"\"\"\n",
    "        # Условие → scale и shift\n",
    "        cond = self.cond_proj(cond)  # [batch, hidden_dim * 2]\n",
    "        scale, shift = cond.chunk(2, dim=-1)  # каждый [batch, hidden_dim]\n",
    "        \n",
    "        # Первый слой с AdaIN\n",
    "        h = self.norm1(x)\n",
    "        h = h * (1 + scale.unsqueeze(1)) + shift.unsqueeze(1)\n",
    "        h = self.linear1(h)\n",
    "        h = F.gelu(h)\n",
    "        h = h[..., :h.size(-1)//2] * torch.sigmoid(h[..., h.size(-1)//2:])  # GLU\n",
    "        \n",
    "        # Второй слой\n",
    "        h = self.norm2(h)\n",
    "        h = self.linear2(h)\n",
    "        \n",
    "        return x + h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Transformer block с кондиционированием.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        hidden_dim: int, \n",
    "        num_heads: int, \n",
    "        diffusion_dim: int,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Self-attention\n",
    "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            hidden_dim, \n",
    "            num_heads, \n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Feedforward с кондиционированием\n",
    "        self.res_block = ConditionalResidualBlock(hidden_dim, diffusion_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        x: torch.Tensor, \n",
    "        cond: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch, seq_len, hidden_dim]\n",
    "            cond: [batch, diffusion_dim]\n",
    "            mask: [batch, seq_len] - attention mask\n",
    "        \"\"\"\n",
    "        # Self-attention\n",
    "        h = self.norm1(x)\n",
    "        h, _ = self.attn(h, h, h, key_padding_mask=mask)\n",
    "        x = x + self.dropout(h)\n",
    "        \n",
    "        # Conditional feedforward\n",
    "        x = self.res_block(x, cond)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSDI(nn.Module):\n",
    "    \"\"\"CSDI: Conditional Score-based Diffusion for Imputation.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: CSDIConfig):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.config = config\n",
    "        \n",
    "        # Input projection\n",
    "        # Вход: (значение, маска) для каждой фичи\n",
    "        self.input_proj = nn.Linear(config.n_features * 2, config.hidden_dim)\n",
    "        \n",
    "        # Diffusion embedding\n",
    "        self.diff_embed = DiffusionEmbedding(config.hidden_dim, config.diffusion_steps)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_enc = PositionalEncoding(config.hidden_dim, config.seq_len)\n",
    "        \n",
    "        # Transformer layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(\n",
    "                config.hidden_dim, \n",
    "                config.num_heads, \n",
    "                config.hidden_dim\n",
    "            )\n",
    "            for _ in range(config.num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_proj = nn.Sequential(\n",
    "            nn.LayerNorm(config.hidden_dim),\n",
    "            nn.Linear(config.hidden_dim, config.hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(config.hidden_dim, config.n_features)\n",
    "        )\n",
    "        \n",
    "        # Noise schedule\n",
    "        self._setup_noise_schedule()\n",
    "    \n",
    "    def _setup_noise_schedule(self):\n",
    "        \"\"\"Настройка noise schedule.\"\"\"\n",
    "        T = self.config.diffusion_steps\n",
    "        \n",
    "        if self.config.schedule == 'linear':\n",
    "            betas = torch.linspace(self.config.beta_start, self.config.beta_end, T)\n",
    "        elif self.config.schedule == 'quad':\n",
    "            betas = torch.linspace(self.config.beta_start**0.5, self.config.beta_end**0.5, T) ** 2\n",
    "        elif self.config.schedule == 'cosine':\n",
    "            steps = torch.arange(T + 1) / T\n",
    "            alphas_bar = torch.cos((steps + 0.008) / 1.008 * np.pi / 2) ** 2\n",
    "            betas = 1 - alphas_bar[1:] / alphas_bar[:-1]\n",
    "            betas = betas.clamp(max=0.999)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown schedule: {self.config.schedule}\")\n",
    "        \n",
    "        alphas = 1 - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "        \n",
    "        self.register_buffer('betas', betas)\n",
    "        self.register_buffer('alphas', alphas)\n",
    "        self.register_buffer('alphas_cumprod', alphas_cumprod)\n",
    "        self.register_buffer('sqrt_alphas_cumprod', torch.sqrt(alphas_cumprod))\n",
    "        self.register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1 - alphas_cumprod))\n",
    "        self.register_buffer('sqrt_recip_alphas', torch.sqrt(1 / alphas))\n",
    "    \n",
    "    def forward_diffusion(\n",
    "        self,\n",
    "        x0: torch.Tensor,\n",
    "        t: torch.Tensor,\n",
    "        noise: Optional[torch.Tensor] = None\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Прямой диффузионный процесс.\n",
    "        \n",
    "        Args:\n",
    "            x0: [batch, seq_len, n_features] - чистые данные\n",
    "            t: [batch] - шаги времени\n",
    "        Returns:\n",
    "            xt, noise\n",
    "        \"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x0)\n",
    "        \n",
    "        sqrt_alpha = self.sqrt_alphas_cumprod[t].view(-1, 1, 1)\n",
    "        sqrt_one_minus_alpha = self.sqrt_one_minus_alphas_cumprod[t].view(-1, 1, 1)\n",
    "        \n",
    "        xt = sqrt_alpha * x0 + sqrt_one_minus_alpha * noise\n",
    "        \n",
    "        return xt, noise\n",
    "    \n",
    "    def denoise(\n",
    "        self,\n",
    "        xt: torch.Tensor,\n",
    "        t: torch.Tensor,\n",
    "        observed: torch.Tensor,\n",
    "        mask: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Предсказание шума.\n",
    "        \n",
    "        Args:\n",
    "            xt: [batch, seq_len, n_features] - зашумлённые данные\n",
    "            t: [batch] - шаги диффузии\n",
    "            observed: [batch, seq_len, n_features] - наблюдаемые значения\n",
    "            mask: [batch, seq_len, n_features] - маска (1=observed, 0=missing)\n",
    "        \"\"\"\n",
    "        batch_size = xt.shape[0]\n",
    "        \n",
    "        # Объединяем условие и зашумлённые данные\n",
    "        # Для наблюдаемых точек используем observed, для пропусков - xt\n",
    "        conditioned = observed * mask + xt * (1 - mask)\n",
    "        \n",
    "        # Конкатенируем значения и маску\n",
    "        x_input = torch.cat([conditioned, mask], dim=-1)  # [batch, seq_len, n_features*2]\n",
    "        \n",
    "        # Input projection\n",
    "        h = self.input_proj(x_input)  # [batch, seq_len, hidden_dim]\n",
    "        \n",
    "        # Positional encoding\n",
    "        h = self.pos_enc(h)\n",
    "        \n",
    "        # Diffusion embedding\n",
    "        diff_emb = self.diff_embed(t.float())  # [batch, hidden_dim]\n",
    "        \n",
    "        # Transformer layers\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, diff_emb)\n",
    "        \n",
    "        # Output\n",
    "        noise_pred = self.output_proj(h)  # [batch, seq_len, n_features]\n",
    "        \n",
    "        return noise_pred\n",
    "    \n",
    "    def compute_loss(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        mask: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Вычислить loss для обучения.\n",
    "        \n",
    "        Args:\n",
    "            x: [batch, seq_len, n_features] - полные данные\n",
    "            mask: [batch, seq_len, n_features] - маска наблюдаемых\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        device = x.device\n",
    "        \n",
    "        # Случайный шаг диффузии\n",
    "        t = torch.randint(0, self.config.diffusion_steps, (batch_size,), device=device)\n",
    "        \n",
    "        # Прямая диффузия\n",
    "        xt, noise = self.forward_diffusion(x, t)\n",
    "        \n",
    "        # Предсказание шума\n",
    "        # Наблюдаемые данные - это x * mask (значения там, где есть данные)\n",
    "        noise_pred = self.denoise(xt, t, x * mask, mask)\n",
    "        \n",
    "        # Loss только на пропущенных значениях\n",
    "        target_mask = 1 - mask  # инвертируем - хотим предсказывать пропуски\n",
    "        \n",
    "        loss = F.mse_loss(noise_pred * target_mask, noise * target_mask, reduction='sum')\n",
    "        loss = loss / (target_mask.sum() + 1e-10)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def sample(\n",
    "        self,\n",
    "        observed: torch.Tensor,\n",
    "        mask: torch.Tensor,\n",
    "        num_samples: int = 1\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Генерация сэмплов (imputation).\n",
    "        \n",
    "        Args:\n",
    "            observed: [batch, seq_len, n_features] - наблюдаемые данные\n",
    "            mask: [batch, seq_len, n_features] - маска\n",
    "            num_samples: количество сэмплов\n",
    "            \n",
    "        Returns:\n",
    "            samples: [batch, num_samples, seq_len, n_features]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, n_features = observed.shape\n",
    "        device = observed.device\n",
    "        \n",
    "        all_samples = []\n",
    "        \n",
    "        for _ in range(num_samples):\n",
    "            # Начинаем с шума для пропущенных позиций\n",
    "            x = torch.randn(batch_size, seq_len, n_features, device=device)\n",
    "            \n",
    "            # Наблюдаемые значения остаются фиксированными\n",
    "            x = observed * mask + x * (1 - mask)\n",
    "            \n",
    "            # Обратный процесс\n",
    "            for t in reversed(range(self.config.diffusion_steps)):\n",
    "                t_tensor = torch.full((batch_size,), t, device=device)\n",
    "                \n",
    "                # Предсказываем шум\n",
    "                noise_pred = self.denoise(x, t_tensor, observed, mask)\n",
    "                \n",
    "                # DDPM update (только для пропусков)\n",
    "                alpha = self.alphas[t]\n",
    "                alpha_cumprod = self.alphas_cumprod[t]\n",
    "                beta = self.betas[t]\n",
    "                \n",
    "                # x_{t-1} = 1/sqrt(alpha) * (x_t - beta/sqrt(1-alpha_bar) * noise_pred) + sigma * z\n",
    "                coef1 = 1 / torch.sqrt(alpha)\n",
    "                coef2 = beta / torch.sqrt(1 - alpha_cumprod)\n",
    "                \n",
    "                mean = coef1 * (x - coef2 * noise_pred)\n",
    "                \n",
    "                if t > 0:\n",
    "                    # Posterior variance\n",
    "                    alpha_cumprod_prev = self.alphas_cumprod[t - 1]\n",
    "                    variance = beta * (1 - alpha_cumprod_prev) / (1 - alpha_cumprod)\n",
    "                    noise = torch.randn_like(x)\n",
    "                    x = mean + torch.sqrt(variance) * noise\n",
    "                else:\n",
    "                    x = mean\n",
    "                \n",
    "                # Сохраняем наблюдаемые значения\n",
    "                x = observed * mask + x * (1 - mask)\n",
    "            \n",
    "            all_samples.append(x)\n",
    "        \n",
    "        return torch.stack(all_samples, dim=1)  # [batch, num_samples, seq_len, n_features]\n",
    "\n",
    "# Создаём модель\n",
    "model = CSDI(config).to(device)\n",
    "print(f\"Параметров: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Подготовка данных и обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ImputationDataset(Dataset):\n",
    "    def __init__(self, data: np.ndarray, masks: np.ndarray):\n",
    "        self.data = torch.FloatTensor(data)\n",
    "        self.masks = torch.FloatTensor(masks)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.masks[idx]\n",
    "\n",
    "# Train/val/test split\n",
    "n = len(data)\n",
    "train_size = int(0.7 * n)\n",
    "val_size = int(0.15 * n)\n",
    "\n",
    "train_data, train_masks = data[:train_size], masks[:train_size]\n",
    "val_data, val_masks = data[train_size:train_size+val_size], masks[train_size:train_size+val_size]\n",
    "test_data, test_masks = data[train_size+val_size:], masks[train_size+val_size:]\n",
    "\n",
    "train_dataset = ImputationDataset(train_data, train_masks)\n",
    "val_dataset = ImputationDataset(val_data, val_masks)\n",
    "test_dataset = ImputationDataset(test_data, test_masks)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for data, mask in loader:\n",
    "        data = data.to(device)\n",
    "        mask = mask.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = model.compute_loss(data, mask)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "def validate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, mask in loader:\n",
    "            data = data.to(device)\n",
    "            mask = mask.to(device)\n",
    "            loss = model.compute_loss(data, mask)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)\n",
    "\n",
    "n_epochs = 30\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in tqdm(range(n_epochs), desc=\"Training\"):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "    val_loss = validate(model, val_loader, device)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_state = model.state_dict().copy()\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1}: Train={train_loss:.4f}, Val={val_loss:.4f}\")\n",
    "\n",
    "model.load_state_dict(best_state)\n",
    "print(f\"\\nBest Val Loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# График обучения\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('CSDI Training Progress')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Imputation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_imputation(\n",
    "    model: CSDI,\n",
    "    data: np.ndarray,\n",
    "    masks: np.ndarray,\n",
    "    num_samples: int = 10\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Оценка качества imputation.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    data_tensor = torch.FloatTensor(data).to(device)\n",
    "    mask_tensor = torch.FloatTensor(masks).to(device)\n",
    "    \n",
    "    # Наблюдаемые данные\n",
    "    observed = data_tensor * mask_tensor\n",
    "    \n",
    "    # Генерируем сэмплы\n",
    "    with torch.no_grad():\n",
    "        samples = model.sample(observed, mask_tensor, num_samples)\n",
    "    \n",
    "    # Среднее предсказание\n",
    "    mean_pred = samples.mean(dim=1)  # [batch, seq_len, n_features]\n",
    "    \n",
    "    # Метрики только на пропущенных значениях\n",
    "    missing_mask = 1 - mask_tensor\n",
    "    \n",
    "    # MSE\n",
    "    mse = ((mean_pred - data_tensor) ** 2 * missing_mask).sum() / missing_mask.sum()\n",
    "    rmse = torch.sqrt(mse)\n",
    "    \n",
    "    # MAE\n",
    "    mae = (torch.abs(mean_pred - data_tensor) * missing_mask).sum() / missing_mask.sum()\n",
    "    \n",
    "    # CRPS (simplified)\n",
    "    samples_np = samples.cpu().numpy()\n",
    "    data_np = data_tensor.cpu().numpy()\n",
    "    missing_np = missing_mask.cpu().numpy()\n",
    "    \n",
    "    # Approximate CRPS\n",
    "    crps = np.mean(np.abs(samples_np - data_np[:, np.newaxis, :, :]) * missing_np[:, np.newaxis, :, :])\n",
    "    \n",
    "    return {\n",
    "        'RMSE': rmse.item(),\n",
    "        'MAE': mae.item(),\n",
    "        'CRPS': crps,\n",
    "        'samples': samples.cpu().numpy(),\n",
    "        'mean_pred': mean_pred.cpu().numpy()\n",
    "    }\n",
    "\n",
    "# Оценка на тестовых данных\n",
    "results = evaluate_imputation(model, test_data, test_masks, num_samples=10)\n",
    "\n",
    "print(\"=== CSDI Imputation Results ===\")\n",
    "print(f\"RMSE: {results['RMSE']:.4f}\")\n",
    "print(f\"MAE: {results['MAE']:.4f}\")\n",
    "print(f\"CRPS: {results['CRPS']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация imputation\n",
    "fig, axes = plt.subplots(3, 2, figsize=(14, 10))\n",
    "\n",
    "for i, ax_row in enumerate(axes):\n",
    "    sample_idx = i\n",
    "    feature_idx = 0  # Close price\n",
    "    \n",
    "    # Левый график: Close price\n",
    "    ax = ax_row[0]\n",
    "    \n",
    "    # Истинные значения\n",
    "    true_values = test_data[sample_idx, :, feature_idx]\n",
    "    \n",
    "    # Наблюдаемые\n",
    "    observed = true_values.copy()\n",
    "    observed[test_masks[sample_idx, :, feature_idx] == 0] = np.nan\n",
    "    \n",
    "    # Предсказанные\n",
    "    pred_mean = results['mean_pred'][sample_idx, :, feature_idx]\n",
    "    samples = results['samples'][sample_idx, :, :, feature_idx]  # [num_samples, seq_len]\n",
    "    pred_std = samples.std(axis=0)\n",
    "    \n",
    "    time = np.arange(len(true_values))\n",
    "    \n",
    "    # CI для пропусков\n",
    "    missing_mask = test_masks[sample_idx, :, feature_idx] == 0\n",
    "    ax.fill_between(\n",
    "        time[missing_mask],\n",
    "        pred_mean[missing_mask] - 2 * pred_std[missing_mask],\n",
    "        pred_mean[missing_mask] + 2 * pred_std[missing_mask],\n",
    "        alpha=0.3,\n",
    "        color='orange',\n",
    "        label='95% CI'\n",
    "    )\n",
    "    \n",
    "    ax.plot(time, true_values, 'g--', alpha=0.5, label='True (hidden)')\n",
    "    ax.scatter(time[~missing_mask], observed[~missing_mask], c='blue', s=10, label='Observed')\n",
    "    ax.plot(time[missing_mask], pred_mean[missing_mask], 'ro', markersize=3, label='Imputed')\n",
    "    \n",
    "    ax.set_title(f'Sample {i+1}: Close Price')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Value')\n",
    "    if i == 0:\n",
    "        ax.legend(loc='upper right', fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Правый график: другая фича\n",
    "    ax = ax_row[1]\n",
    "    feature_idx = 2  # RSI\n",
    "    \n",
    "    true_values = test_data[sample_idx, :, feature_idx]\n",
    "    observed = true_values.copy()\n",
    "    observed[test_masks[sample_idx, :, feature_idx] == 0] = np.nan\n",
    "    pred_mean = results['mean_pred'][sample_idx, :, feature_idx]\n",
    "    \n",
    "    missing_mask = test_masks[sample_idx, :, feature_idx] == 0\n",
    "    \n",
    "    ax.plot(time, true_values, 'g--', alpha=0.5, label='True')\n",
    "    ax.scatter(time[~missing_mask], observed[~missing_mask], c='blue', s=10, label='Observed')\n",
    "    ax.plot(time[missing_mask], pred_mean[missing_mask], 'ro', markersize=3, label='Imputed')\n",
    "    \n",
    "    ax.set_title(f'Sample {i+1}: RSI')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. CSDI для Forecasting\n",
    "\n",
    "Для прогнозирования мы просто маскируем будущие timesteps и используем imputation для их заполнения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_with_csdi(\n",
    "    model: CSDI,\n",
    "    history: np.ndarray,\n",
    "    forecast_horizon: int,\n",
    "    num_samples: int = 50\n",
    ") -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Прогнозирование с помощью CSDI.\n",
    "    \n",
    "    Args:\n",
    "        model: обученная CSDI модель\n",
    "        history: [seq_len, n_features] - исторические данные\n",
    "        forecast_horizon: горизонт прогноза\n",
    "        num_samples: количество сэмплов\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    seq_len, n_features = history.shape\n",
    "    total_len = seq_len + forecast_horizon\n",
    "    \n",
    "    # Дополняем нулями для будущих значений\n",
    "    padded_data = np.zeros((total_len, n_features))\n",
    "    padded_data[:seq_len] = history\n",
    "    \n",
    "    # Маска: 1 для истории, 0 для прогноза\n",
    "    mask = np.zeros((total_len, n_features))\n",
    "    mask[:seq_len] = 1\n",
    "    \n",
    "    # Если модель обучена на фиксированной длине, обрезаем или дополняем\n",
    "    if total_len != model.config.seq_len:\n",
    "        # Используем последние seq_len точек истории + прогноз\n",
    "        if total_len > model.config.seq_len:\n",
    "            start_idx = total_len - model.config.seq_len\n",
    "            padded_data = padded_data[start_idx:]\n",
    "            mask = mask[start_idx:]\n",
    "        else:\n",
    "            # Дополняем нулями в начале\n",
    "            pad_len = model.config.seq_len - total_len\n",
    "            padded_data = np.pad(padded_data, ((pad_len, 0), (0, 0)))\n",
    "            mask = np.pad(mask, ((pad_len, 0), (0, 0)))\n",
    "    \n",
    "    # Конвертируем в тензоры\n",
    "    data_tensor = torch.FloatTensor(padded_data).unsqueeze(0).to(device)\n",
    "    mask_tensor = torch.FloatTensor(mask).unsqueeze(0).to(device)\n",
    "    \n",
    "    observed = data_tensor * mask_tensor\n",
    "    \n",
    "    # Генерируем сэмплы\n",
    "    with torch.no_grad():\n",
    "        samples = model.sample(observed, mask_tensor, num_samples)\n",
    "    \n",
    "    samples_np = samples.cpu().numpy()[0]  # [num_samples, seq_len, n_features]\n",
    "    \n",
    "    # Извлекаем прогноз (последние forecast_horizon точек)\n",
    "    forecast_samples = samples_np[:, -forecast_horizon:, :]\n",
    "    \n",
    "    return {\n",
    "        'samples': forecast_samples,\n",
    "        'mean': forecast_samples.mean(axis=0),\n",
    "        'std': forecast_samples.std(axis=0),\n",
    "        'q10': np.percentile(forecast_samples, 10, axis=0),\n",
    "        'q50': np.percentile(forecast_samples, 50, axis=0),\n",
    "        'q90': np.percentile(forecast_samples, 90, axis=0),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тест прогнозирования\n",
    "# Берём историю из тестовых данных\n",
    "test_sample = test_data[0]\n",
    "history_len = 80\n",
    "forecast_horizon = 20\n",
    "\n",
    "history = test_sample[:history_len]\n",
    "actual_future = test_sample[history_len:history_len + forecast_horizon]\n",
    "\n",
    "forecast = forecast_with_csdi(model, history, forecast_horizon, num_samples=50)\n",
    "\n",
    "print(f\"Forecast shape: {forecast['mean'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация прогноза\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "\n",
    "features = ['Close', 'Volume', 'RSI', 'Volatility']\n",
    "\n",
    "for idx, (ax, feature_name) in enumerate(zip(axes.flatten(), features)):\n",
    "    # История\n",
    "    time_history = np.arange(history_len)\n",
    "    time_forecast = np.arange(history_len, history_len + forecast_horizon)\n",
    "    \n",
    "    ax.plot(time_history, history[:, idx], 'b-', label='History')\n",
    "    \n",
    "    # Прогноз\n",
    "    ax.fill_between(\n",
    "        time_forecast,\n",
    "        forecast['q10'][:, idx],\n",
    "        forecast['q90'][:, idx],\n",
    "        alpha=0.3,\n",
    "        color='orange',\n",
    "        label='80% CI'\n",
    "    )\n",
    "    ax.plot(time_forecast, forecast['mean'][:, idx], 'orange', linewidth=2, label='Forecast')\n",
    "    \n",
    "    # Истинные значения (если есть)\n",
    "    ax.plot(time_forecast, actual_future[:, idx], 'g--', linewidth=2, label='Actual')\n",
    "    \n",
    "    ax.axvline(history_len, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax.set_title(f'{feature_name}')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.legend(loc='upper right', fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('CSDI Forecasting Results', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Сравнение с базовыми методами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanImputer:\n",
    "    \"\"\"Заполнение пропусков средним значением.\"\"\"\n",
    "    \n",
    "    def impute(self, data: np.ndarray, mask: np.ndarray) -> np.ndarray:\n",
    "        result = data.copy()\n",
    "        for f in range(data.shape[-1]):\n",
    "            feature_data = data[..., f]\n",
    "            feature_mask = mask[..., f]\n",
    "            mean_val = np.mean(feature_data[feature_mask == 1])\n",
    "            result[..., f] = np.where(feature_mask == 1, feature_data, mean_val)\n",
    "        return result\n",
    "\n",
    "\n",
    "class LinearInterpolator:\n",
    "    \"\"\"Линейная интерполяция.\"\"\"\n",
    "    \n",
    "    def impute(self, data: np.ndarray, mask: np.ndarray) -> np.ndarray:\n",
    "        result = data.copy()\n",
    "        for i in range(data.shape[0]):  # для каждого сэмпла\n",
    "            for f in range(data.shape[-1]):  # для каждой фичи\n",
    "                series = pd.Series(data[i, :, f])\n",
    "                series[mask[i, :, f] == 0] = np.nan\n",
    "                result[i, :, f] = series.interpolate(method='linear').fillna(method='bfill').fillna(method='ffill').values\n",
    "        return result\n",
    "\n",
    "\n",
    "# Сравнение методов\n",
    "mean_imputer = MeanImputer()\n",
    "linear_imputer = LinearInterpolator()\n",
    "\n",
    "mean_imputed = mean_imputer.impute(test_data, test_masks)\n",
    "linear_imputed = linear_imputer.impute(test_data, test_masks)\n",
    "\n",
    "# Метрики\n",
    "missing_mask = 1 - test_masks\n",
    "\n",
    "def compute_rmse(pred, true, mask):\n",
    "    return np.sqrt(np.sum((pred - true) ** 2 * mask) / np.sum(mask))\n",
    "\n",
    "def compute_mae(pred, true, mask):\n",
    "    return np.sum(np.abs(pred - true) * mask) / np.sum(mask)\n",
    "\n",
    "print(\"=== Сравнение методов Imputation ===\")\n",
    "print(f\"\\n{'Method':<20} {'RMSE':<10} {'MAE':<10}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(f\"{'Mean Imputation':<20} {compute_rmse(mean_imputed, test_data, missing_mask):<10.4f} {compute_mae(mean_imputed, test_data, missing_mask):<10.4f}\")\n",
    "print(f\"{'Linear Interpolation':<20} {compute_rmse(linear_imputed, test_data, missing_mask):<10.4f} {compute_mae(linear_imputed, test_data, missing_mask):<10.4f}\")\n",
    "print(f\"{'CSDI':<20} {results['RMSE']:<10.4f} {results['MAE']:<10.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация сравнения\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "sample_idx = 0\n",
    "feature_idx = 0\n",
    "\n",
    "time = np.arange(100)\n",
    "true_values = test_data[sample_idx, :, feature_idx]\n",
    "mask = test_masks[sample_idx, :, feature_idx]\n",
    "\n",
    "for ax, (method_name, imputed) in zip(axes, [\n",
    "    ('Mean Imputation', mean_imputed[sample_idx, :, feature_idx]),\n",
    "    ('Linear Interpolation', linear_imputed[sample_idx, :, feature_idx]),\n",
    "    ('CSDI', results['mean_pred'][sample_idx, :, feature_idx])\n",
    "]):\n",
    "    ax.plot(time, true_values, 'g--', alpha=0.5, label='True')\n",
    "    ax.scatter(time[mask == 1], true_values[mask == 1], c='blue', s=10, label='Observed')\n",
    "    ax.scatter(time[mask == 0], imputed[mask == 0], c='red', s=15, marker='x', label='Imputed')\n",
    "    \n",
    "    # Ошибка\n",
    "    rmse = np.sqrt(np.mean((imputed[mask == 0] - true_values[mask == 0]) ** 2))\n",
    "    \n",
    "    ax.set_title(f'{method_name}\\nRMSE: {rmse:.4f}')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.legend(loc='upper right', fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Выводы\n",
    "\n",
    "### Преимущества CSDI:\n",
    "\n",
    "1. **Универсальность**: Работает как для imputation, так и для forecasting\n",
    "2. **Вероятностные предсказания**: Генерирует распределение возможных значений\n",
    "3. **Non-autoregressive**: Нет накопления ошибок\n",
    "4. **Гибкость**: Работает с произвольными паттернами пропусков\n",
    "\n",
    "### Ограничения:\n",
    "\n",
    "1. **Вычислительная сложность**: Требует много памяти для длинных последовательностей\n",
    "2. **Фиксированная длина**: Модель обучена на фиксированной seq_len\n",
    "3. **Медленный sampling**: DDPM sampling требует много шагов\n",
    "\n",
    "### Рекомендации:\n",
    "\n",
    "- Используйте DDIM sampling для ускорения генерации\n",
    "- Для forecasting рассмотрите комбинацию с TimeGrad\n",
    "- Для очень длинных последовательностей рассмотрите Diffusion-TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение модели\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'config': config,\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses,\n",
    "}, 'csdi_model.pt')\n",
    "\n",
    "print(\"Model saved to csdi_model.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
