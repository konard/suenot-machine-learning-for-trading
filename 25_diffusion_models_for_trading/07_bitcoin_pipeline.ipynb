{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Bitcoin Forecasting Pipeline with Diffusion Models\n",
    "\n",
    "This notebook implements a production-ready pipeline for Bitcoin price forecasting using diffusion models:\n",
    "\n",
    "1. Data fetching from cryptocurrency exchanges\n",
    "2. Feature engineering (technical indicators)\n",
    "3. DDPM model for probabilistic forecasting\n",
    "4. Monte Carlo uncertainty estimation\n",
    "5. Backtesting with realistic constraints\n",
    "\n",
    "Based on the article: [Diffusion Models vs Cryptocurrency Anarchy](https://marketmaker.cc/en/blog/post/diffusion-models-cryptocurrency-prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Fetching from Bybit\n",
    "\n",
    "We fetch historical OHLCV data from Bybit exchange using their public API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BybitDataFetcher:\n",
    "    \"\"\"Fetch historical cryptocurrency data from Bybit.\"\"\"\n",
    "    \n",
    "    BASE_URL = \"https://api.bybit.com/v5/market/kline\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "    \n",
    "    def fetch_klines(self, symbol=\"BTCUSDT\", interval=\"60\", limit=1000, \n",
    "                     start_time=None, end_time=None):\n",
    "        \"\"\"\n",
    "        Fetch kline/candlestick data.\n",
    "        \n",
    "        Args:\n",
    "            symbol: Trading pair (e.g., BTCUSDT)\n",
    "            interval: Kline interval (1, 5, 15, 30, 60, 120, 240, D, W)\n",
    "            limit: Number of records (max 1000)\n",
    "            start_time: Start timestamp in milliseconds\n",
    "            end_time: End timestamp in milliseconds\n",
    "        \"\"\"\n",
    "        params = {\n",
    "            \"category\": \"linear\",\n",
    "            \"symbol\": symbol,\n",
    "            \"interval\": interval,\n",
    "            \"limit\": limit\n",
    "        }\n",
    "        \n",
    "        if start_time:\n",
    "            params[\"start\"] = start_time\n",
    "        if end_time:\n",
    "            params[\"end\"] = end_time\n",
    "        \n",
    "        response = self.session.get(self.BASE_URL, params=params)\n",
    "        data = response.json()\n",
    "        \n",
    "        if data[\"retCode\"] != 0:\n",
    "            raise Exception(f\"API Error: {data['retMsg']}\")\n",
    "        \n",
    "        return data[\"result\"][\"list\"]\n",
    "    \n",
    "    def fetch_historical_data(self, symbol=\"BTCUSDT\", interval=\"60\", days=30):\n",
    "        \"\"\"\n",
    "        Fetch historical data for specified number of days.\n",
    "        \"\"\"\n",
    "        all_data = []\n",
    "        end_time = int(datetime.now().timestamp() * 1000)\n",
    "        \n",
    "        # Calculate how many requests we need\n",
    "        interval_minutes = int(interval) if interval.isdigit() else 1440  # D = 1440\n",
    "        total_candles = (days * 24 * 60) // interval_minutes\n",
    "        \n",
    "        with tqdm(total=total_candles, desc=\"Fetching data\") as pbar:\n",
    "            while len(all_data) < total_candles:\n",
    "                batch = self.fetch_klines(\n",
    "                    symbol=symbol,\n",
    "                    interval=interval,\n",
    "                    limit=1000,\n",
    "                    end_time=end_time\n",
    "                )\n",
    "                \n",
    "                if not batch:\n",
    "                    break\n",
    "                \n",
    "                all_data.extend(batch)\n",
    "                pbar.update(len(batch))\n",
    "                \n",
    "                # Update end_time for next batch\n",
    "                end_time = int(batch[-1][0]) - 1\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(all_data, columns=[\n",
    "            'timestamp', 'open', 'high', 'low', 'close', 'volume', 'turnover'\n",
    "        ])\n",
    "        \n",
    "        # Convert types\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'].astype(int), unit='ms')\n",
    "        for col in ['open', 'high', 'low', 'close', 'volume', 'turnover']:\n",
    "            df[col] = df[col].astype(float)\n",
    "        \n",
    "        # Sort by timestamp\n",
    "        df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch Bitcoin hourly data\n",
    "fetcher = BybitDataFetcher()\n",
    "\n",
    "try:\n",
    "    btc_data = fetcher.fetch_historical_data(symbol=\"BTCUSDT\", interval=\"60\", days=90)\n",
    "    print(f\"Fetched {len(btc_data)} hourly candles\")\n",
    "    print(f\"Date range: {btc_data['timestamp'].min()} to {btc_data['timestamp'].max()}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not fetch live data: {e}\")\n",
    "    print(\"Generating synthetic data for demonstration...\")\n",
    "    \n",
    "    # Generate synthetic BTC-like data\n",
    "    n_samples = 90 * 24  # 90 days of hourly data\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Start price\n",
    "    price = 50000\n",
    "    prices = [price]\n",
    "    volumes = []\n",
    "    \n",
    "    for i in range(n_samples - 1):\n",
    "        # Simulate returns with volatility clustering\n",
    "        returns = np.random.normal(0.0001, 0.015)  # ~1.5% hourly vol\n",
    "        price = price * (1 + returns)\n",
    "        prices.append(price)\n",
    "        volumes.append(np.random.exponential(1000))\n",
    "    \n",
    "    volumes.append(np.random.exponential(1000))\n",
    "    \n",
    "    btc_data = pd.DataFrame({\n",
    "        'timestamp': pd.date_range(end=datetime.now(), periods=n_samples, freq='H'),\n",
    "        'open': prices,\n",
    "        'high': [p * (1 + abs(np.random.normal(0, 0.005))) for p in prices],\n",
    "        'low': [p * (1 - abs(np.random.normal(0, 0.005))) for p in prices],\n",
    "        'close': prices,\n",
    "        'volume': volumes\n",
    "    })\n",
    "\n",
    "btc_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True)\n",
    "\n",
    "# Price\n",
    "axes[0].plot(btc_data['timestamp'], btc_data['close'], color='blue', linewidth=0.8)\n",
    "axes[0].set_ylabel('Price (USDT)')\n",
    "axes[0].set_title('BTC/USDT Hourly Price')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Volume\n",
    "axes[1].bar(btc_data['timestamp'], btc_data['volume'], color='gray', alpha=0.7, width=0.03)\n",
    "axes[1].set_ylabel('Volume')\n",
    "axes[1].set_title('Trading Volume')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Returns\n",
    "returns = btc_data['close'].pct_change().dropna()\n",
    "axes[2].plot(btc_data['timestamp'][1:], returns.values, color='green', linewidth=0.5)\n",
    "axes[2].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "axes[2].set_ylabel('Returns')\n",
    "axes[2].set_title('Hourly Returns')\n",
    "axes[2].set_xlabel('Date')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering\n",
    "\n",
    "We compute technical indicators commonly used in cryptocurrency trading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEngineer:\n",
    "    \"\"\"Compute technical indicators for cryptocurrency data.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_rsi(prices, period=14):\n",
    "        \"\"\"Relative Strength Index.\"\"\"\n",
    "        delta = prices.diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
    "        rs = gain / (loss + 1e-10)\n",
    "        return 100 - (100 / (1 + rs))\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_macd(prices, fast=12, slow=26, signal=9):\n",
    "        \"\"\"MACD indicator.\"\"\"\n",
    "        ema_fast = prices.ewm(span=fast, adjust=False).mean()\n",
    "        ema_slow = prices.ewm(span=slow, adjust=False).mean()\n",
    "        macd_line = ema_fast - ema_slow\n",
    "        signal_line = macd_line.ewm(span=signal, adjust=False).mean()\n",
    "        histogram = macd_line - signal_line\n",
    "        return macd_line, signal_line, histogram\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_bollinger_bands(prices, period=20, num_std=2):\n",
    "        \"\"\"Bollinger Bands.\"\"\"\n",
    "        sma = prices.rolling(window=period).mean()\n",
    "        std = prices.rolling(window=period).std()\n",
    "        upper = sma + (std * num_std)\n",
    "        lower = sma - (std * num_std)\n",
    "        percent_b = (prices - lower) / (upper - lower + 1e-10)\n",
    "        return upper, lower, percent_b\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_atr(high, low, close, period=14):\n",
    "        \"\"\"Average True Range.\"\"\"\n",
    "        tr1 = high - low\n",
    "        tr2 = abs(high - close.shift())\n",
    "        tr3 = abs(low - close.shift())\n",
    "        tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
    "        return tr.rolling(window=period).mean()\n",
    "    \n",
    "    def compute_all_features(self, df):\n",
    "        \"\"\"Compute all features.\"\"\"\n",
    "        features = df.copy()\n",
    "        \n",
    "        # Returns\n",
    "        features['returns'] = features['close'].pct_change()\n",
    "        features['log_returns'] = np.log(features['close'] / features['close'].shift(1))\n",
    "        \n",
    "        # Volatility\n",
    "        features['volatility_24h'] = features['returns'].rolling(24).std()\n",
    "        features['volatility_7d'] = features['returns'].rolling(24*7).std()\n",
    "        \n",
    "        # RSI\n",
    "        features['rsi_14'] = self.compute_rsi(features['close'], 14)\n",
    "        features['rsi_7'] = self.compute_rsi(features['close'], 7)\n",
    "        \n",
    "        # MACD\n",
    "        macd, signal, hist = self.compute_macd(features['close'])\n",
    "        features['macd'] = macd\n",
    "        features['macd_signal'] = signal\n",
    "        features['macd_hist'] = hist\n",
    "        \n",
    "        # Bollinger Bands\n",
    "        upper, lower, pct_b = self.compute_bollinger_bands(features['close'])\n",
    "        features['bb_upper'] = upper\n",
    "        features['bb_lower'] = lower\n",
    "        features['bb_pct'] = pct_b\n",
    "        \n",
    "        # ATR\n",
    "        features['atr'] = self.compute_atr(\n",
    "            features['high'], features['low'], features['close']\n",
    "        )\n",
    "        \n",
    "        # Volume features\n",
    "        features['volume_sma'] = features['volume'].rolling(24).mean()\n",
    "        features['volume_ratio'] = features['volume'] / (features['volume_sma'] + 1e-10)\n",
    "        \n",
    "        # Price momentum\n",
    "        features['momentum_1h'] = features['close'] / features['close'].shift(1) - 1\n",
    "        features['momentum_24h'] = features['close'] / features['close'].shift(24) - 1\n",
    "        features['momentum_7d'] = features['close'] / features['close'].shift(24*7) - 1\n",
    "        \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute features\n",
    "engineer = FeatureEngineer()\n",
    "btc_features = engineer.compute_all_features(btc_data)\n",
    "\n",
    "# Drop NaN rows\n",
    "btc_features = btc_features.dropna().reset_index(drop=True)\n",
    "print(f\"Features shape: {btc_features.shape}\")\n",
    "print(f\"\\nFeatures: {list(btc_features.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some features\n",
    "fig, axes = plt.subplots(3, 2, figsize=(14, 10))\n",
    "\n",
    "# RSI\n",
    "axes[0, 0].plot(btc_features['timestamp'], btc_features['rsi_14'])\n",
    "axes[0, 0].axhline(y=70, color='r', linestyle='--', alpha=0.5)\n",
    "axes[0, 0].axhline(y=30, color='g', linestyle='--', alpha=0.5)\n",
    "axes[0, 0].set_title('RSI (14)')\n",
    "axes[0, 0].set_ylabel('RSI')\n",
    "\n",
    "# Volatility\n",
    "axes[0, 1].plot(btc_features['timestamp'], btc_features['volatility_24h'])\n",
    "axes[0, 1].set_title('24h Rolling Volatility')\n",
    "axes[0, 1].set_ylabel('Volatility')\n",
    "\n",
    "# MACD\n",
    "axes[1, 0].plot(btc_features['timestamp'], btc_features['macd'], label='MACD')\n",
    "axes[1, 0].plot(btc_features['timestamp'], btc_features['macd_signal'], label='Signal')\n",
    "axes[1, 0].bar(btc_features['timestamp'], btc_features['macd_hist'], alpha=0.3, label='Histogram')\n",
    "axes[1, 0].set_title('MACD')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Bollinger %B\n",
    "axes[1, 1].plot(btc_features['timestamp'], btc_features['bb_pct'])\n",
    "axes[1, 1].axhline(y=1, color='r', linestyle='--', alpha=0.5)\n",
    "axes[1, 1].axhline(y=0, color='g', linestyle='--', alpha=0.5)\n",
    "axes[1, 1].set_title('Bollinger Band %B')\n",
    "\n",
    "# Volume ratio\n",
    "axes[2, 0].plot(btc_features['timestamp'], btc_features['volume_ratio'])\n",
    "axes[2, 0].axhline(y=1, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[2, 0].set_title('Volume Ratio')\n",
    "\n",
    "# Momentum\n",
    "axes[2, 1].plot(btc_features['timestamp'], btc_features['momentum_24h'])\n",
    "axes[2, 1].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[2, 1].set_title('24h Momentum')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Diffusion Model for Forecasting\n",
    "\n",
    "We implement a conditional diffusion model that forecasts future prices given historical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    \"\"\"Dataset for time series with sliding window.\"\"\"\n",
    "    \n",
    "    def __init__(self, data, feature_cols, target_col='close', \n",
    "                 seq_length=100, forecast_horizon=24):\n",
    "        self.data = data\n",
    "        self.feature_cols = feature_cols\n",
    "        self.target_col = target_col\n",
    "        self.seq_length = seq_length\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        \n",
    "        # Scale features\n",
    "        self.feature_scaler = StandardScaler()\n",
    "        self.target_scaler = MinMaxScaler()\n",
    "        \n",
    "        self.features = self.feature_scaler.fit_transform(data[feature_cols].values)\n",
    "        self.targets = self.target_scaler.fit_transform(data[[target_col]].values)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_length - self.forecast_horizon + 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Historical features\n",
    "        x = self.features[idx:idx + self.seq_length]\n",
    "        \n",
    "        # Future targets\n",
    "        y = self.targets[idx + self.seq_length:idx + self.seq_length + self.forecast_horizon]\n",
    "        \n",
    "        return (\n",
    "            torch.tensor(x, dtype=torch.float32),\n",
    "            torch.tensor(y, dtype=torch.float32).squeeze(-1)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    \"\"\"Sinusoidal embeddings for timestep encoding.\"\"\"\n",
    "    \n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "    \n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = np.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat([torch.sin(embeddings), torch.cos(embeddings)], dim=-1)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class ConditionalDiffusionModel(nn.Module):\n",
    "    \"\"\"Conditional diffusion model for time series forecasting.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, seq_length, forecast_horizon, \n",
    "                 hidden_dim=256, time_emb_dim=64, n_layers=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.seq_length = seq_length\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        \n",
    "        # Time embedding\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(time_emb_dim),\n",
    "            nn.Linear(time_emb_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Condition encoder (processes historical features)\n",
    "        self.condition_encoder = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=0.1\n",
    "        )\n",
    "        \n",
    "        # Denoising network\n",
    "        self.input_proj = nn.Linear(forecast_horizon + hidden_dim * 2, hidden_dim)\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.LayerNorm(hidden_dim),\n",
    "                nn.SiLU(),\n",
    "                nn.Dropout(0.1)\n",
    "            ) for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        self.output_proj = nn.Linear(hidden_dim, forecast_horizon)\n",
    "    \n",
    "    def forward(self, x_noisy, t, condition):\n",
    "        \"\"\"\n",
    "        Predict noise given noisy future, timestep, and historical condition.\n",
    "        \n",
    "        Args:\n",
    "            x_noisy: Noisy future values [batch, forecast_horizon]\n",
    "            t: Diffusion timestep [batch]\n",
    "            condition: Historical features [batch, seq_length, input_dim]\n",
    "        \"\"\"\n",
    "        # Encode time\n",
    "        t_emb = self.time_mlp(t)  # [batch, hidden_dim]\n",
    "        \n",
    "        # Encode condition (historical data)\n",
    "        _, (h_n, _) = self.condition_encoder(condition)\n",
    "        cond_emb = h_n[-1]  # [batch, hidden_dim]\n",
    "        \n",
    "        # Combine noisy input with embeddings\n",
    "        combined = torch.cat([x_noisy, t_emb, cond_emb], dim=-1)\n",
    "        h = self.input_proj(combined)\n",
    "        \n",
    "        # Apply layers with residual connections\n",
    "        for layer in self.layers:\n",
    "            h = h + layer(h)\n",
    "        \n",
    "        # Output noise prediction\n",
    "        noise_pred = self.output_proj(h)\n",
    "        \n",
    "        return noise_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CryptoDiffusionPipeline:\n",
    "    \"\"\"Complete pipeline for cryptocurrency forecasting with diffusion models.\"\"\"\n",
    "    \n",
    "    def __init__(self, seq_length=100, forecast_horizon=24, \n",
    "                 num_diffusion_steps=1000, device='cuda'):\n",
    "        self.seq_length = seq_length\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.num_steps = num_diffusion_steps\n",
    "        self.device = device\n",
    "        \n",
    "        # Initialize noise schedule (cosine)\n",
    "        self._init_noise_schedule()\n",
    "        \n",
    "        self.model = None\n",
    "        self.optimizer = None\n",
    "    \n",
    "    def _init_noise_schedule(self):\n",
    "        \"\"\"Initialize cosine noise schedule.\"\"\"\n",
    "        s = 0.008\n",
    "        steps = self.num_steps + 1\n",
    "        t = torch.linspace(0, self.num_steps, steps) / self.num_steps\n",
    "        alphas_cumprod = torch.cos((t + s) / (1 + s) * np.pi / 2) ** 2\n",
    "        alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "        betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "        betas = torch.clamp(betas, 0.0001, 0.9999)\n",
    "        \n",
    "        self.betas = betas.to(self.device)\n",
    "        self.alphas = (1.0 - betas).to(self.device)\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0).to(self.device)\n",
    "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod).to(self.device)\n",
    "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1 - self.alphas_cumprod).to(self.device)\n",
    "    \n",
    "    def init_model(self, input_dim, hidden_dim=256):\n",
    "        \"\"\"Initialize the diffusion model.\"\"\"\n",
    "        self.model = ConditionalDiffusionModel(\n",
    "            input_dim=input_dim,\n",
    "            seq_length=self.seq_length,\n",
    "            forecast_horizon=self.forecast_horizon,\n",
    "            hidden_dim=hidden_dim\n",
    "        ).to(self.device)\n",
    "        \n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=1e-4)\n",
    "    \n",
    "    def add_noise(self, x_0, t, noise=None):\n",
    "        \"\"\"Add noise to data at timestep t.\"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_0)\n",
    "        \n",
    "        sqrt_alpha = self.sqrt_alphas_cumprod[t][:, None]\n",
    "        sqrt_one_minus_alpha = self.sqrt_one_minus_alphas_cumprod[t][:, None]\n",
    "        \n",
    "        return sqrt_alpha * x_0 + sqrt_one_minus_alpha * noise, noise\n",
    "    \n",
    "    def train_step(self, condition, target):\n",
    "        \"\"\"Single training step.\"\"\"\n",
    "        batch_size = condition.shape[0]\n",
    "        \n",
    "        # Sample random timesteps\n",
    "        t = torch.randint(0, self.num_steps, (batch_size,), device=self.device)\n",
    "        \n",
    "        # Add noise to target\n",
    "        x_noisy, noise = self.add_noise(target, t)\n",
    "        \n",
    "        # Predict noise\n",
    "        noise_pred = self.model(x_noisy, t.float(), condition)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(noise_pred, noise)\n",
    "        \n",
    "        # Backprop\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def train(self, dataloader, epochs=100):\n",
    "        \"\"\"Full training loop.\"\"\"\n",
    "        self.model.train()\n",
    "        losses = []\n",
    "        \n",
    "        for epoch in tqdm(range(epochs), desc=\"Training\"):\n",
    "            epoch_losses = []\n",
    "            for condition, target in dataloader:\n",
    "                condition = condition.to(self.device)\n",
    "                target = target.to(self.device)\n",
    "                \n",
    "                loss = self.train_step(condition, target)\n",
    "                epoch_losses.append(loss)\n",
    "            \n",
    "            avg_loss = np.mean(epoch_losses)\n",
    "            losses.append(avg_loss)\n",
    "            \n",
    "            if (epoch + 1) % 20 == 0:\n",
    "                print(f\"Epoch {epoch+1}: Loss = {avg_loss:.6f}\")\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def sample(self, condition, n_samples=100):\n",
    "        \"\"\"\n",
    "        Generate forecast samples using DDPM sampling.\n",
    "        \n",
    "        Args:\n",
    "            condition: Historical features [1, seq_length, input_dim]\n",
    "            n_samples: Number of Monte Carlo samples\n",
    "        \n",
    "        Returns:\n",
    "            samples: [n_samples, forecast_horizon]\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Expand condition for all samples\n",
    "        condition = condition.expand(n_samples, -1, -1).to(self.device)\n",
    "        \n",
    "        # Start from pure noise\n",
    "        x = torch.randn(n_samples, self.forecast_horizon, device=self.device)\n",
    "        \n",
    "        # Iteratively denoise\n",
    "        for t in reversed(range(self.num_steps)):\n",
    "            t_tensor = torch.full((n_samples,), t, device=self.device, dtype=torch.float32)\n",
    "            \n",
    "            # Predict noise\n",
    "            noise_pred = self.model(x, t_tensor, condition)\n",
    "            \n",
    "            # Compute coefficients\n",
    "            alpha = self.alphas[t]\n",
    "            alpha_cumprod = self.alphas_cumprod[t]\n",
    "            beta = self.betas[t]\n",
    "            \n",
    "            # Denoise\n",
    "            x = (1 / torch.sqrt(alpha)) * (\n",
    "                x - (beta / torch.sqrt(1 - alpha_cumprod)) * noise_pred\n",
    "            )\n",
    "            \n",
    "            # Add noise (except for last step)\n",
    "            if t > 0:\n",
    "                noise = torch.randn_like(x)\n",
    "                x = x + torch.sqrt(beta) * noise\n",
    "        \n",
    "        return x.cpu().numpy()\n",
    "    \n",
    "    def forecast(self, condition, n_samples=100):\n",
    "        \"\"\"\n",
    "        Generate probabilistic forecast with uncertainty.\n",
    "        \n",
    "        Returns:\n",
    "            dict with 'mean', 'std', 'confidence_95', 'confidence_5'\n",
    "        \"\"\"\n",
    "        samples = self.sample(condition, n_samples)\n",
    "        \n",
    "        return {\n",
    "            'samples': samples,\n",
    "            'mean': np.mean(samples, axis=0),\n",
    "            'std': np.std(samples, axis=0),\n",
    "            'median': np.median(samples, axis=0),\n",
    "            'confidence_5': np.percentile(samples, 5, axis=0),\n",
    "            'confidence_95': np.percentile(samples, 95, axis=0),\n",
    "            'confidence_25': np.percentile(samples, 25, axis=0),\n",
    "            'confidence_75': np.percentile(samples, 75, axis=0)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "feature_cols = ['returns', 'volatility_24h', 'rsi_14', 'macd', 'bb_pct', \n",
    "                'volume_ratio', 'momentum_24h']\n",
    "\n",
    "# Create dataset\n",
    "dataset = TimeSeriesDataset(\n",
    "    btc_features, \n",
    "    feature_cols=feature_cols,\n",
    "    target_col='close',\n",
    "    seq_length=100,\n",
    "    forecast_horizon=24\n",
    ")\n",
    "\n",
    "# Split into train/val/test\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(dataset, range(train_size))\n",
    "val_dataset = torch.utils.data.Subset(dataset, range(train_size, train_size + val_size))\n",
    "test_dataset = torch.utils.data.Subset(dataset, range(train_size + val_size, len(dataset)))\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the pipeline\n",
    "pipeline = CryptoDiffusionPipeline(\n",
    "    seq_length=100,\n",
    "    forecast_horizon=24,\n",
    "    num_diffusion_steps=500,  # Reduced for faster training\n",
    "    device=device\n",
    ")\n",
    "\n",
    "pipeline.init_model(input_dim=len(feature_cols), hidden_dim=128)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in pipeline.model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "losses = pipeline.train(train_loader, epochs=50)\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Forecasting with Uncertainty Quantification\n",
    "\n",
    "We generate probabilistic forecasts using Monte Carlo sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a test sample\n",
    "test_condition, test_target = test_dataset[0]\n",
    "test_condition = test_condition.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Generate forecast\n",
    "forecast_result = pipeline.forecast(test_condition, n_samples=100)\n",
    "\n",
    "print(f\"Forecast shape: {forecast_result['mean'].shape}\")\n",
    "print(f\"Mean forecast: {forecast_result['mean'][:5]}\")\n",
    "print(f\"Std: {forecast_result['std'][:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse transform to get actual prices\n",
    "actual_target = dataset.target_scaler.inverse_transform(\n",
    "    test_target.numpy().reshape(-1, 1)\n",
    ").flatten()\n",
    "\n",
    "forecast_mean = dataset.target_scaler.inverse_transform(\n",
    "    forecast_result['mean'].reshape(-1, 1)\n",
    ").flatten()\n",
    "\n",
    "forecast_5 = dataset.target_scaler.inverse_transform(\n",
    "    forecast_result['confidence_5'].reshape(-1, 1)\n",
    ").flatten()\n",
    "\n",
    "forecast_95 = dataset.target_scaler.inverse_transform(\n",
    "    forecast_result['confidence_95'].reshape(-1, 1)\n",
    ").flatten()\n",
    "\n",
    "forecast_25 = dataset.target_scaler.inverse_transform(\n",
    "    forecast_result['confidence_25'].reshape(-1, 1)\n",
    ").flatten()\n",
    "\n",
    "forecast_75 = dataset.target_scaler.inverse_transform(\n",
    "    forecast_result['confidence_75'].reshape(-1, 1)\n",
    ").flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize forecast\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "hours = np.arange(24)\n",
    "\n",
    "# Plot actual\n",
    "ax.plot(hours, actual_target, 'b-', linewidth=2, label='Actual')\n",
    "\n",
    "# Plot forecast mean\n",
    "ax.plot(hours, forecast_mean, 'r-', linewidth=2, label='Forecast (mean)')\n",
    "\n",
    "# Plot confidence intervals\n",
    "ax.fill_between(hours, forecast_5, forecast_95, color='red', alpha=0.2, label='90% CI')\n",
    "ax.fill_between(hours, forecast_25, forecast_75, color='red', alpha=0.3, label='50% CI')\n",
    "\n",
    "ax.set_xlabel('Hours Ahead')\n",
    "ax.set_ylabel('BTC Price (USDT)')\n",
    "ax.set_title('24-Hour Bitcoin Price Forecast with Uncertainty')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on multiple test samples\n",
    "def evaluate_forecasts(pipeline, dataset, test_indices, n_samples=50):\n",
    "    \"\"\"Evaluate forecasts on multiple test samples.\"\"\"\n",
    "    all_actuals = []\n",
    "    all_forecasts = []\n",
    "    all_stds = []\n",
    "    \n",
    "    for idx in tqdm(test_indices, desc=\"Evaluating\"):\n",
    "        condition, target = dataset[idx]\n",
    "        condition = condition.unsqueeze(0)\n",
    "        \n",
    "        forecast_result = pipeline.forecast(condition, n_samples=n_samples)\n",
    "        \n",
    "        # Inverse transform\n",
    "        actual = dataset.target_scaler.inverse_transform(\n",
    "            target.numpy().reshape(-1, 1)\n",
    "        ).flatten()\n",
    "        \n",
    "        forecast = dataset.target_scaler.inverse_transform(\n",
    "            forecast_result['mean'].reshape(-1, 1)\n",
    "        ).flatten()\n",
    "        \n",
    "        all_actuals.append(actual)\n",
    "        all_forecasts.append(forecast)\n",
    "        all_stds.append(forecast_result['std'])\n",
    "    \n",
    "    return np.array(all_actuals), np.array(all_forecasts), np.array(all_stds)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_indices = range(len(test_dataset))\n",
    "actuals, forecasts, stds = evaluate_forecasts(\n",
    "    pipeline, \n",
    "    test_dataset.dataset,  # Access underlying dataset\n",
    "    [test_dataset.indices[i] for i in range(min(20, len(test_dataset)))],\n",
    "    n_samples=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "def compute_metrics(actuals, forecasts):\n",
    "    \"\"\"Compute forecasting metrics.\"\"\"\n",
    "    # Per-horizon metrics\n",
    "    mse_per_horizon = np.mean((actuals - forecasts) ** 2, axis=0)\n",
    "    mae_per_horizon = np.mean(np.abs(actuals - forecasts), axis=0)\n",
    "    \n",
    "    # MAPE (handle zeros)\n",
    "    mape_per_horizon = np.mean(np.abs((actuals - forecasts) / (actuals + 1e-10)) * 100, axis=0)\n",
    "    \n",
    "    # Overall metrics\n",
    "    overall_mse = np.mean(mse_per_horizon)\n",
    "    overall_mae = np.mean(mae_per_horizon)\n",
    "    overall_mape = np.mean(mape_per_horizon)\n",
    "    overall_rmse = np.sqrt(overall_mse)\n",
    "    \n",
    "    return {\n",
    "        'mse_per_horizon': mse_per_horizon,\n",
    "        'mae_per_horizon': mae_per_horizon,\n",
    "        'mape_per_horizon': mape_per_horizon,\n",
    "        'overall_mse': overall_mse,\n",
    "        'overall_mae': overall_mae,\n",
    "        'overall_mape': overall_mape,\n",
    "        'overall_rmse': overall_rmse\n",
    "    }\n",
    "\n",
    "metrics = compute_metrics(actuals, forecasts)\n",
    "\n",
    "print(\"\\n=== Forecast Metrics ===\")\n",
    "print(f\"Overall RMSE: ${metrics['overall_rmse']:.2f}\")\n",
    "print(f\"Overall MAE: ${metrics['overall_mae']:.2f}\")\n",
    "print(f\"Overall MAPE: {metrics['overall_mape']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot metrics by horizon\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "hours = np.arange(1, 25)\n",
    "\n",
    "axes[0].plot(hours, np.sqrt(metrics['mse_per_horizon']), 'b-o')\n",
    "axes[0].set_xlabel('Forecast Horizon (hours)')\n",
    "axes[0].set_ylabel('RMSE ($)')\n",
    "axes[0].set_title('RMSE by Forecast Horizon')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(hours, metrics['mae_per_horizon'], 'g-o')\n",
    "axes[1].set_xlabel('Forecast Horizon (hours)')\n",
    "axes[1].set_ylabel('MAE ($)')\n",
    "axes[1].set_title('MAE by Forecast Horizon')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].plot(hours, metrics['mape_per_horizon'], 'r-o')\n",
    "axes[2].set_xlabel('Forecast Horizon (hours)')\n",
    "axes[2].set_ylabel('MAPE (%)')\n",
    "axes[2].set_title('MAPE by Forecast Horizon')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Key Takeaways\n",
    "\n",
    "1. **Diffusion models** provide natural uncertainty quantification through Monte Carlo sampling\n",
    "2. **Technical indicators** (RSI, MACD, Bollinger Bands) help condition the forecast\n",
    "3. **Probabilistic forecasts** are more useful than point predictions for risk management\n",
    "4. **Forecast error increases** with horizon (as expected)\n",
    "\n",
    "### Limitations:\n",
    "- Slow inference (requires many denoising steps)\n",
    "- Computationally expensive training\n",
    "- Struggles with extreme events (black swans)\n",
    "\n",
    "### Next Steps:\n",
    "- See notebook 06 for comparison with GANs\n",
    "- See Rust implementation for production deployment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
