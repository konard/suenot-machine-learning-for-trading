{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion Models vs GANs: Сравнение для финансовых временных рядов\n",
    "\n",
    "В этом ноутбуке мы сравниваем два подхода к генеративному моделированию:\n",
    "\n",
    "1. **GANs (Generative Adversarial Networks)** - из Главы 21\n",
    "2. **Diffusion Models** - из этой главы\n",
    "\n",
    "## Ключевые различия:\n",
    "\n",
    "| Аспект | GANs | Diffusion |  \n",
    "|--------|------|-----------|  \n",
    "| Обучение | Adversarial (min-max game) | Простой MSE loss |\n",
    "| Стабильность | Mode collapse, нестабильность | Стабильное обучение |\n",
    "| Качество | Высокое, но вариативное | Высокое, консистентное |\n",
    "| Скорость генерации | Быстрая (один forward pass) | Медленная (много итераций) |\n",
    "| Вероятности | Implicit | Explicit (можно оценить likelihood) |\n",
    "| Контроль | Сложный | Гибкий (conditional, guidance) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Установка зависимостей\n",
    "!pip install torch numpy pandas matplotlib seaborn scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, Optional, Dict, List\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Генерация тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_financial_data(\n",
    "    n_samples: int = 1000,\n",
    "    seq_len: int = 50,\n",
    "    seed: int = 42\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Генерация финансовых временных рядов (log returns).\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Случайные параметры для разнообразия\n",
    "        mu = np.random.uniform(-0.001, 0.001)\n",
    "        sigma = np.random.uniform(0.01, 0.03)\n",
    "        \n",
    "        # GBM returns\n",
    "        returns = np.random.normal(mu, sigma, seq_len)\n",
    "        \n",
    "        # Добавляем автокорреляцию (GARCH-like effect)\n",
    "        vol = np.abs(returns)\n",
    "        for t in range(1, seq_len):\n",
    "            vol[t] = 0.1 * vol[t-1] + 0.9 * vol[t]\n",
    "        returns = returns * (vol / vol.mean())\n",
    "        \n",
    "        data.append(returns)\n",
    "    \n",
    "    data = np.array(data)\n",
    "    \n",
    "    # Нормализация\n",
    "    mean = data.mean()\n",
    "    std = data.std()\n",
    "    data = (data - mean) / std\n",
    "    \n",
    "    return data, mean, std\n",
    "\n",
    "# Генерируем данные\n",
    "data, data_mean, data_std = generate_financial_data(n_samples=2000, seq_len=50)\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "print(f\"Mean: {data.mean():.4f}, Std: {data.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация реальных данных\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Примеры временных рядов\n",
    "ax1 = axes[0]\n",
    "for i in range(20):\n",
    "    ax1.plot(data[i], alpha=0.5)\n",
    "ax1.set_title('Sample Financial Time Series')\n",
    "ax1.set_xlabel('Time')\n",
    "ax1.set_ylabel('Normalized Return')\n",
    "\n",
    "# Распределение\n",
    "ax2 = axes[1]\n",
    "ax2.hist(data.flatten(), bins=100, density=True, alpha=0.7)\n",
    "ax2.set_title('Distribution of Returns')\n",
    "ax2.set_xlabel('Return')\n",
    "ax2.set_ylabel('Density')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготовка DataLoader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data: np.ndarray):\n",
    "        self.data = torch.FloatTensor(data).unsqueeze(-1)  # [N, seq_len, 1]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Split\n",
    "train_data = data[:1600]\n",
    "test_data = data[1600:]\n",
    "\n",
    "train_dataset = TimeSeriesDataset(train_data)\n",
    "test_dataset = TimeSeriesDataset(test_data)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}, Test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GAN Implementation (TimeGAN-style)\n",
    "\n",
    "Реализуем упрощённую версию TimeGAN для временных рядов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANGenerator(nn.Module):\n",
    "    \"\"\"Generator для временных рядов.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        latent_dim: int = 32,\n",
    "        hidden_dim: int = 128,\n",
    "        seq_len: int = 50,\n",
    "        output_dim: int = 1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        # LSTM-based generator\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=latent_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=0.1\n",
    "        )\n",
    "        \n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim // 2, output_dim),\n",
    "            nn.Tanh()  # Output in [-1, 1]\n",
    "        )\n",
    "    \n",
    "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            z: [batch, seq_len, latent_dim] - latent noise\n",
    "        Returns:\n",
    "            [batch, seq_len, output_dim]\n",
    "        \"\"\"\n",
    "        h, _ = self.lstm(z)\n",
    "        out = self.output(h)\n",
    "        return out\n",
    "\n",
    "\n",
    "class GANDiscriminator(nn.Module):\n",
    "    \"\"\"Discriminator для временных рядов.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int = 1,\n",
    "        hidden_dim: int = 128\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch, seq_len, input_dim]\n",
    "        Returns:\n",
    "            [batch, 1] - real/fake score\n",
    "        \"\"\"\n",
    "        h, _ = self.lstm(x)\n",
    "        # Global average pooling\n",
    "        h = h.mean(dim=1)\n",
    "        out = self.output(h)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesGAN:\n",
    "    \"\"\"GAN для генерации временных рядов.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        latent_dim: int = 32,\n",
    "        hidden_dim: int = 128,\n",
    "        seq_len: int = 50,\n",
    "        device: torch.device = torch.device('cpu')\n",
    "    ):\n",
    "        self.latent_dim = latent_dim\n",
    "        self.seq_len = seq_len\n",
    "        self.device = device\n",
    "        \n",
    "        self.generator = GANGenerator(\n",
    "            latent_dim=latent_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            seq_len=seq_len\n",
    "        ).to(device)\n",
    "        \n",
    "        self.discriminator = GANDiscriminator(\n",
    "            input_dim=1,\n",
    "            hidden_dim=hidden_dim\n",
    "        ).to(device)\n",
    "    \n",
    "    def sample_latent(self, batch_size: int) -> torch.Tensor:\n",
    "        \"\"\"Sample latent noise.\"\"\"\n",
    "        return torch.randn(batch_size, self.seq_len, self.latent_dim, device=self.device)\n",
    "    \n",
    "    def generate(self, n_samples: int) -> torch.Tensor:\n",
    "        \"\"\"Generate synthetic samples.\"\"\"\n",
    "        self.generator.eval()\n",
    "        with torch.no_grad():\n",
    "            z = self.sample_latent(n_samples)\n",
    "            samples = self.generator(z)\n",
    "        return samples\n",
    "    \n",
    "    def train(\n",
    "        self,\n",
    "        loader: DataLoader,\n",
    "        n_epochs: int = 50,\n",
    "        lr: float = 0.0002\n",
    "    ) -> Dict[str, List[float]]:\n",
    "        \"\"\"Train GAN.\"\"\"\n",
    "        \n",
    "        opt_g = torch.optim.Adam(self.generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "        opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "        \n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        history = {'g_loss': [], 'd_loss': []}\n",
    "        \n",
    "        for epoch in tqdm(range(n_epochs), desc=\"Training GAN\"):\n",
    "            g_losses = []\n",
    "            d_losses = []\n",
    "            \n",
    "            for real_data in loader:\n",
    "                real_data = real_data.to(self.device)\n",
    "                batch_size = real_data.shape[0]\n",
    "                \n",
    "                # Labels\n",
    "                real_labels = torch.ones(batch_size, 1, device=self.device)\n",
    "                fake_labels = torch.zeros(batch_size, 1, device=self.device)\n",
    "                \n",
    "                # ---- Train Discriminator ----\n",
    "                opt_d.zero_grad()\n",
    "                \n",
    "                # Real data\n",
    "                real_pred = self.discriminator(real_data)\n",
    "                d_loss_real = criterion(real_pred, real_labels)\n",
    "                \n",
    "                # Fake data\n",
    "                z = self.sample_latent(batch_size)\n",
    "                fake_data = self.generator(z).detach()\n",
    "                fake_pred = self.discriminator(fake_data)\n",
    "                d_loss_fake = criterion(fake_pred, fake_labels)\n",
    "                \n",
    "                d_loss = (d_loss_real + d_loss_fake) / 2\n",
    "                d_loss.backward()\n",
    "                opt_d.step()\n",
    "                \n",
    "                # ---- Train Generator ----\n",
    "                opt_g.zero_grad()\n",
    "                \n",
    "                z = self.sample_latent(batch_size)\n",
    "                fake_data = self.generator(z)\n",
    "                fake_pred = self.discriminator(fake_data)\n",
    "                g_loss = criterion(fake_pred, real_labels)  # Fool discriminator\n",
    "                \n",
    "                g_loss.backward()\n",
    "                opt_g.step()\n",
    "                \n",
    "                g_losses.append(g_loss.item())\n",
    "                d_losses.append(d_loss.item())\n",
    "            \n",
    "            history['g_loss'].append(np.mean(g_losses))\n",
    "            history['d_loss'].append(np.mean(d_losses))\n",
    "            \n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}: G_loss={history['g_loss'][-1]:.4f}, D_loss={history['d_loss'][-1]:.4f}\")\n",
    "        \n",
    "        return history\n",
    "\n",
    "# Create and train GAN\n",
    "gan = TimeSeriesGAN(latent_dim=32, hidden_dim=128, seq_len=50, device=device)\n",
    "print(f\"Generator params: {sum(p.numel() for p in gan.generator.parameters()):,}\")\n",
    "print(f\"Discriminator params: {sum(p.numel() for p in gan.discriminator.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GAN\n",
    "start_time = time.time()\n",
    "gan_history = gan.train(train_loader, n_epochs=50, lr=0.0002)\n",
    "gan_train_time = time.time() - start_time\n",
    "print(f\"\\nGAN Training time: {gan_train_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot GAN training\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(gan_history['g_loss'], label='Generator Loss')\n",
    "ax.plot(gan_history['d_loss'], label='Discriminator Loss')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('GAN Training Progress')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Diffusion Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDiffusion(nn.Module):\n",
    "    \"\"\"Простая Diffusion модель для временных рядов.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_len: int = 50,\n",
    "        hidden_dim: int = 128,\n",
    "        num_layers: int = 4,\n",
    "        diffusion_steps: int = 100\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        self.diffusion_steps = diffusion_steps\n",
    "        \n",
    "        # Time embedding\n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Linear(1, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_proj = nn.Linear(1, hidden_dim)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, seq_len, hidden_dim) * 0.02)\n",
    "        \n",
    "        # Transformer layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=hidden_dim,\n",
    "                nhead=4,\n",
    "                dim_feedforward=hidden_dim * 4,\n",
    "                dropout=0.1,\n",
    "                batch_first=True\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_proj = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "        # Noise schedule\n",
    "        self._setup_schedule()\n",
    "    \n",
    "    def _setup_schedule(self):\n",
    "        \"\"\"Cosine schedule.\"\"\"\n",
    "        T = self.diffusion_steps\n",
    "        s = 0.008\n",
    "        \n",
    "        steps = torch.arange(T + 1) / T\n",
    "        alphas_bar = torch.cos((steps + s) / (1 + s) * np.pi / 2) ** 2\n",
    "        alphas_bar = alphas_bar / alphas_bar[0]\n",
    "        \n",
    "        betas = 1 - alphas_bar[1:] / alphas_bar[:-1]\n",
    "        betas = betas.clamp(max=0.999)\n",
    "        \n",
    "        alphas = 1 - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "        \n",
    "        self.register_buffer('betas', betas)\n",
    "        self.register_buffer('alphas', alphas)\n",
    "        self.register_buffer('alphas_cumprod', alphas_cumprod)\n",
    "        self.register_buffer('sqrt_alphas_cumprod', torch.sqrt(alphas_cumprod))\n",
    "        self.register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1 - alphas_cumprod))\n",
    "    \n",
    "    def forward_diffusion(\n",
    "        self,\n",
    "        x0: torch.Tensor,\n",
    "        t: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Forward diffusion.\"\"\"\n",
    "        noise = torch.randn_like(x0)\n",
    "        \n",
    "        sqrt_alpha = self.sqrt_alphas_cumprod[t].view(-1, 1, 1)\n",
    "        sqrt_one_minus_alpha = self.sqrt_one_minus_alphas_cumprod[t].view(-1, 1, 1)\n",
    "        \n",
    "        xt = sqrt_alpha * x0 + sqrt_one_minus_alpha * noise\n",
    "        \n",
    "        return xt, noise\n",
    "    \n",
    "    def denoise(self, xt: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Predict noise.\"\"\"\n",
    "        batch_size = xt.shape[0]\n",
    "        \n",
    "        # Input projection\n",
    "        h = self.input_proj(xt) + self.pos_embed\n",
    "        \n",
    "        # Time embedding\n",
    "        t_emb = self.time_embed(t.float().unsqueeze(-1) / self.diffusion_steps)\n",
    "        h = h + t_emb.unsqueeze(1)\n",
    "        \n",
    "        # Transformer layers\n",
    "        for layer in self.layers:\n",
    "            h = layer(h)\n",
    "        \n",
    "        # Output\n",
    "        return self.output_proj(h)\n",
    "    \n",
    "    def compute_loss(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute training loss.\"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        t = torch.randint(0, self.diffusion_steps, (batch_size,), device=x.device)\n",
    "        \n",
    "        xt, noise = self.forward_diffusion(x, t)\n",
    "        noise_pred = self.denoise(xt, t)\n",
    "        \n",
    "        return F.mse_loss(noise_pred, noise)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def sample(self, n_samples: int, device: torch.device) -> torch.Tensor:\n",
    "        \"\"\"Generate samples via DDPM.\"\"\"\n",
    "        x = torch.randn(n_samples, self.seq_len, 1, device=device)\n",
    "        \n",
    "        for t in tqdm(reversed(range(self.diffusion_steps)), desc='Sampling', leave=False):\n",
    "            t_tensor = torch.full((n_samples,), t, device=device)\n",
    "            noise_pred = self.denoise(x, t_tensor)\n",
    "            \n",
    "            alpha = self.alphas[t]\n",
    "            alpha_bar = self.alphas_cumprod[t]\n",
    "            beta = self.betas[t]\n",
    "            \n",
    "            mean = (1 / torch.sqrt(alpha)) * (\n",
    "                x - (beta / torch.sqrt(1 - alpha_bar)) * noise_pred\n",
    "            )\n",
    "            \n",
    "            if t > 0:\n",
    "                alpha_bar_prev = self.alphas_cumprod[t - 1]\n",
    "                variance = beta * (1 - alpha_bar_prev) / (1 - alpha_bar)\n",
    "                noise = torch.randn_like(x)\n",
    "                x = mean + torch.sqrt(variance) * noise\n",
    "            else:\n",
    "                x = mean\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def sample_ddim(self, n_samples: int, device: torch.device, num_steps: int = 20) -> torch.Tensor:\n",
    "        \"\"\"Fast sampling via DDIM.\"\"\"\n",
    "        x = torch.randn(n_samples, self.seq_len, 1, device=device)\n",
    "        \n",
    "        # Subset of timesteps\n",
    "        step_size = self.diffusion_steps // num_steps\n",
    "        timesteps = list(range(0, self.diffusion_steps, step_size))[::-1]\n",
    "        \n",
    "        for i, t in enumerate(tqdm(timesteps, desc='DDIM Sampling', leave=False)):\n",
    "            t_tensor = torch.full((n_samples,), t, device=device)\n",
    "            noise_pred = self.denoise(x, t_tensor)\n",
    "            \n",
    "            alpha_bar = self.alphas_cumprod[t]\n",
    "            \n",
    "            # Predicted x0\n",
    "            x0_pred = (x - torch.sqrt(1 - alpha_bar) * noise_pred) / torch.sqrt(alpha_bar)\n",
    "            \n",
    "            if i < len(timesteps) - 1:\n",
    "                t_prev = timesteps[i + 1]\n",
    "                alpha_bar_prev = self.alphas_cumprod[t_prev]\n",
    "                \n",
    "                # DDIM update (deterministic)\n",
    "                x = torch.sqrt(alpha_bar_prev) * x0_pred + \\\n",
    "                    torch.sqrt(1 - alpha_bar_prev) * noise_pred\n",
    "            else:\n",
    "                x = x0_pred\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create diffusion model\n",
    "diffusion = SimpleDiffusion(\n",
    "    seq_len=50,\n",
    "    hidden_dim=128,\n",
    "    num_layers=4,\n",
    "    diffusion_steps=100\n",
    ").to(device)\n",
    "\n",
    "print(f\"Diffusion params: {sum(p.numel() for p in diffusion.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Diffusion\n",
    "optimizer = torch.optim.AdamW(diffusion.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n",
    "\n",
    "n_epochs = 50\n",
    "diffusion_losses = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in tqdm(range(n_epochs), desc=\"Training Diffusion\"):\n",
    "    diffusion.train()\n",
    "    epoch_losses = []\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = diffusion.compute_loss(batch)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(diffusion.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_losses.append(loss.item())\n",
    "    \n",
    "    scheduler.step()\n",
    "    diffusion_losses.append(np.mean(epoch_losses))\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}: Loss={diffusion_losses[-1]:.4f}\")\n",
    "\n",
    "diffusion_train_time = time.time() - start_time\n",
    "print(f\"\\nDiffusion Training time: {diffusion_train_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Diffusion training\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(diffusion_losses, label='Diffusion Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Diffusion Model Training Progress')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Генерация и сравнение сэмплов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples from both models\n",
    "n_samples = 500\n",
    "\n",
    "# GAN samples\n",
    "print(\"Generating GAN samples...\")\n",
    "start_time = time.time()\n",
    "gan_samples = gan.generate(n_samples).cpu().numpy().squeeze(-1)\n",
    "gan_gen_time = time.time() - start_time\n",
    "\n",
    "# Diffusion samples (DDPM)\n",
    "print(\"Generating Diffusion samples (DDPM)...\")\n",
    "diffusion.eval()\n",
    "start_time = time.time()\n",
    "diffusion_samples = diffusion.sample(n_samples, device).cpu().numpy().squeeze(-1)\n",
    "diffusion_gen_time = time.time() - start_time\n",
    "\n",
    "# Diffusion samples (DDIM - fast)\n",
    "print(\"Generating Diffusion samples (DDIM)...\")\n",
    "start_time = time.time()\n",
    "diffusion_ddim_samples = diffusion.sample_ddim(n_samples, device, num_steps=20).cpu().numpy().squeeze(-1)\n",
    "diffusion_ddim_gen_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nGAN generation time: {gan_gen_time:.2f}s ({n_samples/gan_gen_time:.1f} samples/s)\")\n",
    "print(f\"DDPM generation time: {diffusion_gen_time:.2f}s ({n_samples/diffusion_gen_time:.1f} samples/s)\")\n",
    "print(f\"DDIM generation time: {diffusion_ddim_gen_time:.2f}s ({n_samples/diffusion_ddim_gen_time:.1f} samples/s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "# Row 1: Sample time series\n",
    "for i, (ax, samples, name) in enumerate(zip(axes[0], \n",
    "                                             [test_data[:20], gan_samples[:20], diffusion_samples[:20]],\n",
    "                                             ['Real', 'GAN', 'Diffusion'])):\n",
    "    for s in samples:\n",
    "        ax.plot(s, alpha=0.5)\n",
    "    ax.set_title(f'{name} Samples')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Value')\n",
    "\n",
    "# Row 2: Distributions\n",
    "for i, (ax, samples, name) in enumerate(zip(axes[1],\n",
    "                                             [test_data, gan_samples, diffusion_samples],\n",
    "                                             ['Real', 'GAN', 'Diffusion'])):\n",
    "    ax.hist(samples.flatten(), bins=100, density=True, alpha=0.7)\n",
    "    ax.set_title(f'{name} Distribution')\n",
    "    ax.set_xlabel('Value')\n",
    "    ax.set_ylabel('Density')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Количественное сравнение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(real: np.ndarray, synthetic: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"Compute quality metrics for synthetic data.\"\"\"\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    # Statistical metrics\n",
    "    real_flat = real.flatten()\n",
    "    synth_flat = synthetic.flatten()\n",
    "    \n",
    "    # Mean absolute difference\n",
    "    metrics['mean_diff'] = abs(np.mean(real_flat) - np.mean(synth_flat))\n",
    "    metrics['std_diff'] = abs(np.std(real_flat) - np.std(synth_flat))\n",
    "    \n",
    "    # Skewness and Kurtosis\n",
    "    from scipy.stats import skew, kurtosis\n",
    "    metrics['skew_diff'] = abs(skew(real_flat) - skew(synth_flat))\n",
    "    metrics['kurtosis_diff'] = abs(kurtosis(real_flat) - kurtosis(synth_flat))\n",
    "    \n",
    "    # Autocorrelation\n",
    "    def acf(x, lag):\n",
    "        return np.corrcoef(x[:-lag], x[lag:])[0, 1]\n",
    "    \n",
    "    real_acf = [acf(real_flat, lag) for lag in [1, 5, 10]]\n",
    "    synth_acf = [acf(synth_flat, lag) for lag in [1, 5, 10]]\n",
    "    metrics['acf_mae'] = np.mean(np.abs(np.array(real_acf) - np.array(synth_acf)))\n",
    "    \n",
    "    # Discriminative score\n",
    "    n_real = min(len(real), 500)\n",
    "    n_synth = min(len(synthetic), 500)\n",
    "    \n",
    "    X = np.vstack([real[:n_real].reshape(n_real, -1), \n",
    "                   synthetic[:n_synth].reshape(n_synth, -1)])\n",
    "    y = np.array([0] * n_real + [1] * n_synth)\n",
    "    \n",
    "    clf = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)\n",
    "    scores = cross_val_score(clf, X, y, cv=3)\n",
    "    metrics['discriminative_score'] = scores.mean()\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics for both models\n",
    "print(\"Computing metrics...\")\n",
    "\n",
    "gan_metrics = compute_metrics(test_data, gan_samples)\n",
    "diffusion_metrics = compute_metrics(test_data, diffusion_samples)\n",
    "ddim_metrics = compute_metrics(test_data, diffusion_ddim_samples)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"COMPARISON: GAN vs Diffusion Models\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n{'Metric':<25} {'GAN':<15} {'DDPM':<15} {'DDIM':<15}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for metric in gan_metrics.keys():\n",
    "    gan_val = gan_metrics[metric]\n",
    "    diff_val = diffusion_metrics[metric]\n",
    "    ddim_val = ddim_metrics[metric]\n",
    "    \n",
    "    # Highlight better value\n",
    "    if metric == 'discriminative_score':\n",
    "        # Closer to 0.5 is better\n",
    "        gan_dist = abs(gan_val - 0.5)\n",
    "        diff_dist = abs(diff_val - 0.5)\n",
    "        ddim_dist = abs(ddim_val - 0.5)\n",
    "        best = min(gan_dist, diff_dist, ddim_dist)\n",
    "        gan_mark = '*' if gan_dist == best else ' '\n",
    "        diff_mark = '*' if diff_dist == best else ' '\n",
    "        ddim_mark = '*' if ddim_dist == best else ' '\n",
    "    else:\n",
    "        # Lower is better\n",
    "        best = min(gan_val, diff_val, ddim_val)\n",
    "        gan_mark = '*' if gan_val == best else ' '\n",
    "        diff_mark = '*' if diff_val == best else ' '\n",
    "        ddim_mark = '*' if ddim_val == best else ' '\n",
    "    \n",
    "    print(f\"{metric:<25} {gan_val:>14.4f}{gan_mark} {diff_val:>14.4f}{diff_mark} {ddim_val:>14.4f}{ddim_mark}\")\n",
    "\n",
    "print(\"\\n* = best value\")\n",
    "print(\"\\nNote: discriminative_score closer to 0.5 = harder to distinguish from real\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speed comparison\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"SPEED COMPARISON\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\n{'Metric':<25} {'GAN':<15} {'DDPM':<15} {'DDIM':<15}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Training time (s)':<25} {gan_train_time:<15.1f} {diffusion_train_time:<15.1f} {'-':^15}\")\n",
    "print(f\"{'Generation time (s)':<25} {gan_gen_time:<15.2f} {diffusion_gen_time:<15.2f} {diffusion_ddim_gen_time:<15.2f}\")\n",
    "print(f\"{'Samples per second':<25} {n_samples/gan_gen_time:<15.1f} {n_samples/diffusion_gen_time:<15.1f} {n_samples/diffusion_ddim_gen_time:<15.1f}\")\n",
    "print(f\"{'Speedup vs DDPM':<25} {diffusion_gen_time/gan_gen_time:<15.1f}x {1.0:<15.1f}x {diffusion_gen_time/diffusion_ddim_gen_time:<15.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. t-SNE Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE embedding\n",
    "n_vis = 300\n",
    "\n",
    "# Prepare data\n",
    "real_vis = test_data[:n_vis].reshape(n_vis, -1)\n",
    "gan_vis = gan_samples[:n_vis].reshape(n_vis, -1)\n",
    "diff_vis = diffusion_samples[:n_vis].reshape(n_vis, -1)\n",
    "\n",
    "combined = np.vstack([real_vis, gan_vis, diff_vis])\n",
    "labels = np.array([0] * n_vis + [1] * n_vis + [2] * n_vis)\n",
    "\n",
    "print(\"Computing t-SNE...\")\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "embedded = tsne.fit_transform(combined)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = ['blue', 'red', 'green']\n",
    "names = ['Real', 'GAN', 'Diffusion']\n",
    "\n",
    "for i, (color, name) in enumerate(zip(colors, names)):\n",
    "    mask = labels == i\n",
    "    plt.scatter(\n",
    "        embedded[mask, 0], embedded[mask, 1],\n",
    "        c=color, label=name, alpha=0.5, s=20\n",
    "    )\n",
    "\n",
    "plt.title('t-SNE: Real vs GAN vs Diffusion')\n",
    "plt.xlabel('t-SNE 1')\n",
    "plt.ylabel('t-SNE 2')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Diversity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_diversity(samples: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"Compute diversity metrics.\"\"\"\n",
    "    n = len(samples)\n",
    "    flat = samples.reshape(n, -1)\n",
    "    \n",
    "    # Pairwise distances\n",
    "    from sklearn.metrics import pairwise_distances\n",
    "    dists = pairwise_distances(flat[:200], metric='euclidean')\n",
    "    \n",
    "    # Mean pairwise distance (higher = more diverse)\n",
    "    upper_tri = dists[np.triu_indices(len(dists), k=1)]\n",
    "    \n",
    "    return {\n",
    "        'mean_distance': np.mean(upper_tri),\n",
    "        'std_distance': np.std(upper_tri),\n",
    "        'min_distance': np.min(upper_tri),\n",
    "        'max_distance': np.max(upper_tri)\n",
    "    }\n",
    "\n",
    "real_diversity = compute_diversity(test_data)\n",
    "gan_diversity = compute_diversity(gan_samples)\n",
    "diffusion_diversity = compute_diversity(diffusion_samples)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DIVERSITY ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n{'Metric':<20} {'Real':<15} {'GAN':<15} {'Diffusion':<15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for metric in real_diversity.keys():\n",
    "    print(f\"{metric:<20} {real_diversity[metric]:<15.4f} {gan_diversity[metric]:<15.4f} {diffusion_diversity[metric]:<15.4f}\")\n",
    "\n",
    "print(\"\\nNote: Higher mean_distance = more diverse samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Training stability\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(gan_history['g_loss'], label='GAN Generator', alpha=0.7)\n",
    "ax1.plot(gan_history['d_loss'], label='GAN Discriminator', alpha=0.7)\n",
    "ax1.plot(diffusion_losses, label='Diffusion', alpha=0.7)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training Stability')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Distribution comparison\n",
    "ax2 = axes[0, 1]\n",
    "ax2.hist(test_data.flatten(), bins=100, alpha=0.5, label='Real', density=True)\n",
    "ax2.hist(gan_samples.flatten(), bins=100, alpha=0.5, label='GAN', density=True)\n",
    "ax2.hist(diffusion_samples.flatten(), bins=100, alpha=0.5, label='Diffusion', density=True)\n",
    "ax2.set_xlabel('Value')\n",
    "ax2.set_ylabel('Density')\n",
    "ax2.set_title('Distribution Comparison')\n",
    "ax2.legend()\n",
    "ax2.set_xlim(-4, 4)\n",
    "\n",
    "# 3. Speed comparison\n",
    "ax3 = axes[1, 0]\n",
    "models = ['GAN', 'DDPM', 'DDIM']\n",
    "gen_times = [gan_gen_time, diffusion_gen_time, diffusion_ddim_gen_time]\n",
    "bars = ax3.bar(models, gen_times, color=['red', 'blue', 'green'])\n",
    "ax3.set_ylabel('Generation Time (s)')\n",
    "ax3.set_title(f'Generation Speed ({n_samples} samples)')\n",
    "for bar, t in zip(bars, gen_times):\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "             f'{t:.2f}s', ha='center', va='bottom')\n",
    "\n",
    "# 4. Quality metrics\n",
    "ax4 = axes[1, 1]\n",
    "metrics_names = ['mean_diff', 'std_diff', 'skew_diff', 'acf_mae']\n",
    "x = np.arange(len(metrics_names))\n",
    "width = 0.35\n",
    "\n",
    "gan_vals = [gan_metrics[m] for m in metrics_names]\n",
    "diff_vals = [diffusion_metrics[m] for m in metrics_names]\n",
    "\n",
    "bars1 = ax4.bar(x - width/2, gan_vals, width, label='GAN', color='red', alpha=0.7)\n",
    "bars2 = ax4.bar(x + width/2, diff_vals, width, label='Diffusion', color='blue', alpha=0.7)\n",
    "\n",
    "ax4.set_ylabel('Error (lower is better)')\n",
    "ax4.set_title('Quality Metrics')\n",
    "ax4.set_xticks(x)\n",
    "ax4.set_xticklabels(metrics_names, rotation=45)\n",
    "ax4.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Выводы\n",
    "\n",
    "### GAN (Generative Adversarial Networks)\n",
    "\n",
    "**Преимущества:**\n",
    "- Очень быстрая генерация (один forward pass)\n",
    "- Хорошо изучены, много архитектур\n",
    "- Могут генерировать очень реалистичные данные\n",
    "\n",
    "**Недостатки:**\n",
    "- Mode collapse (потеря разнообразия)\n",
    "- Нестабильное обучение (adversarial training)\n",
    "- Трудно оценить качество без дополнительных метрик\n",
    "- Сложная настройка гиперпараметров\n",
    "\n",
    "### Diffusion Models\n",
    "\n",
    "**Преимущества:**\n",
    "- Стабильное обучение (простой MSE loss)\n",
    "- Высокое качество и разнообразие\n",
    "- Можно оценить likelihood\n",
    "- Гибкое кондиционирование и guidance\n",
    "- Лучше для imputation и conditional generation\n",
    "\n",
    "**Недостатки:**\n",
    "- Медленная генерация (много итераций)\n",
    "- Требует больше памяти при обучении\n",
    "- Более новая область, меньше готовых решений\n",
    "\n",
    "### Рекомендации по выбору\n",
    "\n",
    "| Задача | Рекомендуемый подход |\n",
    "|--------|---------------------|\n",
    "| Real-time генерация | GAN или DDIM |\n",
    "| Высокое качество | Diffusion (DDPM) |\n",
    "| Imputation/Forecasting | CSDI или TimeGrad |\n",
    "| Data augmentation | Оба подхода работают |\n",
    "| Стабильность обучения | Diffusion |\n",
    "| Низкие вычислительные ресурсы | GAN |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models and results\n",
    "torch.save({\n",
    "    'generator_state_dict': gan.generator.state_dict(),\n",
    "    'discriminator_state_dict': gan.discriminator.state_dict(),\n",
    "    'history': gan_history,\n",
    "    'metrics': gan_metrics\n",
    "}, 'gan_model.pt')\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': diffusion.state_dict(),\n",
    "    'losses': diffusion_losses,\n",
    "    'metrics': diffusion_metrics\n",
    "}, 'diffusion_model.pt')\n",
    "\n",
    "# Save comparison results\n",
    "comparison_results = {\n",
    "    'gan_metrics': gan_metrics,\n",
    "    'diffusion_metrics': diffusion_metrics,\n",
    "    'ddim_metrics': ddim_metrics,\n",
    "    'gan_train_time': gan_train_time,\n",
    "    'diffusion_train_time': diffusion_train_time,\n",
    "    'gan_gen_time': gan_gen_time,\n",
    "    'diffusion_gen_time': diffusion_gen_time,\n",
    "    'ddim_gen_time': diffusion_ddim_gen_time\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('comparison_results.json', 'w') as f:\n",
    "    json.dump(comparison_results, f, indent=2)\n",
    "\n",
    "print(\"Models and results saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
