{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion-TS: Генерация синтетических финансовых данных\n",
    "\n",
    "Diffusion-TS (ICLR 2024) - это современная архитектура для генерации синтетических временных рядов с декомпозицией на тренд и сезонность.\n",
    "\n",
    "## Применения в финансах:\n",
    "\n",
    "1. **Data Augmentation**: Увеличение обучающей выборки для ML моделей\n",
    "2. **Scenario Generation**: Генерация сценариев для стресс-тестирования\n",
    "3. **Privacy-preserving**: Создание синтетических данных без раскрытия реальных\n",
    "4. **Backtesting**: Генерация дополнительных исторических данных\n",
    "\n",
    "## Ключевые особенности Diffusion-TS:\n",
    "\n",
    "- **Decomposition**: Разделение на тренд, сезонность и шум\n",
    "- **Interpretable Diffusion**: Понятный процесс генерации\n",
    "- **Conditional Generation**: Генерация с условиями (класс, метки)\n",
    "\n",
    "**Статья**: Shen et al., \"Diffusion-TS: Interpretable Diffusion for General Time Series Generation\" (ICLR 2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Установка зависимостей\n",
    "!pip install torch numpy pandas matplotlib seaborn scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, Optional, Dict, List\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Создание реалистичных финансовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinancialDataGenerator:\n",
    "    \"\"\"Генератор реалистичных финансовых временных рядов.\"\"\"\n",
    "    \n",
    "    def __init__(self, seed: int = 42):\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    def generate_gbm(\n",
    "        self,\n",
    "        n_samples: int,\n",
    "        seq_len: int,\n",
    "        mu: float = 0.0001,\n",
    "        sigma: float = 0.02,\n",
    "        s0: float = 100\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Геометрическое броуновское движение.\n",
    "        \"\"\"\n",
    "        dt = 1.0\n",
    "        returns = np.random.normal(\n",
    "            (mu - 0.5 * sigma**2) * dt,\n",
    "            sigma * np.sqrt(dt),\n",
    "            (n_samples, seq_len)\n",
    "        )\n",
    "        prices = s0 * np.exp(np.cumsum(returns, axis=1))\n",
    "        return prices\n",
    "    \n",
    "    def generate_with_jumps(\n",
    "        self,\n",
    "        n_samples: int,\n",
    "        seq_len: int,\n",
    "        jump_intensity: float = 0.02,\n",
    "        jump_size_mean: float = 0,\n",
    "        jump_size_std: float = 0.05\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        GBM с пуассоновскими скачками (Merton model).\n",
    "        \"\"\"\n",
    "        prices = self.generate_gbm(n_samples, seq_len)\n",
    "        \n",
    "        # Добавляем скачки\n",
    "        jump_times = np.random.poisson(jump_intensity, (n_samples, seq_len))\n",
    "        jump_sizes = np.random.normal(jump_size_mean, jump_size_std, (n_samples, seq_len))\n",
    "        jumps = jump_times * jump_sizes\n",
    "        \n",
    "        prices = prices * np.exp(np.cumsum(jumps, axis=1))\n",
    "        return prices\n",
    "    \n",
    "    def generate_regime_switching(\n",
    "        self,\n",
    "        n_samples: int,\n",
    "        seq_len: int,\n",
    "        regimes: List[Dict] = None\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Режимное переключение (bull/bear markets).\n",
    "        \"\"\"\n",
    "        if regimes is None:\n",
    "            regimes = [\n",
    "                {'mu': 0.001, 'sigma': 0.01, 'prob': 0.6},  # Bull\n",
    "                {'mu': -0.001, 'sigma': 0.03, 'prob': 0.4}  # Bear\n",
    "            ]\n",
    "        \n",
    "        prices = np.zeros((n_samples, seq_len))\n",
    "        regime_labels = np.zeros((n_samples, seq_len), dtype=int)\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            current_regime = 0\n",
    "            s = 100\n",
    "            \n",
    "            for t in range(seq_len):\n",
    "                # Случайное переключение режима\n",
    "                if np.random.random() < 0.05:  # 5% шанс переключения\n",
    "                    current_regime = 1 - current_regime\n",
    "                \n",
    "                regime = regimes[current_regime]\n",
    "                ret = np.random.normal(regime['mu'], regime['sigma'])\n",
    "                s = s * np.exp(ret)\n",
    "                \n",
    "                prices[i, t] = s\n",
    "                regime_labels[i, t] = current_regime\n",
    "        \n",
    "        return prices, regime_labels\n",
    "    \n",
    "    def generate_correlated_assets(\n",
    "        self,\n",
    "        n_samples: int,\n",
    "        seq_len: int,\n",
    "        n_assets: int = 5,\n",
    "        correlation: float = 0.5\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Коррелированные активы.\n",
    "        \"\"\"\n",
    "        # Ковариационная матрица\n",
    "        cov = np.full((n_assets, n_assets), correlation * 0.02**2)\n",
    "        np.fill_diagonal(cov, 0.02**2)\n",
    "        \n",
    "        prices = np.zeros((n_samples, seq_len, n_assets))\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            returns = np.random.multivariate_normal(\n",
    "                np.full(n_assets, 0.0001),\n",
    "                cov,\n",
    "                seq_len\n",
    "            )\n",
    "            prices[i] = 100 * np.exp(np.cumsum(returns, axis=0))\n",
    "        \n",
    "        return prices\n",
    "\n",
    "# Генерируем данные\n",
    "generator = FinancialDataGenerator(seed=42)\n",
    "\n",
    "n_samples = 1000\n",
    "seq_len = 100\n",
    "\n",
    "# Простой GBM\n",
    "gbm_data = generator.generate_gbm(n_samples, seq_len)\n",
    "\n",
    "# GBM со скачками\n",
    "jump_data = generator.generate_with_jumps(n_samples // 2, seq_len)\n",
    "\n",
    "# Режимное переключение\n",
    "regime_data, regime_labels = generator.generate_regime_switching(n_samples // 2, seq_len)\n",
    "\n",
    "print(f\"GBM data: {gbm_data.shape}\")\n",
    "print(f\"Jump data: {jump_data.shape}\")\n",
    "print(f\"Regime data: {regime_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация разных типов данных\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# GBM\n",
    "ax1 = axes[0, 0]\n",
    "for i in range(10):\n",
    "    ax1.plot(gbm_data[i], alpha=0.7)\n",
    "ax1.set_title('Geometric Brownian Motion')\n",
    "ax1.set_xlabel('Time')\n",
    "ax1.set_ylabel('Price')\n",
    "\n",
    "# Jump diffusion\n",
    "ax2 = axes[0, 1]\n",
    "for i in range(10):\n",
    "    ax2.plot(jump_data[i], alpha=0.7)\n",
    "ax2.set_title('Jump Diffusion (Merton Model)')\n",
    "ax2.set_xlabel('Time')\n",
    "ax2.set_ylabel('Price')\n",
    "\n",
    "# Regime switching\n",
    "ax3 = axes[1, 0]\n",
    "for i in range(5):\n",
    "    ax3.plot(regime_data[i], alpha=0.7, label=f'Sample {i+1}')\n",
    "ax3.set_title('Regime Switching')\n",
    "ax3.set_xlabel('Time')\n",
    "ax3.set_ylabel('Price')\n",
    "\n",
    "# Распределение доходностей\n",
    "ax4 = axes[1, 1]\n",
    "gbm_returns = np.diff(np.log(gbm_data), axis=1).flatten()\n",
    "jump_returns = np.diff(np.log(jump_data), axis=1).flatten()\n",
    "\n",
    "ax4.hist(gbm_returns, bins=100, alpha=0.5, label='GBM', density=True)\n",
    "ax4.hist(jump_returns, bins=100, alpha=0.5, label='Jump', density=True)\n",
    "ax4.set_title('Return Distributions')\n",
    "ax4.set_xlabel('Log Return')\n",
    "ax4.set_ylabel('Density')\n",
    "ax4.legend()\n",
    "ax4.set_xlim(-0.1, 0.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Подготовка данных для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объединяем и нормализуем данные\n",
    "all_data = np.concatenate([gbm_data, jump_data, regime_data], axis=0)\n",
    "\n",
    "# Нормализация (log returns)\n",
    "log_returns = np.diff(np.log(all_data + 1e-10), axis=1)\n",
    "\n",
    "# Стандартизация\n",
    "mean = log_returns.mean()\n",
    "std = log_returns.std()\n",
    "normalized_data = (log_returns - mean) / std\n",
    "\n",
    "print(f\"Data shape: {normalized_data.shape}\")\n",
    "print(f\"Mean: {normalized_data.mean():.6f}, Std: {normalized_data.std():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DiffusionTSConfig:\n",
    "    \"\"\"Конфигурация Diffusion-TS.\"\"\"\n",
    "    seq_len: int = 99  # после diff\n",
    "    hidden_dim: int = 128\n",
    "    num_heads: int = 4\n",
    "    num_layers: int = 4\n",
    "    diffusion_steps: int = 100\n",
    "    beta_start: float = 0.0001\n",
    "    beta_end: float = 0.02\n",
    "    # Decomposition parameters\n",
    "    trend_poly_degree: int = 3\n",
    "    seasonal_periods: List[int] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.seasonal_periods is None:\n",
    "            self.seasonal_periods = [24, 12]  # daily, half-day\n",
    "\n",
    "config = DiffusionTSConfig()\n",
    "print(f\"Config: {config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data: np.ndarray):\n",
    "        # Добавляем dimension для channels\n",
    "        self.data = torch.FloatTensor(data).unsqueeze(-1)  # [N, seq_len, 1]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Split\n",
    "n = len(normalized_data)\n",
    "train_size = int(0.8 * n)\n",
    "\n",
    "train_data = normalized_data[:train_size]\n",
    "test_data = normalized_data[train_size:]\n",
    "\n",
    "train_dataset = TimeSeriesDataset(train_data)\n",
    "test_dataset = TimeSeriesDataset(test_data)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}, Test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Diffusion-TS Architecture\n",
    "\n",
    "Diffusion-TS использует interpretable decomposition:\n",
    "\n",
    "$$x = \\text{trend}(x) + \\text{seasonal}(x) + \\text{residual}(x)$$\n",
    "\n",
    "Диффузия применяется к residual компоненте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrendDecomposition(nn.Module):\n",
    "    \"\"\"Извлечение тренда через moving average.\"\"\"\n",
    "    \n",
    "    def __init__(self, kernel_size: int = 25):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        # Learnable moving average\n",
    "        self.avg = nn.AvgPool1d(kernel_size, stride=1, padding=kernel_size//2)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch, seq_len, channels]\n",
    "        Returns:\n",
    "            trend, residual\n",
    "        \"\"\"\n",
    "        # [batch, channels, seq_len]\n",
    "        x_t = x.transpose(1, 2)\n",
    "        trend = self.avg(x_t)\n",
    "        \n",
    "        # Обрезаем до оригинальной длины\n",
    "        trend = trend[:, :, :x.size(1)]\n",
    "        trend = trend.transpose(1, 2)\n",
    "        \n",
    "        residual = x - trend\n",
    "        \n",
    "        return trend, residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeasonalDecomposition(nn.Module):\n",
    "    \"\"\"Извлечение сезонности через Fourier.\"\"\"\n",
    "    \n",
    "    def __init__(self, seq_len: int, n_harmonics: int = 5):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.n_harmonics = n_harmonics\n",
    "        \n",
    "        # Learnable Fourier coefficients\n",
    "        self.cos_weights = nn.Parameter(torch.randn(n_harmonics) * 0.01)\n",
    "        self.sin_weights = nn.Parameter(torch.randn(n_harmonics) * 0.01)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch, seq_len, channels]\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        device = x.device\n",
    "        \n",
    "        # Time indices\n",
    "        t = torch.arange(self.seq_len, device=device).float() / self.seq_len\n",
    "        \n",
    "        # Fourier basis\n",
    "        seasonal = torch.zeros(self.seq_len, device=device)\n",
    "        for k in range(self.n_harmonics):\n",
    "            freq = (k + 1) * 2 * np.pi\n",
    "            seasonal = seasonal + self.cos_weights[k] * torch.cos(freq * t)\n",
    "            seasonal = seasonal + self.sin_weights[k] * torch.sin(freq * t)\n",
    "        \n",
    "        # Expand to batch and channels\n",
    "        seasonal = seasonal.unsqueeze(0).unsqueeze(-1)  # [1, seq_len, 1]\n",
    "        seasonal = seasonal.expand(batch_size, -1, x.size(-1))\n",
    "        \n",
    "        residual = x - seasonal\n",
    "        \n",
    "        return seasonal, residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeEmbedding(nn.Module):\n",
    "    \"\"\"Sinusoidal time embedding.\"\"\"\n",
    "    \n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(dim * 4, dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        half_dim = self.dim // 2\n",
    "        emb = np.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=t.device) * -emb)\n",
    "        emb = t[:, None] * emb[None, :]\n",
    "        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)\n",
    "        return self.mlp(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Residual block with time conditioning.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_dim: int, out_dim: int, time_dim: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(in_dim)\n",
    "        self.conv1 = nn.Linear(in_dim, out_dim)\n",
    "        self.norm2 = nn.LayerNorm(out_dim)\n",
    "        self.conv2 = nn.Linear(out_dim, out_dim)\n",
    "        \n",
    "        self.time_proj = nn.Linear(time_dim, out_dim * 2)\n",
    "        \n",
    "        if in_dim != out_dim:\n",
    "            self.skip = nn.Linear(in_dim, out_dim)\n",
    "        else:\n",
    "            self.skip = nn.Identity()\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, t_emb: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch, seq_len, in_dim]\n",
    "            t_emb: [batch, time_dim]\n",
    "        \"\"\"\n",
    "        h = self.norm1(x)\n",
    "        h = F.gelu(self.conv1(h))\n",
    "        \n",
    "        # Time conditioning\n",
    "        t_params = self.time_proj(t_emb)  # [batch, out_dim * 2]\n",
    "        scale, shift = t_params.chunk(2, dim=-1)\n",
    "        h = h * (1 + scale.unsqueeze(1)) + shift.unsqueeze(1)\n",
    "        \n",
    "        h = self.norm2(h)\n",
    "        h = F.gelu(self.conv2(h))\n",
    "        \n",
    "        return self.skip(x) + h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Self-attention block.\"\"\"\n",
    "    \n",
    "    def __init__(self, dim: int, num_heads: int, time_dim: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = nn.MultiheadAttention(dim, num_heads, batch_first=True)\n",
    "        \n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(dim * 4, dim)\n",
    "        )\n",
    "        \n",
    "        self.time_proj = nn.Linear(time_dim, dim * 2)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, t_emb: torch.Tensor) -> torch.Tensor:\n",
    "        # Self-attention\n",
    "        h = self.norm1(x)\n",
    "        h, _ = self.attn(h, h, h)\n",
    "        x = x + h\n",
    "        \n",
    "        # FFN with time conditioning\n",
    "        h = self.norm2(x)\n",
    "        \n",
    "        t_params = self.time_proj(t_emb)\n",
    "        scale, shift = t_params.chunk(2, dim=-1)\n",
    "        h = h * (1 + scale.unsqueeze(1)) + shift.unsqueeze(1)\n",
    "        \n",
    "        h = self.ffn(h)\n",
    "        x = x + h\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionTS(nn.Module):\n",
    "    \"\"\"Diffusion-TS: Interpretable Diffusion for Time Series Generation.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: DiffusionTSConfig):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.config = config\n",
    "        \n",
    "        # Decomposition modules\n",
    "        self.trend_decomp = TrendDecomposition(kernel_size=25)\n",
    "        self.seasonal_decomp = SeasonalDecomposition(config.seq_len, n_harmonics=5)\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_proj = nn.Linear(1, config.hidden_dim)\n",
    "        \n",
    "        # Time embedding\n",
    "        self.time_embed = TimeEmbedding(config.hidden_dim)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, config.seq_len, config.hidden_dim) * 0.02)\n",
    "        \n",
    "        # Encoder blocks\n",
    "        self.encoder = nn.ModuleList([\n",
    "            TransformerBlock(config.hidden_dim, config.num_heads, config.hidden_dim)\n",
    "            for _ in range(config.num_layers // 2)\n",
    "        ])\n",
    "        \n",
    "        # Decoder blocks\n",
    "        self.decoder = nn.ModuleList([\n",
    "            TransformerBlock(config.hidden_dim, config.num_heads, config.hidden_dim)\n",
    "            for _ in range(config.num_layers // 2)\n",
    "        ])\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_proj = nn.Sequential(\n",
    "            nn.LayerNorm(config.hidden_dim),\n",
    "            nn.Linear(config.hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "        # Noise schedule\n",
    "        self._setup_schedule()\n",
    "    \n",
    "    def _setup_schedule(self):\n",
    "        \"\"\"Cosine noise schedule.\"\"\"\n",
    "        T = self.config.diffusion_steps\n",
    "        s = 0.008\n",
    "        \n",
    "        steps = torch.arange(T + 1) / T\n",
    "        alphas_bar = torch.cos((steps + s) / (1 + s) * np.pi / 2) ** 2\n",
    "        alphas_bar = alphas_bar / alphas_bar[0]\n",
    "        \n",
    "        betas = 1 - alphas_bar[1:] / alphas_bar[:-1]\n",
    "        betas = betas.clamp(max=0.999)\n",
    "        \n",
    "        alphas = 1 - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "        \n",
    "        self.register_buffer('betas', betas)\n",
    "        self.register_buffer('alphas', alphas)\n",
    "        self.register_buffer('alphas_cumprod', alphas_cumprod)\n",
    "        self.register_buffer('sqrt_alphas_cumprod', torch.sqrt(alphas_cumprod))\n",
    "        self.register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1 - alphas_cumprod))\n",
    "    \n",
    "    def forward_diffusion(\n",
    "        self,\n",
    "        x0: torch.Tensor,\n",
    "        t: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Forward diffusion process.\"\"\"\n",
    "        noise = torch.randn_like(x0)\n",
    "        \n",
    "        sqrt_alpha = self.sqrt_alphas_cumprod[t].view(-1, 1, 1)\n",
    "        sqrt_one_minus_alpha = self.sqrt_one_minus_alphas_cumprod[t].view(-1, 1, 1)\n",
    "        \n",
    "        xt = sqrt_alpha * x0 + sqrt_one_minus_alpha * noise\n",
    "        \n",
    "        return xt, noise\n",
    "    \n",
    "    def denoise(\n",
    "        self,\n",
    "        xt: torch.Tensor,\n",
    "        t: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Predict noise from noisy input.\n",
    "        \n",
    "        Args:\n",
    "            xt: [batch, seq_len, 1] - noisy data\n",
    "            t: [batch] - diffusion steps\n",
    "        \"\"\"\n",
    "        batch_size = xt.shape[0]\n",
    "        \n",
    "        # Input projection\n",
    "        h = self.input_proj(xt)  # [batch, seq_len, hidden_dim]\n",
    "        \n",
    "        # Add positional embedding\n",
    "        h = h + self.pos_embed\n",
    "        \n",
    "        # Time embedding\n",
    "        t_emb = self.time_embed(t.float())\n",
    "        \n",
    "        # Encoder\n",
    "        for block in self.encoder:\n",
    "            h = block(h, t_emb)\n",
    "        \n",
    "        # Decoder\n",
    "        for block in self.decoder:\n",
    "            h = block(h, t_emb)\n",
    "        \n",
    "        # Output\n",
    "        noise_pred = self.output_proj(h)\n",
    "        \n",
    "        return noise_pred\n",
    "    \n",
    "    def compute_loss(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute training loss.\"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        device = x.device\n",
    "        \n",
    "        # Extract residual (apply decomposition)\n",
    "        trend, residual = self.trend_decomp(x)\n",
    "        seasonal, residual = self.seasonal_decomp(residual)\n",
    "        \n",
    "        # Random diffusion step\n",
    "        t = torch.randint(0, self.config.diffusion_steps, (batch_size,), device=device)\n",
    "        \n",
    "        # Forward diffusion on residual\n",
    "        xt, noise = self.forward_diffusion(residual, t)\n",
    "        \n",
    "        # Predict noise\n",
    "        noise_pred = self.denoise(xt, t)\n",
    "        \n",
    "        # MSE loss\n",
    "        loss = F.mse_loss(noise_pred, noise)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def sample(self, n_samples: int, device: torch.device) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Generate synthetic samples.\n",
    "        \n",
    "        Args:\n",
    "            n_samples: number of samples to generate\n",
    "            device: torch device\n",
    "            \n",
    "        Returns:\n",
    "            samples: [n_samples, seq_len, 1]\n",
    "        \"\"\"\n",
    "        # Start from noise\n",
    "        x = torch.randn(n_samples, self.config.seq_len, 1, device=device)\n",
    "        \n",
    "        # Reverse diffusion\n",
    "        for t in tqdm(reversed(range(self.config.diffusion_steps)), desc='Sampling', leave=False):\n",
    "            t_tensor = torch.full((n_samples,), t, device=device)\n",
    "            \n",
    "            # Predict noise\n",
    "            noise_pred = self.denoise(x, t_tensor)\n",
    "            \n",
    "            # DDPM update\n",
    "            alpha = self.alphas[t]\n",
    "            alpha_bar = self.alphas_cumprod[t]\n",
    "            beta = self.betas[t]\n",
    "            \n",
    "            mean = (1 / torch.sqrt(alpha)) * (\n",
    "                x - (beta / torch.sqrt(1 - alpha_bar)) * noise_pred\n",
    "            )\n",
    "            \n",
    "            if t > 0:\n",
    "                alpha_bar_prev = self.alphas_cumprod[t - 1]\n",
    "                variance = beta * (1 - alpha_bar_prev) / (1 - alpha_bar)\n",
    "                noise = torch.randn_like(x)\n",
    "                x = mean + torch.sqrt(variance) * noise\n",
    "            else:\n",
    "                x = mean\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create model\n",
    "model = DiffusionTS(config).to(device)\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = model.compute_loss(batch)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "def validate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            loss = model.compute_loss(batch)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n",
    "\n",
    "n_epochs = 50\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in tqdm(range(n_epochs), desc=\"Training\"):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "    val_loss = validate(model, test_loader, device)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_state = model.state_dict().copy()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}: Train={train_loss:.4f}, Val={val_loss:.4f}\")\n",
    "\n",
    "model.load_state_dict(best_state)\n",
    "print(f\"\\nBest Val Loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Diffusion-TS Training Progress')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Генерация синтетических данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic samples\n",
    "n_synthetic = 200\n",
    "print(f\"Generating {n_synthetic} synthetic samples...\")\n",
    "\n",
    "model.eval()\n",
    "synthetic_samples = model.sample(n_synthetic, device)\n",
    "synthetic_samples = synthetic_samples.cpu().numpy().squeeze(-1)\n",
    "\n",
    "print(f\"Synthetic samples shape: {synthetic_samples.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Real samples\n",
    "ax1 = axes[0, 0]\n",
    "for i in range(20):\n",
    "    ax1.plot(test_data[i], alpha=0.5)\n",
    "ax1.set_title('Real Samples (Returns)')\n",
    "ax1.set_xlabel('Time')\n",
    "ax1.set_ylabel('Normalized Return')\n",
    "\n",
    "# Synthetic samples\n",
    "ax2 = axes[0, 1]\n",
    "for i in range(20):\n",
    "    ax2.plot(synthetic_samples[i], alpha=0.5)\n",
    "ax2.set_title('Synthetic Samples (Returns)')\n",
    "ax2.set_xlabel('Time')\n",
    "ax2.set_ylabel('Normalized Return')\n",
    "\n",
    "# Distribution comparison\n",
    "ax3 = axes[1, 0]\n",
    "ax3.hist(test_data.flatten(), bins=100, alpha=0.5, label='Real', density=True)\n",
    "ax3.hist(synthetic_samples.flatten(), bins=100, alpha=0.5, label='Synthetic', density=True)\n",
    "ax3.set_title('Distribution Comparison')\n",
    "ax3.set_xlabel('Value')\n",
    "ax3.set_ylabel('Density')\n",
    "ax3.legend()\n",
    "\n",
    "# Autocorrelation comparison\n",
    "ax4 = axes[1, 1]\n",
    "\n",
    "def autocorr(x, lag):\n",
    "    return np.corrcoef(x[:-lag], x[lag:])[0, 1]\n",
    "\n",
    "lags = range(1, 30)\n",
    "real_acf = [autocorr(test_data.flatten(), lag) for lag in lags]\n",
    "synth_acf = [autocorr(synthetic_samples.flatten(), lag) for lag in lags]\n",
    "\n",
    "ax4.plot(lags, real_acf, 'b-o', label='Real')\n",
    "ax4.plot(lags, synth_acf, 'r-s', label='Synthetic')\n",
    "ax4.set_title('Autocorrelation Comparison')\n",
    "ax4.set_xlabel('Lag')\n",
    "ax4.set_ylabel('ACF')\n",
    "ax4.legend()\n",
    "ax4.axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Оценка качества синтетических данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_statistics(data: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"Вычислить статистики временного ряда.\"\"\"\n",
    "    flat = data.flatten()\n",
    "    \n",
    "    stats = {\n",
    "        'mean': np.mean(flat),\n",
    "        'std': np.std(flat),\n",
    "        'skewness': pd.Series(flat).skew(),\n",
    "        'kurtosis': pd.Series(flat).kurtosis(),\n",
    "        'min': np.min(flat),\n",
    "        'max': np.max(flat),\n",
    "        'acf_1': autocorr(flat, 1) if len(flat) > 1 else 0,\n",
    "        'acf_5': autocorr(flat, 5) if len(flat) > 5 else 0,\n",
    "    }\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Compare statistics\n",
    "real_stats = compute_statistics(test_data)\n",
    "synth_stats = compute_statistics(synthetic_samples)\n",
    "\n",
    "print(\"=== Statistical Comparison ===\")\n",
    "print(f\"{'Metric':<15} {'Real':<15} {'Synthetic':<15} {'Diff %':<10}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for key in real_stats:\n",
    "    real_val = real_stats[key]\n",
    "    synth_val = synth_stats[key]\n",
    "    if abs(real_val) > 1e-10:\n",
    "        diff_pct = abs(real_val - synth_val) / abs(real_val) * 100\n",
    "    else:\n",
    "        diff_pct = 0\n",
    "    print(f\"{key:<15} {real_val:<15.4f} {synth_val:<15.4f} {diff_pct:<10.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE visualization\n",
    "def prepare_for_tsne(data, n_samples=200):\n",
    "    \"\"\"Подготовить данные для t-SNE.\"\"\"\n",
    "    if len(data) > n_samples:\n",
    "        idx = np.random.choice(len(data), n_samples, replace=False)\n",
    "        data = data[idx]\n",
    "    return data.reshape(len(data), -1)\n",
    "\n",
    "real_flat = prepare_for_tsne(test_data, 200)\n",
    "synth_flat = prepare_for_tsne(synthetic_samples, 200)\n",
    "\n",
    "# Combine for t-SNE\n",
    "combined = np.vstack([real_flat, synth_flat])\n",
    "labels = np.array([0] * len(real_flat) + [1] * len(synth_flat))\n",
    "\n",
    "print(\"Computing t-SNE...\")\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "embedded = tsne.fit_transform(combined)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(\n",
    "    embedded[:, 0], embedded[:, 1],\n",
    "    c=labels, cmap='coolwarm', alpha=0.6, s=20\n",
    ")\n",
    "plt.colorbar(scatter, ticks=[0, 1], label='Data Type')\n",
    "plt.title('t-SNE Visualization: Real vs Synthetic')\n",
    "plt.xlabel('t-SNE 1')\n",
    "plt.ylabel('t-SNE 2')\n",
    "\n",
    "# Legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor='blue', label='Real'),\n",
    "                   Patch(facecolor='red', label='Synthetic')]\n",
    "plt.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminative Score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def compute_discriminative_score(real: np.ndarray, synthetic: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Вычислить discriminative score.\n",
    "    Чем ближе к 0.5, тем лучше (труднее различить real и synthetic).\n",
    "    \"\"\"\n",
    "    n_real = len(real)\n",
    "    n_synth = len(synthetic)\n",
    "    \n",
    "    X = np.vstack([real.reshape(n_real, -1), synthetic.reshape(n_synth, -1)])\n",
    "    y = np.array([0] * n_real + [1] * n_synth)\n",
    "    \n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    scores = cross_val_score(clf, X, y, cv=5)\n",
    "    \n",
    "    return scores.mean()\n",
    "\n",
    "disc_score = compute_discriminative_score(test_data, synthetic_samples)\n",
    "print(f\"Discriminative Score: {disc_score:.4f}\")\n",
    "print(f\"(0.5 = perfect, classifier cannot distinguish real from synthetic)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum Mean Discrepancy (MMD)\n",
    "def compute_mmd(X: np.ndarray, Y: np.ndarray, kernel='rbf', gamma=1.0) -> float:\n",
    "    \"\"\"\n",
    "    Вычислить MMD между двумя распределениями.\n",
    "    Чем ближе к 0, тем более похожи распределения.\n",
    "    \"\"\"\n",
    "    X = X.reshape(len(X), -1)\n",
    "    Y = Y.reshape(len(Y), -1)\n",
    "    \n",
    "    XX = pairwise_distances(X, X, metric='rbf', gamma=gamma)\n",
    "    YY = pairwise_distances(Y, Y, metric='rbf', gamma=gamma)\n",
    "    XY = pairwise_distances(X, Y, metric='rbf', gamma=gamma)\n",
    "    \n",
    "    mmd = XX.mean() + YY.mean() - 2 * XY.mean()\n",
    "    return mmd\n",
    "\n",
    "mmd_score = compute_mmd(test_data[:200], synthetic_samples[:200])\n",
    "print(f\"MMD Score: {mmd_score:.6f}\")\n",
    "print(f\"(Closer to 0 = more similar distributions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Конвертация обратно в цены"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def returns_to_prices(returns: np.ndarray, initial_price: float = 100) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Конвертировать log returns обратно в цены.\n",
    "    \"\"\"\n",
    "    # Денормализация\n",
    "    returns_denorm = returns * std + mean\n",
    "    \n",
    "    # Cumulative returns\n",
    "    cum_returns = np.cumsum(returns_denorm, axis=1)\n",
    "    \n",
    "    # Prices\n",
    "    prices = initial_price * np.exp(cum_returns)\n",
    "    \n",
    "    return prices\n",
    "\n",
    "# Convert synthetic returns to prices\n",
    "synthetic_prices = returns_to_prices(synthetic_samples)\n",
    "\n",
    "# Convert real returns to prices for comparison\n",
    "real_prices = returns_to_prices(test_data)\n",
    "\n",
    "print(f\"Synthetic prices shape: {synthetic_prices.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize synthetic price paths\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Real prices\n",
    "ax1 = axes[0]\n",
    "for i in range(30):\n",
    "    ax1.plot(real_prices[i], alpha=0.5)\n",
    "ax1.set_title('Real Price Paths')\n",
    "ax1.set_xlabel('Time')\n",
    "ax1.set_ylabel('Price')\n",
    "\n",
    "# Synthetic prices\n",
    "ax2 = axes[1]\n",
    "for i in range(30):\n",
    "    ax2.plot(synthetic_prices[i], alpha=0.5)\n",
    "ax2.set_title('Synthetic Price Paths')\n",
    "ax2.set_xlabel('Time')\n",
    "ax2.set_ylabel('Price')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Применение: Data Augmentation для ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def create_trading_labels(prices: np.ndarray, threshold: float = 0.01) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Создать метки для классификации: buy (1) если цена вырастет > threshold.\n",
    "    \"\"\"\n",
    "    # Доходность на следующий период\n",
    "    returns = np.diff(np.log(prices), axis=1)\n",
    "    \n",
    "    # Последняя доходность как цель\n",
    "    labels = (returns[:, -1] > threshold).astype(int)\n",
    "    \n",
    "    # Features - все кроме последней точки\n",
    "    features = prices[:, :-1]\n",
    "    \n",
    "    return features, labels\n",
    "\n",
    "# Создаём датасет\n",
    "X_real, y_real = create_trading_labels(real_prices[:500])\n",
    "X_synth, y_synth = create_trading_labels(synthetic_prices[:200])\n",
    "\n",
    "print(f\"Real: {X_real.shape}, labels distribution: {np.bincount(y_real)}\")\n",
    "print(f\"Synthetic: {X_synth.shape}, labels distribution: {np.bincount(y_synth)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: Train with and without augmentation\n",
    "\n",
    "# Split real data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_real, y_real, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Model 1: Train only on real data\n",
    "clf_real = LogisticRegression(max_iter=1000, random_state=42)\n",
    "clf_real.fit(X_train, y_train)\n",
    "y_pred_real = clf_real.predict(X_test)\n",
    "acc_real = accuracy_score(y_test, y_pred_real)\n",
    "f1_real = f1_score(y_test, y_pred_real)\n",
    "\n",
    "# Model 2: Train on real + synthetic (augmented)\n",
    "X_augmented = np.vstack([X_train, X_synth])\n",
    "y_augmented = np.concatenate([y_train, y_synth])\n",
    "\n",
    "clf_aug = LogisticRegression(max_iter=1000, random_state=42)\n",
    "clf_aug.fit(X_augmented, y_augmented)\n",
    "y_pred_aug = clf_aug.predict(X_test)\n",
    "acc_aug = accuracy_score(y_test, y_pred_aug)\n",
    "f1_aug = f1_score(y_test, y_pred_aug)\n",
    "\n",
    "print(\"=== Data Augmentation Results ===\")\n",
    "print(f\"\\nTrained on Real Data Only:\")\n",
    "print(f\"  Accuracy: {acc_real:.4f}\")\n",
    "print(f\"  F1 Score: {f1_real:.4f}\")\n",
    "\n",
    "print(f\"\\nTrained on Real + Synthetic (Augmented):\")\n",
    "print(f\"  Accuracy: {acc_aug:.4f}\")\n",
    "print(f\"  F1 Score: {f1_aug:.4f}\")\n",
    "\n",
    "print(f\"\\nImprovement:\")\n",
    "print(f\"  Accuracy: {(acc_aug - acc_real) / acc_real * 100:+.1f}%\")\n",
    "print(f\"  F1 Score: {(f1_aug - f1_real) / f1_real * 100:+.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Выводы\n",
    "\n",
    "### Преимущества Diffusion-TS:\n",
    "\n",
    "1. **Реалистичные данные**: Сохраняет статистические свойства оригинала\n",
    "2. **Interpretable**: Декомпозиция на тренд/сезонность/residual\n",
    "3. **Data Augmentation**: Улучшает качество ML моделей\n",
    "4. **Privacy**: Можно делиться синтетическими данными вместо реальных\n",
    "\n",
    "### Ограничения:\n",
    "\n",
    "1. **Вычислительная сложность**: Генерация требует много шагов\n",
    "2. **Tail events**: Может недооценивать экстремальные события\n",
    "3. **Multivariate**: Сложнее для многомерных рядов\n",
    "\n",
    "### Рекомендации:\n",
    "\n",
    "- Валидируйте качество синтетических данных статистически\n",
    "- Используйте discriminative score для оценки\n",
    "- Комбинируйте с реальными данными для augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and samples\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'config': config,\n",
    "    'normalization': {'mean': mean, 'std': std}\n",
    "}, 'diffusion_ts_model.pt')\n",
    "\n",
    "np.savez(\n",
    "    'synthetic_financial_data.npz',\n",
    "    returns=synthetic_samples,\n",
    "    prices=synthetic_prices,\n",
    "    normalization={'mean': mean, 'std': std}\n",
    ")\n",
    "\n",
    "print(\"Model and samples saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
