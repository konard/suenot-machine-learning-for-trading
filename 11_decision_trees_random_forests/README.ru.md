# Случайные леса — Стратегия лонг-шорт для японских акций

В этой главе мы изучим два новых класса моделей машинного обучения для трейдинга: деревья решений и случайные леса. Мы увидим, как деревья решений обучаются правилам на основе данных, которые кодируют нелинейные зависимости между входными и выходными переменными. Мы покажем, как обучить дерево решений и использовать его для прогнозирования в задачах регрессии и классификации, таких как доходность активов и движение цен. Мы также визуализируем и интерпретируем правила, выученные моделью, и настроим гиперпараметры модели для оптимизации компромисса между смещением и дисперсией и предотвращения переобучения.

Деревья решений важны не только как самостоятельные модели, но и часто используются как компоненты других моделей. Во второй части этой главы мы представим ансамблевые модели, которые объединяют несколько отдельных моделей для получения единого агрегированного прогноза с меньшей дисперсией ошибки предсказания.

Мы рассмотрим бутстреп-агрегирование, часто называемое бэггингом, как один из нескольких методов рандомизации построения отдельных моделей и снижения корреляции ошибок предсказания, допускаемых компонентами ансамбля. Мы покажем, как бэггинг эффективно снижает дисперсию, и научимся настраивать, обучать и тюнить случайные леса. Мы увидим, как случайные леса как ансамбль большого количества деревьев решений могут значительно снизить ошибки предсказания ценой некоторой потери в интерпретируемости.

Затем мы перейдём к построению торговой стратегии лонг-шорт, которая использует ансамбль случайного леса для генерации прибыльных сигналов для японских акций с высокой капитализацией за последние три года. Мы получим и подготовим данные о ценах акций, настроим гиперпараметры модели случайного леса и проведём бэктестинг торговых правил на основе сигналов модели. Полученная стратегия лонг-шорт использует машинное обучение, а не коинтеграционную связь, которую мы видели в главе 9 о моделях временных рядов, для идентификации и торговли корзинами ценных бумаг, цены которых, вероятно, будут двигаться в противоположных направлениях в течение заданного инвестиционного горизонта.

## Содержание

1. [Деревья решений: обучение правилам на основе данных](#деревья-решений-обучение-правилам-на-основе-данных)
2. [Пример кода: деревья решений на практике](#пример-кода-деревья-решений-на-практике)
    * [Данные: месячная доходность акций и признаки](#данные-месячная-доходность-акций-и-признаки)
    * [Построение регрессионного дерева с данными временных рядов](#построение-регрессионного-дерева-с-данными-временных-рядов)
    * [Построение классификационного дерева](#построение-классификационного-дерева)
    * [Визуализация дерева решений](#визуализация-дерева-решений)
    * [Переобучение и регуляризация](#переобучение-и-регуляризация)
    * [Как настраивать гиперпараметры](#как-настраивать-гиперпараметры)
3. [Случайные леса: улучшенные прогнозы с ансамблями](#случайные-леса-улучшенные-прогнозы-с-ансамблями)
    * [Почему ансамблевые модели работают лучше](#почему-ансамблевые-модели-работают-лучше)
    * [Пример кода: как бэггинг снижает дисперсию модели](#пример-кода-как-бэггинг-снижает-дисперсию-модели)
    * [Пример кода: как обучить и настроить случайный лес](#пример-кода-как-обучить-и-настроить-случайный-лес)
4. [Пример кода: сигналы лонг-шорт для японских акций с LightGBM](#пример-кода-сигналы-лонг-шорт-для-японских-акций-с-lightgbm)
    * [Пользовательский бандл Zipline](#пользовательский-бандл-zipline)
    * [Инженерия признаков](#инженерия-признаков)
    * [Настройка модели случайного леса LightGBM](#настройка-модели-случайного-леса-lightgbm)
    * [Оценка сигналов с Alphalens](#оценка-сигналов-с-alphalens)
    * [Бэктестинг с Zipline](#бэктестинг-с-zipline)

## Деревья решений: обучение правилам на основе данных

Деревья решений — это алгоритм машинного обучения, который прогнозирует значение целевой переменной на основе правил принятия решений, выученных из данных. Алгоритм может применяться к задачам регрессии и классификации путём изменения целевой функции, которая управляет тем, как алгоритм обучается правилам.

Мы обсудим, как деревья решений используют правила для прогнозирования, как обучать их для предсказания (непрерывной) доходности, а также (категориальных) направлений движения цен, и как их эффективно интерпретировать, визуализировать и настраивать.

## Пример кода: деревья решений на практике

Ноутбук [decision_trees](01_decision_trees.ipynb) демонстрирует, как использовать модели на основе деревьев для получения инсайтов и прогнозирования. Мы будем прогнозировать доходность для демонстрации регрессионных деревьев и положительные или отрицательные движения цен активов для классификации.

### Данные: месячная доходность акций и признаки

Мы используем вариацию набора данных, созданного в [главе 4 «Исследование альфа-факторов»](../04_alpha_factor_research). Он состоит из дневных цен акций, предоставленных Quandl за период 2010-2017 годов, и различных сконструированных признаков.
- Подробности можно найти в ноутбуке [data_prep](00_data_prep.ipynb).

### Построение регрессионного дерева с данными временных рядов

Регрессионные деревья делают прогнозы на основе среднего значения результата для обучающих выборок, назначенных данному узлу, и обычно используют среднеквадратичную ошибку для выбора оптимальных правил во время рекурсивного бинарного разбиения.

### Построение классификационного дерева

Классификационное дерево работает так же, как регрессионная версия, за исключением того, что категориальная природа результата требует иного подхода к прогнозированию и измерению потерь. В то время как регрессионное дерево прогнозирует отклик для наблюдения, назначенного листовому узлу, используя среднее значение результата связанных обучающих выборок, классификационное дерево использует моду, то есть наиболее распространённый класс среди обучающих выборок в соответствующей области. Классификационное дерево также может генерировать вероятностные прогнозы на основе относительных частот классов.

### Визуализация дерева решений

Вы можете визуализировать дерево с помощью библиотеки [graphviz](https://graphviz.gitlab.io/download/), поскольку sklearn может выводить описание дерева на языке .dot, используемом этой библиотекой. Вы можете настроить вывод для включения меток признаков и классов и ограничить количество уровней, чтобы график оставался читаемым.

### Переобучение и регуляризация

Деревья решений имеют сильную тенденцию к переобучению, особенно когда набор данных имеет большое количество признаков по сравнению с количеством выборок. Ноутбук [decision_trees](01_decision_trees.ipynb) объясняет соответствующие гиперпараметры регуляризации и демонстрирует их использование.

### Как настраивать гиперпараметры

Ноутбук также демонстрирует использование кросс-валидации, включая класс [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) из `sklearn` для исчерпывающего поиска по комбинациям гиперпараметров.

## Случайные леса: улучшенные прогнозы с ансамблями

Деревья решений полезны не только своей прозрачностью и интерпретируемостью, но также являются фундаментальными строительными блоками для гораздо более мощных ансамблевых моделей, которые объединяют множество отдельных деревьев со стратегиями случайного варьирования их конструкции для решения проблем переобучения и высокой дисперсии, обсуждённых в предыдущем разделе.

### Почему ансамблевые модели работают лучше

Ансамблевое обучение включает объединение нескольких моделей машинного обучения в единую новую модель, которая стремится делать лучшие прогнозы, чем любая отдельная модель. Более конкретно, ансамбль интегрирует прогнозы нескольких базовых оценщиков, обученных с использованием одного или нескольких данных алгоритмов обучения, чтобы уменьшить ошибку обобщения, которую эти модели могут производить по отдельности.

### Пример кода: как бэггинг снижает дисперсию модели

Бэггинг означает агрегирование бутстреп-выборок, которые представляют собой случайные выборки с возвратом. Такая случайная выборка имеет то же количество наблюдений, что и исходный набор данных, но может содержать дубликаты из-за возврата.

Бэггинг снижает дисперсию базовых оценщиков путём рандомизации того, как, например, выращивается каждое дерево, а затем усредняет прогнозы для уменьшения ошибки обобщения. Это часто простой подход к улучшению данной модели без необходимости изменения базового алгоритма. Лучше всего он работает со сложными моделями с низким смещением и высокой дисперсией, такими как глубокие деревья решений, поскольку его цель — ограничить переобучение. Методы бустинга, напротив, лучше всего работают со слабыми моделями, такими как неглубокие деревья решений.

Ноутбук [bagged_decision_trees](02_bagged_decision_trees.ipynb) демонстрирует компромисс между смещением и дисперсией и то, как бэггинг снижает дисперсию по сравнению с отдельным деревом решений.

### Пример кода: как обучить и настроить случайный лес

Алгоритм случайного леса расширяет рандомизацию, введённую бутстреп-выборками, генерируемыми бэггингом, для дальнейшего снижения дисперсии и улучшения прогностической эффективности.

Помимо обучения каждого члена ансамбля на бутстреп-данных обучения, случайные леса также случайным образом выбирают из признаков, используемых в модели (без возврата). В зависимости от реализации случайные выборки могут быть взяты для каждого дерева или каждого разбиения. В результате алгоритм сталкивается с различными вариантами при изучении новых правил, либо на уровне дерева, либо для каждого разбиения.

Ноутбук [random_forest_tuning](03_random_forest_tuning.ipynb) содержит детали реализации для этого раздела.

## Пример кода: сигналы лонг-шорт для японских акций с LightGBM

В [главе 9](../09) мы использовали тесты на коинтеграцию для выявления пар акций с долгосрочным равновесным соотношением в форме общего тренда, к которому их цены возвращаются.

В этой главе мы будем использовать прогнозы модели машинного обучения для идентификации активов, которые, вероятно, вырастут или упадут, чтобы мы могли соответственно открывать рыночно-нейтральные длинные и короткие позиции. Подход аналогичен нашей начальной торговой стратегии, которая использовала линейную регрессию в главе 7 «Линейные модели» и главе 8 «Рабочий процесс стратегии: сквозная алгоритмическая торговля».

Вместо реализации случайного леса из scikit-learn мы будем использовать пакет [LightGBM](https://lightgbm.readthedocs.io/en/latest/), который был в первую очередь разработан для градиентного бустинга. Одним из нескольких преимуществ является способность LightGBM эффективно кодировать категориальные переменные как числовые признаки, а не использовать one-hot кодирование (Fisher 1958). Мы дадим более подробное введение в следующей главе, но примеры кода должны быть легко понятны, поскольку логика аналогична версии scikit-learn.

### Пользовательский бандл Zipline

- Директория [custom_bundle](00_custom_bundle) содержит инструкции о том, как получить данные и создать пользовательский бандл Zipline.

### Инженерия признаков

- Ноутбук [japanese_equity_features](04_japanese_equity_features.ipynb) показывает, как генерировать признаки модели.

### Настройка модели случайного леса LightGBM

- Ноутбук [random_forest_return_signals](05_random_forest_return_signals.ipynb) содержит код для обучения и настройки модели случайного леса [LightGBM](https://lightgbm.readthedocs.io/en/latest/).

### Оценка сигналов с Alphalens

- Ноутбук [alphalens_signals_quality](06_alphalens_signals_quality.ipynb) показывает, как оценивать прогнозы модели с помощью [Alphalens](https://github.com/quantopian/alphalens).

### Бэктестинг с Zipline

- Ноутбук [backtesting_with_zipline](07_backtesting_with_zipline.ipynb) оценивает прогнозы модели с помощью стратегии лонг-шорт, смоделированной с использованием [Zipline](https://zipline.ml4trading.io/).
