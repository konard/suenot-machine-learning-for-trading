# Линейное снижение размерности

Данный раздел посвящен методам линейного снижения размерности, в частности методу главных компонент (PCA), и его применению в алгоритмической торговле.

## Содержание

### 1. Проклятие размерности (00_the_curse_of_dimensionality.ipynb)

Увеличение количества измерений в наборе данных означает увеличение количества элементов в векторе признаков, представляющем каждое наблюдение в соответствующем евклидовом пространстве.

**Ключевые концепции:**
- **Евклидово расстояние** между двумя n-мерными векторами с декартовыми координатами p = (p₁, p₂, ..., pₙ) и q = (q₁, q₂, ..., qₙ) вычисляется по формуле Пифагора:

$$d(p, q)=\sqrt{\sum_{i=1}^n(p_i−q_i)^2}$$

- Каждое новое измерение добавляет неотрицательное слагаемое к сумме, поэтому расстояние увеличивается с ростом количества измерений для различных векторов
- По мере роста числа признаков при фиксированном количестве наблюдений пространство признаков становится всё более разреженным
- Меньшая плотность данных требует большего количества наблюдений для поддержания одинакового среднего расстояния между точками данных

**Симуляция:**
- Эксперимент показывает, что среднее расстояние между точками увеличивается до 11 раз от диапазона признаков для нормального распределения
- До 20 раз для некоррелированного равномерного распределения при увеличении размерности до 2500

### 2. Ключевые идеи PCA (01_pca_key_ideas.ipynb)

PCA находит главные компоненты как линейные комбинации существующих признаков и использует эти компоненты для представления исходных данных.

**Основные принципы:**
- Количество компонент — гиперпараметр, определяющий целевую размерность
- PCA стремится захватить большую часть дисперсии данных
- Облегчает восстановление исходных признаков
- Каждая компонента добавляет информацию

**Как работает алгоритм:**
- Находит последовательность главных компонент, каждая из которых выравнивается с направлением максимальной дисперсии
- Последовательная оптимизация обеспечивает некоррелированность новых компонент с существующими
- Результирующий набор составляет ортогональный базис векторного пространства
- Новый базис соответствует повёрнутой версии исходного базиса

**Сравнение с линейной регрессией:**
- PCA минимизирует расстояния, ортогональные к гиперплоскости
- OLS минимизирует расстояния вдоль оси целевой переменной

### 3. Математика PCA (02_the_math_behind_pca.ipynb)

**Предположения PCA:**
- Высокая дисперсия означает высокое отношение сигнал/шум
- Данные стандартизированы для сопоставимости дисперсий признаков
- Линейные преобразования захватывают релевантные аспекты данных
- Статистики выше первого и второго моментов не важны (нормальное распределение)

**Два способа вычисления:**

1. **Через собственное разложение ковариационной матрицы:**
   - Собственные векторы матрицы ковариации = главные компоненты
   - Собственные значения = объяснённая дисперсия

2. **Через сингулярное разложение (SVD):**
   - Более медленный метод, когда наблюдений больше, чем признаков
   - Лучшая численная стабильность при сильно коррелированных признаках
   - SVD: M = UΣV*, где V* содержит главные компоненты

**Практические аспекты:**
- `sklearn.decomposition.PCA` предоставляет различные алгоритмы: full, arpack, randomized, auto
- Параметр `n_components` определяет количество компонент
- Параметр `whiten` стандартизирует векторы компонент до единичной дисперсии

### 4. PCA и модели факторов риска (03_pca_and_risk_factor_models.ipynb)

PCA полезен для алгоритмической торговли для data-driven выведения факторов риска из доходностей активов.

**Подход:**
- В отличие от модели Фамы-Френча, которая задаёт факторы на основе априорных знаний
- PCA рассматривает факторы риска как латентные переменные
- Одновременно оценивает факторы и их влияние на доходности

**Результаты анализа:**
- Первый фактор объясняет ~30% дневной вариации доходностей (интерпретируется как "рынок")
- Остальные факторы могут интерпретироваться как отраслевые или стилевые
- ~10 факторов объясняют 60% доходностей для крупной выборки акций
- Характерный "локтевой" паттерн на графике кумулятивной объяснённой дисперсии

**Предобработка данных:**
- Винсоризация выбросов на квантилях 2.5% и 97.5%
- Удаление акций с менее чем 95% данных за период
- Импутация пропущенных значений средней доходностью за день

### 5. PCA и собственные портфели (04_pca_and_eigen_portfolios.ipynb)

**Собственные портфели (Eigenportfolios):**
- Главные компоненты корреляционной матрицы захватывают большую часть ковариации между активами
- Они взаимно некоррелированы
- Стандартизированные главные компоненты можно использовать как веса портфеля

**Построение портфелей:**
1. Вычисление ковариационной матрицы доходностей
2. Применение PCA к ковариационной матрице
3. Нормализация компонент (сумма весов = 1)
4. Использование в качестве весов портфеля

**Характеристики собственных портфелей:**
- Портфель 1 ведёт себя подобно рынку
- Остальные портфели захватывают различные паттерны доходности
- Каждый портфель имеет уникальный акцент на определённые активы

## Практическое применение

1. **Снижение размерности** для уменьшения переобучения моделей
2. **Выявление скрытых факторов** рыночного риска
3. **Построение диверсифицированных портфелей** с некоррелированными компонентами
4. **Визуализация** высокоразмерных финансовых данных
5. **Предобработка** данных перед применением ML-алгоритмов

## Требования

- Python 3.6+
- numpy
- pandas
- scikit-learn
- matplotlib
- seaborn

## Ноутбуки

| Файл | Описание |
|------|----------|
| `00_the_curse_of_dimensionality.ipynb` | Симуляция влияния размерности на расстояния |
| `01_pca_key_ideas.ipynb` | Основные концепции и визуализация PCA |
| `02_the_math_behind_pca.ipynb` | Математические основы: собственное и сингулярное разложение |
| `03_pca_and_risk_factor_models.ipynb` | Применение PCA для факторов риска |
| `04_pca_and_eigen_portfolios.ipynb` | Построение собственных портфелей |
