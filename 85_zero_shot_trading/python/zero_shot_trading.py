"""
Zero-Shot Trading - PyTorch Implementation

This module implements zero-shot learning for algorithmic trading,
enabling predictions on new assets without task-specific training.

Key components:
- MarketEncoder: Encodes time series market data to embeddings
- AttributeEncoder: Encodes asset/regime attributes to embeddings
- ZeroShotTradingModel: Combines encoders for zero-shot prediction
- ZeroShotTradingStrategy: Trading strategy using zero-shot predictions
- BybitZeroShotClient: Data fetching from Bybit exchange
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
from datetime import datetime, timedelta
import numpy as np
import pandas as pd
import asyncio
import aiohttp
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# =============================================================================
# Data Classes
# =============================================================================

@dataclass
class TradingSignal:
    """Trading signal generated by zero-shot prediction."""
    regime: str
    confidence: float
    regime_probabilities: Dict[str, float]
    action: str
    target_position: float
    position_change: float
    reasoning: str
    timestamp: datetime = None

    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = datetime.now()


@dataclass
class AssetAttributes:
    """Semantic attributes for an asset."""
    asset_type: str  # crypto, stock, forex, commodity
    volatility_class: str  # low, medium, high, extreme
    market_cap_class: str  # large, medium, small, micro
    trend: str  # strong_up, weak_up, sideways, weak_down, strong_down
    sector: str = "unknown"
    annualized_vol: float = 0.0
    correlation_btc: float = 0.0
    correlation_eth: float = 0.0


@dataclass
class BacktestResult:
    """Results from backtesting zero-shot strategy."""
    total_return: float
    sharpe_ratio: float
    sortino_ratio: float
    max_drawdown: float
    regime_accuracy: float
    average_confidence: float
    num_trades: int
    win_rate: float
    profit_factor: float


# =============================================================================
# Neural Network Components
# =============================================================================

class MarketEncoder(nn.Module):
    """
    Encodes market time series data into embeddings.

    Architecture:
    1. Convolutional layers for local feature extraction
    2. Transformer encoder for sequence modeling
    3. Projection head for embedding space

    Args:
        input_dim: Number of input features per timestep
        hidden_dim: Hidden dimension for intermediate layers
        embed_dim: Final embedding dimension
        num_layers: Number of transformer encoder layers
        num_heads: Number of attention heads
        dropout: Dropout probability
    """

    def __init__(
        self,
        input_dim: int,
        hidden_dim: int = 128,
        embed_dim: int = 64,
        num_layers: int = 2,
        num_heads: int = 4,
        dropout: float = 0.1
    ):
        super().__init__()

        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.embed_dim = embed_dim

        # Temporal feature extraction via convolutions
        self.conv1 = nn.Conv1d(input_dim, hidden_dim, kernel_size=3, padding=1)
        self.conv2 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm1d(hidden_dim)
        self.bn2 = nn.BatchNorm1d(hidden_dim)
        self.dropout = nn.Dropout(dropout)

        # Transformer for sequence modeling
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=hidden_dim,
            nhead=num_heads,
            dim_feedforward=hidden_dim * 4,
            dropout=dropout,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

        # Projection to embedding space
        self.projection = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, embed_dim)
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass.

        Args:
            x: Market data tensor of shape (batch, seq_len, features)

        Returns:
            Embedding tensor of shape (batch, embed_dim)
        """
        # x: (batch, seq_len, features) -> (batch, features, seq_len)
        x = x.transpose(1, 2)

        # Convolutional feature extraction
        x = F.relu(self.bn1(self.conv1(x)))
        x = self.dropout(x)
        x = F.relu(self.bn2(self.conv2(x)))
        x = self.dropout(x)

        # x: (batch, hidden, seq_len) -> (batch, seq_len, hidden)
        x = x.transpose(1, 2)

        # Transformer encoding
        x = self.transformer(x)

        # Global average pooling
        x = x.mean(dim=1)

        # Project to embedding space
        x = self.projection(x)

        # L2 normalize for cosine similarity
        x = F.normalize(x, p=2, dim=1)

        return x


class AttributeEncoder(nn.Module):
    """
    Encodes asset/regime attributes into embeddings.

    Handles both categorical (embedded) and numerical (MLP) attributes,
    combining them into a unified embedding space.

    Args:
        categorical_dims: Dict mapping attribute names to number of categories
        numerical_dim: Number of continuous numerical attributes
        embed_dim: Final embedding dimension
        hidden_dim: Hidden dimension for intermediate layers
    """

    def __init__(
        self,
        categorical_dims: Dict[str, int],
        numerical_dim: int,
        embed_dim: int = 64,
        hidden_dim: int = 128
    ):
        super().__init__()

        self.categorical_dims = categorical_dims
        self.numerical_dim = numerical_dim
        self.embed_dim = embed_dim

        # Categorical embeddings - each attribute gets its own embedding table
        cat_embed_dim = max(16, hidden_dim // len(categorical_dims)) if categorical_dims else 0
        self.cat_embeddings = nn.ModuleDict({
            name: nn.Embedding(num_cats, cat_embed_dim)
            for name, num_cats in categorical_dims.items()
        })

        # Numerical feature processing
        if numerical_dim > 0:
            self.num_mlp = nn.Sequential(
                nn.Linear(numerical_dim, hidden_dim // 2),
                nn.ReLU(),
                nn.Linear(hidden_dim // 2, hidden_dim // 2)
            )
            num_output_dim = hidden_dim // 2
        else:
            self.num_mlp = None
            num_output_dim = 0

        # Calculate total input dimension for projection
        total_cat_dim = cat_embed_dim * len(categorical_dims) if categorical_dims else 0
        total_dim = total_cat_dim + num_output_dim

        # Combined projection
        self.projection = nn.Sequential(
            nn.Linear(total_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, embed_dim)
        )

    def forward(
        self,
        categorical_attrs: Dict[str, torch.Tensor],
        numerical_attrs: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        Forward pass.

        Args:
            categorical_attrs: Dict mapping attr names to category indices (batch,)
            numerical_attrs: Tensor of shape (batch, numerical_dim), optional

        Returns:
            Embedding tensor of shape (batch, embed_dim)
        """
        embeddings = []

        # Embed categorical attributes
        for name in self.categorical_dims.keys():
            if name in categorical_attrs:
                cat_embed = self.cat_embeddings[name](categorical_attrs[name])
                embeddings.append(cat_embed)

        # Process numerical attributes if present
        if self.num_mlp is not None and numerical_attrs is not None:
            num_embed = self.num_mlp(numerical_attrs)
            embeddings.append(num_embed)

        # Concatenate all embeddings
        combined = torch.cat(embeddings, dim=1)

        # Project to embedding space
        x = self.projection(combined)

        # L2 normalize
        x = F.normalize(x, p=2, dim=1)

        return x


class ZeroShotTradingModel(nn.Module):
    """
    Complete zero-shot trading model.

    Combines market encoder and attribute encoder to enable
    zero-shot prediction of market regimes.

    Args:
        market_input_dim: Number of market features per timestep
        categorical_dims: Dict mapping attribute names to category counts
        numerical_dim: Number of numerical attributes
        embed_dim: Embedding dimension for both encoders
        hidden_dim: Hidden dimension for intermediate layers
        temperature: Temperature for softmax scaling
    """

    def __init__(
        self,
        market_input_dim: int,
        categorical_dims: Dict[str, int],
        numerical_dim: int,
        embed_dim: int = 64,
        hidden_dim: int = 128,
        temperature: float = 0.1
    ):
        super().__init__()

        self.market_encoder = MarketEncoder(
            input_dim=market_input_dim,
            hidden_dim=hidden_dim,
            embed_dim=embed_dim
        )

        self.attribute_encoder = AttributeEncoder(
            categorical_dims=categorical_dims,
            numerical_dim=numerical_dim,
            embed_dim=embed_dim,
            hidden_dim=hidden_dim
        )

        self.temperature = temperature
        self.embed_dim = embed_dim

    def encode_market(self, market_data: torch.Tensor) -> torch.Tensor:
        """Encode market data to embedding space."""
        return self.market_encoder(market_data)

    def encode_attributes(
        self,
        categorical_attrs: Dict[str, torch.Tensor],
        numerical_attrs: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """Encode attributes to embedding space."""
        return self.attribute_encoder(categorical_attrs, numerical_attrs)

    def compute_compatibility(
        self,
        market_embed: torch.Tensor,
        attr_embed: torch.Tensor
    ) -> torch.Tensor:
        """
        Compute compatibility scores between market and attribute embeddings.

        Args:
            market_embed: (batch_size, embed_dim)
            attr_embed: (num_classes, embed_dim) or (batch_size, num_classes, embed_dim)

        Returns:
            Compatibility scores (batch_size, num_classes)
        """
        if attr_embed.dim() == 2:
            # attr_embed: (num_classes, embed_dim)
            scores = torch.matmul(market_embed, attr_embed.T) / self.temperature
        else:
            # attr_embed: (batch_size, num_classes, embed_dim)
            scores = torch.bmm(
                attr_embed,
                market_embed.unsqueeze(-1)
            ).squeeze(-1) / self.temperature

        return scores

    def forward(
        self,
        market_data: torch.Tensor,
        categorical_attrs: Dict[str, torch.Tensor],
        numerical_attrs: Optional[torch.Tensor] = None
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Forward pass for zero-shot prediction.

        Returns:
            (compatibility_scores, predicted_class_probabilities)
        """
        market_embed = self.encode_market(market_data)
        attr_embed = self.encode_attributes(categorical_attrs, numerical_attrs)

        scores = self.compute_compatibility(market_embed, attr_embed)
        probs = F.softmax(scores, dim=-1)

        return scores, probs


# =============================================================================
# Training
# =============================================================================

class ZeroShotTrainer:
    """
    Trainer for zero-shot trading model using contrastive learning.

    Args:
        model: ZeroShotTradingModel instance
        learning_rate: Learning rate for optimizer
        margin: Margin for triplet loss
        device: Device to use for training
    """

    def __init__(
        self,
        model: ZeroShotTradingModel,
        learning_rate: float = 1e-4,
        margin: float = 0.2,
        device: str = "cpu"
    ):
        self.model = model.to(device)
        self.device = device
        self.optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
        self.margin = margin
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='min', factor=0.5, patience=5
        )

    def contrastive_loss(
        self,
        market_embed: torch.Tensor,
        positive_attr_embed: torch.Tensor,
        negative_attr_embeds: torch.Tensor
    ) -> torch.Tensor:
        """
        Contrastive loss to align market data with correct attributes.

        Args:
            market_embed: Market data embedding (batch, embed_dim)
            positive_attr_embed: Correct attribute embedding (batch, embed_dim)
            negative_attr_embeds: Wrong attribute embeddings (batch, num_neg, embed_dim)
        """
        # Positive similarity (higher is better)
        pos_sim = F.cosine_similarity(market_embed, positive_attr_embed)

        # Negative similarities (should be lower than positive)
        neg_sims = F.cosine_similarity(
            market_embed.unsqueeze(1),
            negative_attr_embeds,
            dim=2
        )

        # Triplet margin loss: push positive closer, negative farther
        # margin + neg_sim - pos_sim should be negative for good embeddings
        loss = F.relu(self.margin - pos_sim.unsqueeze(1) + neg_sims).mean()

        return loss

    def train_step(
        self,
        market_data: torch.Tensor,
        positive_cat_attrs: Dict[str, torch.Tensor],
        positive_num_attrs: Optional[torch.Tensor],
        negative_cat_attrs_list: List[Dict[str, torch.Tensor]],
        negative_num_attrs_list: List[Optional[torch.Tensor]]
    ) -> float:
        """
        Single training step.

        Args:
            market_data: Market features (batch, seq_len, features)
            positive_cat_attrs: Correct categorical attributes
            positive_num_attrs: Correct numerical attributes
            negative_cat_attrs_list: List of incorrect categorical attributes
            negative_num_attrs_list: List of incorrect numerical attributes

        Returns:
            Loss value
        """
        self.model.train()
        self.optimizer.zero_grad()

        # Move data to device
        market_data = market_data.to(self.device)

        # Encode market data
        market_embed = self.model.encode_market(market_data)

        # Encode positive attributes
        pos_cat = {k: v.to(self.device) for k, v in positive_cat_attrs.items()}
        pos_num = positive_num_attrs.to(self.device) if positive_num_attrs is not None else None
        pos_attr_embed = self.model.encode_attributes(pos_cat, pos_num)

        # Encode negative attributes
        neg_attr_embeds = []
        for neg_cat, neg_num in zip(negative_cat_attrs_list, negative_num_attrs_list):
            neg_cat_device = {k: v.to(self.device) for k, v in neg_cat.items()}
            neg_num_device = neg_num.to(self.device) if neg_num is not None else None
            neg_embed = self.model.encode_attributes(neg_cat_device, neg_num_device)
            neg_attr_embeds.append(neg_embed)

        neg_attr_embeds = torch.stack(neg_attr_embeds, dim=1)

        # Compute loss
        loss = self.contrastive_loss(market_embed, pos_attr_embed, neg_attr_embeds)

        # Backprop
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
        self.optimizer.step()

        return loss.item()

    def train_epoch(
        self,
        dataloader,
        epoch: int
    ) -> float:
        """Train for one epoch."""
        total_loss = 0.0
        num_batches = 0

        for batch in dataloader:
            loss = self.train_step(*batch)
            total_loss += loss
            num_batches += 1

        avg_loss = total_loss / max(num_batches, 1)
        self.scheduler.step(avg_loss)

        logger.info(f"Epoch {epoch}: Average Loss = {avg_loss:.4f}")
        return avg_loss


# =============================================================================
# Trading Strategy
# =============================================================================

class ZeroShotTradingStrategy:
    """
    Trading strategy using zero-shot regime prediction.

    Args:
        model: Trained ZeroShotTradingModel
        regime_attributes: Dict mapping regime names to (cat_attrs, num_attrs)
        confidence_threshold: Minimum confidence for trading
        device: Device for inference
    """

    # Default regime to action mapping
    REGIME_ACTIONS = {
        "strong_uptrend": {"action": "long", "base_size": 1.0},
        "weak_uptrend": {"action": "long", "base_size": 0.5},
        "sideways": {"action": "neutral", "base_size": 0.0},
        "weak_downtrend": {"action": "short", "base_size": 0.5},
        "strong_downtrend": {"action": "short", "base_size": 1.0},
    }

    def __init__(
        self,
        model: ZeroShotTradingModel,
        regime_attributes: Dict[str, Tuple[Dict[str, int], np.ndarray]],
        confidence_threshold: float = 0.6,
        device: str = "cpu"
    ):
        self.model = model.to(device)
        self.model.eval()
        self.regime_attributes = regime_attributes
        self.confidence_threshold = confidence_threshold
        self.device = device

        # Precompute regime embeddings
        self._precompute_regime_embeddings()

    def _precompute_regime_embeddings(self):
        """Precompute attribute embeddings for all known regimes."""
        self.regime_embeddings = {}
        self.regime_names = list(self.regime_attributes.keys())

        with torch.no_grad():
            for regime_name, (cat_attrs, num_attrs) in self.regime_attributes.items():
                cat_tensors = {
                    k: torch.tensor([v], device=self.device)
                    for k, v in cat_attrs.items()
                }
                num_tensor = torch.tensor(
                    [num_attrs], dtype=torch.float32, device=self.device
                ) if num_attrs is not None else None

                embed = self.model.encode_attributes(cat_tensors, num_tensor)
                self.regime_embeddings[regime_name] = embed.squeeze(0)

    def predict_regime(
        self,
        market_data: torch.Tensor
    ) -> Tuple[str, float, Dict[str, float]]:
        """
        Predict market regime using zero-shot inference.

        Args:
            market_data: Market features (1, seq_len, features)

        Returns:
            (predicted_regime, confidence, all_regime_probabilities)
        """
        self.model.eval()
        market_data = market_data.to(self.device)

        with torch.no_grad():
            market_embed = self.model.encode_market(market_data)

            # Compute similarities to all regime embeddings
            similarities = {}
            for regime_name, regime_embed in self.regime_embeddings.items():
                sim = F.cosine_similarity(
                    market_embed,
                    regime_embed.unsqueeze(0)
                ).item()
                similarities[regime_name] = sim

        # Convert to probabilities via softmax
        sim_values = np.array(list(similarities.values()))
        exp_sims = np.exp(sim_values / self.model.temperature)
        probs = exp_sims / exp_sims.sum()

        regime_probs = dict(zip(similarities.keys(), probs))

        # Get prediction
        predicted_regime = max(regime_probs, key=regime_probs.get)
        confidence = regime_probs[predicted_regime]

        return predicted_regime, confidence, regime_probs

    def generate_signal(
        self,
        market_data: torch.Tensor,
        current_position: float = 0.0
    ) -> TradingSignal:
        """
        Generate trading signal based on zero-shot regime prediction.

        Args:
            market_data: Market features tensor (1, seq_len, features)
            current_position: Current position (-1 to 1)

        Returns:
            TradingSignal with action and reasoning
        """
        regime, confidence, regime_probs = self.predict_regime(market_data)

        # Get action for regime
        signal_info = self.REGIME_ACTIONS.get(
            regime,
            {"action": "neutral", "base_size": 0.0}
        )

        # Adjust size based on confidence
        base_size = signal_info["base_size"]
        if confidence < self.confidence_threshold:
            base_size *= 0.5  # Reduce size if uncertain

        # Compute target position
        if signal_info["action"] == "long":
            target_position = base_size * confidence
        elif signal_info["action"] == "short":
            target_position = -base_size * confidence
        else:
            target_position = 0.0

        position_change = target_position - current_position

        return TradingSignal(
            regime=regime,
            confidence=confidence,
            regime_probabilities=regime_probs,
            action=signal_info["action"],
            target_position=target_position,
            position_change=position_change,
            reasoning=f"Zero-shot detected {regime} regime with {confidence:.1%} confidence"
        )


# =============================================================================
# Data Handling
# =============================================================================

class BybitZeroShotClient:
    """
    Bybit client for zero-shot trading data collection.

    Fetches market data and computes semantic attributes for assets.
    """

    BASE_URL = "https://api.bybit.com"

    def __init__(self):
        self.session: Optional[aiohttp.ClientSession] = None

    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()

    async def fetch_klines(
        self,
        symbol: str,
        interval: str = "60",
        limit: int = 200
    ) -> pd.DataFrame:
        """
        Fetch OHLCV kline data from Bybit.

        Args:
            symbol: Trading pair (e.g., "BTCUSDT")
            interval: Kline interval in minutes
            limit: Number of klines to fetch

        Returns:
            DataFrame with OHLCV data
        """
        if self.session is None:
            raise RuntimeError("Client not initialized. Use async context manager.")

        endpoint = f"{self.BASE_URL}/v5/market/kline"
        params = {
            "category": "linear",
            "symbol": symbol,
            "interval": interval,
            "limit": limit
        }

        async with self.session.get(endpoint, params=params) as response:
            data = await response.json()

        if data["retCode"] != 0:
            raise ValueError(f"API error: {data['retMsg']}")

        klines = data["result"]["list"]

        df = pd.DataFrame(klines, columns=[
            "timestamp", "open", "high", "low", "close", "volume", "turnover"
        ])

        for col in ["open", "high", "low", "close", "volume", "turnover"]:
            df[col] = pd.to_numeric(df[col])
        df["timestamp"] = pd.to_datetime(df["timestamp"].astype(int), unit="ms")

        return df.sort_values("timestamp").reset_index(drop=True)

    async def fetch_ticker_info(self, symbol: str) -> Dict[str, Any]:
        """Fetch current ticker information."""
        if self.session is None:
            raise RuntimeError("Client not initialized.")

        endpoint = f"{self.BASE_URL}/v5/market/tickers"
        params = {"category": "linear", "symbol": symbol}

        async with self.session.get(endpoint, params=params) as response:
            data = await response.json()

        if data["retCode"] != 0:
            raise ValueError(f"API error: {data['retMsg']}")

        return data["result"]["list"][0] if data["result"]["list"] else {}

    async def compute_asset_attributes(
        self,
        symbol: str,
        reference_symbols: List[str] = None
    ) -> AssetAttributes:
        """
        Compute semantic attributes for an asset.

        Args:
            symbol: Target asset symbol
            reference_symbols: Reference assets for correlation

        Returns:
            AssetAttributes instance
        """
        if reference_symbols is None:
            reference_symbols = ["BTCUSDT", "ETHUSDT"]

        # Fetch data for target and reference assets
        tasks = [self.fetch_klines(symbol, "60", 168)]  # 1 week hourly
        tasks.extend([
            self.fetch_klines(ref, "60", 168)
            for ref in reference_symbols
        ])

        results = await asyncio.gather(*tasks, return_exceptions=True)

        # Handle any errors
        valid_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.warning(f"Failed to fetch data for index {i}: {result}")
                valid_results.append(None)
            else:
                valid_results.append(result)

        target_df = valid_results[0]
        if target_df is None:
            raise ValueError(f"Could not fetch data for {symbol}")

        ref_dfs = valid_results[1:]

        # Compute returns
        target_returns = target_df["close"].pct_change().dropna()

        # Volatility class
        annualized_vol = target_returns.std() * np.sqrt(24 * 365)
        if annualized_vol < 0.3:
            volatility_class = "low"
        elif annualized_vol < 0.6:
            volatility_class = "medium"
        elif annualized_vol < 1.0:
            volatility_class = "high"
        else:
            volatility_class = "extreme"

        # Correlations with reference assets
        correlation_btc = 0.0
        correlation_eth = 0.0

        for ref_name, ref_df in zip(reference_symbols, ref_dfs):
            if ref_df is None:
                continue
            ref_returns = ref_df["close"].pct_change().dropna()
            min_len = min(len(target_returns), len(ref_returns))
            if min_len > 10:
                corr = np.corrcoef(
                    target_returns.iloc[-min_len:],
                    ref_returns.iloc[-min_len:]
                )[0, 1]
                if "BTC" in ref_name:
                    correlation_btc = corr
                elif "ETH" in ref_name:
                    correlation_eth = corr

        # Market cap class (approximated from volume)
        avg_volume = target_df["turnover"].mean()
        if avg_volume > 1e9:
            market_cap_class = "large"
        elif avg_volume > 1e8:
            market_cap_class = "medium"
        elif avg_volume > 1e7:
            market_cap_class = "small"
        else:
            market_cap_class = "micro"

        # Trend detection
        if len(target_df) >= 50:
            sma_20 = target_df["close"].rolling(20).mean().iloc[-1]
            sma_50 = target_df["close"].rolling(50).mean().iloc[-1]
            current_price = target_df["close"].iloc[-1]

            if current_price > sma_20 > sma_50:
                trend = "strong_up"
            elif current_price > sma_20:
                trend = "weak_up"
            elif current_price < sma_20 < sma_50:
                trend = "strong_down"
            elif current_price < sma_20:
                trend = "weak_down"
            else:
                trend = "sideways"
        else:
            trend = "sideways"

        return AssetAttributes(
            asset_type="crypto",
            volatility_class=volatility_class,
            market_cap_class=market_cap_class,
            trend=trend,
            annualized_vol=annualized_vol,
            correlation_btc=correlation_btc,
            correlation_eth=correlation_eth
        )


# =============================================================================
# Feature Engineering
# =============================================================================

def prepare_features(df: pd.DataFrame, sequence_length: int = 50) -> torch.Tensor:
    """
    Prepare market features from OHLCV DataFrame.

    Args:
        df: DataFrame with columns [open, high, low, close, volume]
        sequence_length: Number of timesteps to include

    Returns:
        Feature tensor of shape (1, sequence_length, num_features)
    """
    features = []

    # Price features
    df = df.copy()
    df["returns"] = df["close"].pct_change()
    df["log_returns"] = np.log(df["close"] / df["close"].shift(1))
    df["high_low_range"] = (df["high"] - df["low"]) / df["close"]
    df["close_position"] = (df["close"] - df["low"]) / (df["high"] - df["low"] + 1e-8)

    # Volume features
    df["volume_ma"] = df["volume"].rolling(20).mean()
    df["volume_ratio"] = df["volume"] / (df["volume_ma"] + 1e-8)

    # Volatility features
    df["volatility_20"] = df["returns"].rolling(20).std()
    df["volatility_5"] = df["returns"].rolling(5).std()

    # Technical indicators
    df["sma_10"] = df["close"].rolling(10).mean()
    df["sma_20"] = df["close"].rolling(20).mean()
    df["sma_ratio"] = df["sma_10"] / (df["sma_20"] + 1e-8)

    # RSI
    delta = df["close"].diff()
    gain = (delta.where(delta > 0, 0)).rolling(14).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(14).mean()
    rs = gain / (loss + 1e-8)
    df["rsi"] = 100 - (100 / (1 + rs))
    df["rsi_normalized"] = df["rsi"] / 100

    # MACD
    ema_12 = df["close"].ewm(span=12).mean()
    ema_26 = df["close"].ewm(span=26).mean()
    df["macd"] = (ema_12 - ema_26) / df["close"]

    # Bollinger Bands
    df["bb_middle"] = df["close"].rolling(20).mean()
    df["bb_std"] = df["close"].rolling(20).std()
    df["bb_upper"] = df["bb_middle"] + 2 * df["bb_std"]
    df["bb_lower"] = df["bb_middle"] - 2 * df["bb_std"]
    df["bb_position"] = (df["close"] - df["bb_lower"]) / (df["bb_upper"] - df["bb_lower"] + 1e-8)

    # Select features
    feature_columns = [
        "returns", "log_returns", "high_low_range", "close_position",
        "volume_ratio", "volatility_20", "volatility_5",
        "sma_ratio", "rsi_normalized", "macd", "bb_position"
    ]

    # Drop NaN rows
    df = df.dropna()

    if len(df) < sequence_length:
        raise ValueError(f"Not enough data: {len(df)} < {sequence_length}")

    # Take last sequence_length rows
    df_seq = df.iloc[-sequence_length:]

    # Extract features
    feature_array = df_seq[feature_columns].values

    # Normalize features (z-score)
    mean = feature_array.mean(axis=0, keepdims=True)
    std = feature_array.std(axis=0, keepdims=True) + 1e-8
    feature_array = (feature_array - mean) / std

    # Convert to tensor
    tensor = torch.tensor(feature_array, dtype=torch.float32).unsqueeze(0)

    return tensor


# =============================================================================
# Utility Functions
# =============================================================================

def create_default_model() -> ZeroShotTradingModel:
    """Create a default ZeroShotTradingModel with standard configuration."""
    categorical_dims = {
        "asset_type": 4,       # crypto, stock, forex, commodity
        "volatility_class": 4,  # low, medium, high, extreme
        "market_cap_class": 4,  # large, medium, small, micro
        "trend": 5              # strong_up, weak_up, sideways, weak_down, strong_down
    }

    return ZeroShotTradingModel(
        market_input_dim=11,    # Number of features from prepare_features
        categorical_dims=categorical_dims,
        numerical_dim=3,        # annualized_vol, corr_btc, corr_eth
        embed_dim=64,
        hidden_dim=128,
        temperature=0.1
    )


def create_default_regime_attributes() -> Dict[str, Tuple[Dict[str, int], np.ndarray]]:
    """Create default regime attribute definitions."""
    return {
        "strong_uptrend": (
            {"asset_type": 0, "volatility_class": 2, "market_cap_class": 1, "trend": 0},
            np.array([0.8, 0.7, 0.6])
        ),
        "weak_uptrend": (
            {"asset_type": 0, "volatility_class": 1, "market_cap_class": 1, "trend": 1},
            np.array([0.5, 0.6, 0.5])
        ),
        "sideways": (
            {"asset_type": 0, "volatility_class": 0, "market_cap_class": 1, "trend": 2},
            np.array([0.3, 0.5, 0.4])
        ),
        "weak_downtrend": (
            {"asset_type": 0, "volatility_class": 1, "market_cap_class": 1, "trend": 3},
            np.array([0.5, 0.5, 0.5])
        ),
        "strong_downtrend": (
            {"asset_type": 0, "volatility_class": 3, "market_cap_class": 1, "trend": 4},
            np.array([1.0, 0.6, 0.7])
        ),
    }


# =============================================================================
# Main Example
# =============================================================================

async def main():
    """Example usage of zero-shot trading."""
    logger.info("Zero-Shot Trading Example")
    logger.info("=" * 50)

    # Create model
    model = create_default_model()
    logger.info(f"Created model with {sum(p.numel() for p in model.parameters())} parameters")

    # Create regime attributes
    regime_attributes = create_default_regime_attributes()
    logger.info(f"Defined {len(regime_attributes)} market regimes")

    # Create strategy
    strategy = ZeroShotTradingStrategy(
        model=model,
        regime_attributes=regime_attributes,
        confidence_threshold=0.6
    )
    logger.info("Created trading strategy")

    # Fetch real data from Bybit
    logger.info("\nFetching data from Bybit...")
    try:
        async with BybitZeroShotClient() as client:
            # Fetch BTCUSDT data
            df = await client.fetch_klines("BTCUSDT", "60", 200)
            logger.info(f"Fetched {len(df)} candles for BTCUSDT")

            # Compute asset attributes
            attrs = await client.compute_asset_attributes("BTCUSDT")
            logger.info(f"Asset attributes: volatility={attrs.volatility_class}, "
                       f"trend={attrs.trend}, cap={attrs.market_cap_class}")

        # Prepare features
        features = prepare_features(df, sequence_length=50)
        logger.info(f"Prepared features with shape: {features.shape}")

        # Generate trading signal
        signal = strategy.generate_signal(features)

        logger.info("\n" + "=" * 50)
        logger.info("TRADING SIGNAL")
        logger.info("=" * 50)
        logger.info(f"Regime: {signal.regime}")
        logger.info(f"Confidence: {signal.confidence:.1%}")
        logger.info(f"Action: {signal.action}")
        logger.info(f"Target Position: {signal.target_position:.2f}")
        logger.info(f"Reasoning: {signal.reasoning}")
        logger.info("\nRegime Probabilities:")
        for regime, prob in sorted(signal.regime_probabilities.items(),
                                   key=lambda x: -x[1]):
            logger.info(f"  {regime}: {prob:.1%}")

    except Exception as e:
        logger.error(f"Error: {e}")
        logger.info("\nGenerating example with synthetic data...")

        # Create synthetic data for demonstration
        np.random.seed(42)
        seq_len = 50
        num_features = 11

        synthetic_features = torch.randn(1, seq_len, num_features)
        signal = strategy.generate_signal(synthetic_features)

        logger.info(f"\nSynthetic Signal - Regime: {signal.regime}, "
                   f"Confidence: {signal.confidence:.1%}")


if __name__ == "__main__":
    asyncio.run(main())
