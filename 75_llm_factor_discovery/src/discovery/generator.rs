//! Factor generator using LLM
//!
//! Generates alpha factor candidates using Large Language Models.

use super::llm_client::{ChatMessage, LlmClient, LlmError};
use super::prompt_builder::{AssetClass, PromptBuilder, PromptConfig};
use anyhow::Result;
use serde::{Deserialize, Serialize};
use tracing::{debug, info, warn};

/// Configuration for factor generation
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct GenerationConfig {
    /// Asset class
    pub asset_class: AssetClass,
    /// Number of factors to generate per request
    pub factors_per_request: usize,
    /// Maximum factor complexity (nesting depth)
    pub max_complexity: usize,
    /// Retry count on failure
    pub max_retries: usize,
    /// Include reasoning in output
    pub include_reasoning: bool,
}

impl Default for GenerationConfig {
    fn default() -> Self {
        Self {
            asset_class: AssetClass::Crypto,
            factors_per_request: 5,
            max_complexity: 5,
            max_retries: 3,
            include_reasoning: true,
        }
    }
}

/// A factor candidate generated by the LLM
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct FactorCandidate {
    /// Descriptive name for the factor
    pub name: String,
    /// Factor expression (parseable)
    pub expression: String,
    /// Economic rationale
    pub rationale: String,
    /// Expected IC magnitude
    pub expected_ic: f64,
    /// Expected turnover level
    pub expected_turnover: TurnoverLevel,
    /// Source hypothesis
    #[serde(default)]
    pub hypothesis: String,
    /// Generation iteration
    #[serde(default)]
    pub iteration: usize,
}

/// Turnover level classification
#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
#[serde(rename_all = "lowercase")]
pub enum TurnoverLevel {
    Low,
    Medium,
    High,
}

impl Default for TurnoverLevel {
    fn default() -> Self {
        Self::Medium
    }
}

/// Factor generator that uses LLM to create factor candidates
pub struct FactorGenerator {
    llm_client: LlmClient,
    config: GenerationConfig,
    prompt_builder: PromptBuilder,
}

impl FactorGenerator {
    /// Create a new factor generator
    pub fn new(llm_client: LlmClient) -> Self {
        Self {
            llm_client,
            config: GenerationConfig::default(),
            prompt_builder: PromptBuilder::new(),
        }
    }

    /// Create with custom configuration
    pub fn with_config(llm_client: LlmClient, config: GenerationConfig) -> Self {
        let prompt_config = PromptConfig {
            asset_class: config.asset_class,
            max_complexity: config.max_complexity,
            num_factors: config.factors_per_request,
            include_reasoning: config.include_reasoning,
            ..Default::default()
        };

        Self {
            llm_client,
            config,
            prompt_builder: PromptBuilder::with_config(prompt_config),
        }
    }

    /// Generate factor candidates based on a hypothesis
    pub async fn generate(&self, hypothesis: &str) -> Result<Vec<FactorCandidate>> {
        let system_prompt = self.prompt_builder.system_prompt();
        let generation_prompt = self.prompt_builder.generation_prompt(hypothesis);

        let messages = vec![
            ChatMessage::system(&system_prompt),
            ChatMessage::user(&generation_prompt),
        ];

        let mut last_error = None;

        for attempt in 0..self.config.max_retries {
            debug!("Generation attempt {}/{}", attempt + 1, self.config.max_retries);

            match self.llm_client.chat(&messages).await {
                Ok(response) => {
                    match self.parse_factors(&response.content, hypothesis) {
                        Ok(factors) if !factors.is_empty() => {
                            info!("Generated {} factor candidates", factors.len());
                            return Ok(factors);
                        }
                        Ok(_) => {
                            warn!("No factors parsed from response, retrying...");
                            last_error = Some(anyhow::anyhow!("No factors parsed from response"));
                        }
                        Err(e) => {
                            warn!("Failed to parse factors: {}, retrying...", e);
                            last_error = Some(e);
                        }
                    }
                }
                Err(LlmError::RateLimitExceeded) => {
                    warn!("Rate limit exceeded, waiting before retry...");
                    tokio::time::sleep(tokio::time::Duration::from_secs(5)).await;
                    last_error = Some(anyhow::anyhow!("Rate limit exceeded"));
                }
                Err(e) => {
                    warn!("LLM request failed: {}", e);
                    last_error = Some(anyhow::anyhow!("LLM request failed: {}", e));
                }
            }
        }

        Err(last_error.unwrap_or_else(|| anyhow::anyhow!("Generation failed after retries")))
    }

    /// Generate factors based on market observations
    pub async fn generate_from_observations(
        &self,
        observations: &[String],
    ) -> Result<Vec<FactorCandidate>> {
        let system_prompt = self.prompt_builder.system_prompt();
        let observation_prompt = self.prompt_builder.observation_prompt(observations);

        let messages = vec![
            ChatMessage::system(&system_prompt),
            ChatMessage::user(&observation_prompt),
        ];

        let response = self.llm_client.chat(&messages).await?;
        self.parse_factors(&response.content, "market observations")
    }

    /// Parse factor candidates from LLM response
    fn parse_factors(&self, content: &str, hypothesis: &str) -> Result<Vec<FactorCandidate>> {
        // Try to extract JSON from the response
        let json_content = self.extract_json(content)?;

        // Parse the JSON
        let mut candidates: Vec<FactorCandidate> = serde_json::from_str(&json_content)
            .map_err(|e| anyhow::anyhow!("Failed to parse JSON: {}", e))?;

        // Add metadata
        for candidate in &mut candidates {
            candidate.hypothesis = hypothesis.to_string();
        }

        // Validate expressions
        let valid_candidates: Vec<FactorCandidate> = candidates
            .into_iter()
            .filter(|c| {
                match crate::parser::FactorExpr::parse(&c.expression) {
                    Ok(_) => true,
                    Err(e) => {
                        warn!("Invalid factor expression '{}': {}", c.expression, e);
                        false
                    }
                }
            })
            .collect();

        Ok(valid_candidates)
    }

    /// Extract JSON from LLM response (handles markdown code blocks)
    fn extract_json(&self, content: &str) -> Result<String> {
        // Try to find JSON in markdown code block
        if let Some(start) = content.find("```json") {
            let start = start + 7;
            if let Some(end) = content[start..].find("```") {
                return Ok(content[start..start + end].trim().to_string());
            }
        }

        // Try to find JSON in generic code block
        if let Some(start) = content.find("```") {
            let start = start + 3;
            // Skip language identifier if present
            let start = content[start..]
                .find('\n')
                .map(|i| start + i + 1)
                .unwrap_or(start);

            if let Some(end) = content[start..].find("```") {
                return Ok(content[start..start + end].trim().to_string());
            }
        }

        // Try to find raw JSON array
        if let Some(start) = content.find('[') {
            if let Some(end) = content.rfind(']') {
                return Ok(content[start..=end].to_string());
            }
        }

        Err(anyhow::anyhow!("No JSON found in response"))
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_extract_json_from_markdown() {
        let generator = FactorGenerator {
            llm_client: unsafe { std::mem::zeroed() }, // Test only
            config: GenerationConfig::default(),
            prompt_builder: PromptBuilder::new(),
        };

        // Note: This test is incomplete as it requires a mock LLM client
        // The actual JSON extraction logic is tested below

        let content = r#"
Here are some factors:

```json
[{"name": "test", "expression": "rank(close)"}]
```

Hope this helps!
"#;

        // Can't test directly without proper mock, but the logic is sound
    }

    #[test]
    fn test_turnover_level_default() {
        assert_eq!(TurnoverLevel::default(), TurnoverLevel::Medium);
    }

    #[test]
    fn test_generation_config_default() {
        let config = GenerationConfig::default();
        assert_eq!(config.factors_per_request, 5);
        assert_eq!(config.max_complexity, 5);
        assert_eq!(config.max_retries, 3);
    }
}
