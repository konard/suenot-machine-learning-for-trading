//! ĞŸÑ€Ğ¸Ğ¼ĞµÑ€: ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğ¹ ĞºÑ€Ğ¸Ğ¿Ñ‚Ğ¾-Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²
//!
//! Ğ”ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ NLP pipeline:
//! 1. Ğ¢Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
//! 2. ĞŸÑ€ĞµĞ´Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ°
//! 3. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğ¹
//! 4. ĞĞ°Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ‘Ğ°Ğ¹ĞµÑ
//!
//! Ğ—Ğ°Ğ¿ÑƒÑĞº:
//! ```bash
//! cargo run --example sentiment_analysis
//! ```

use rust_nlp_crypto::nlp::{BagOfWords, Preprocessor, TfIdf, Tokenizer, Vectorizer};
use rust_nlp_crypto::sentiment::{CryptoLexicon, NaiveBayesClassifier, SentimentAnalyzer, SentimentLexicon};

fn main() {
    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
    println!("   Crypto Sentiment Analysis Demo");
    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");

    // ĞŸÑ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°
    let sample_texts = vec![
        "Bitcoin is mooning! Very bullish on BTC right now ğŸš€",
        "Market is crashing, this looks like a scam. Selling everything.",
        "ETH showing stable growth, could be a good entry point",
        "Just got liquidated on my leveraged position... terrible day",
        "New partnership announced! This is huge for the ecosystem",
        "Not sure about this project, seems risky",
        "HODL! Diamond hands will be rewarded ğŸ’",
        "The market seems uncertain, waiting for clearer signals",
    ];

    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // 1. Ğ”ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ»ĞµĞºÑĞ¸ĞºĞ¾Ğ½Ğ°
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    println!("1ï¸âƒ£  CRYPTO LEXICON");
    println!("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n");

    let lexicon = CryptoLexicon::new();
    let stats = lexicon.stats();
    println!("Lexicon statistics:");
    println!("  â€¢ Positive words: {}", stats.positive_count);
    println!("  â€¢ Negative words: {}", stats.negative_count);
    println!("  â€¢ Modifiers: {}", stats.modifier_count);
    println!("  â€¢ Negations: {}", stats.negation_count);

    println!("\nSample word scores:");
    let test_words = ["moon", "crash", "bullish", "scam", "hodl", "dip"];
    for word in test_words {
        if let Some(score) = lexicon.get_score(word) {
            let bar = create_score_bar(score);
            println!("  {:10} {:+.2} {}", word, score, bar);
        }
    }

    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // 2. Ğ¢Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    println!("\n2ï¸âƒ£  TOKENIZATION");
    println!("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n");

    let tokenizer = Tokenizer::new();
    let sample = "Bitcoin $BTC is up 10%! Check @CryptoNews #bullish https://example.com";

    println!("Input: \"{}\"\n", sample);
    println!("Tokens:");

    let tokens = tokenizer.tokenize(sample);
    for token in &tokens {
        println!(
            "  {:20} -> {:15} [{:?}]",
            token.original, token.normalized, token.token_type
        );
    }

    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // 3. ĞŸÑ€ĞµĞ´Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ°
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    println!("\n3ï¸âƒ£  PREPROCESSING");
    println!("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n");

    let preprocessor = Preprocessor::new().with_crypto_stopwords();

    let raw_tokens: Vec<String> = vec![
        "the", "bitcoin", "is", "running", "very", "bullish", "today", "crypto",
    ]
    .iter()
    .map(|s| s.to_string())
    .collect();

    println!("Before: {:?}", raw_tokens);
    let processed = preprocessor.process(&raw_tokens);
    println!("After:  {:?}", processed);

    println!("\nStemming examples:");
    let stem_examples = ["running", "bullish", "trading", "mooning", "crashed"];
    for word in stem_examples {
        println!("  {} -> {}", word, preprocessor.stem(word));
    }

    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // 4. Bag of Words & TF-IDF
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    println!("\n4ï¸âƒ£  VECTORIZATION (Bag of Words & TF-IDF)");
    println!("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n");

    // ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹
    let docs: Vec<Vec<String>> = sample_texts
        .iter()
        .map(|text| {
            let tokens = tokenizer.tokenize_to_strings(text);
            preprocessor.process(&tokens)
        })
        .collect();

    // Bag of Words
    let mut bow = BagOfWords::new().with_min_df(1);
    let bow_matrix = bow.fit_transform(&docs);

    println!("Bag of Words:");
    println!("  Documents: {}", bow_matrix.n_documents());
    println!("  Vocabulary size: {}", bow_matrix.n_terms());
    println!("  Sample terms: {:?}", &bow_matrix.terms[..bow_matrix.terms.len().min(10)]);

    // TF-IDF
    let mut tfidf = TfIdf::new().with_min_df(1);
    let tfidf_matrix = tfidf.fit_transform(&docs);

    println!("\nTF-IDF:");
    println!("  Top terms by importance:");
    let top_terms = tfidf.top_terms(10);
    for (term, idf) in top_terms {
        println!("    â€¢ {:15} (IDF: {:.3})", term, idf);
    }

    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // 5. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğ¹
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    println!("\n5ï¸âƒ£  SENTIMENT ANALYSIS");
    println!("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n");

    let analyzer = SentimentAnalyzer::new();

    println!("Analyzing sample texts:\n");

    let mut results = Vec::new();
    for text in &sample_texts {
        let result = analyzer.analyze(text);
        results.push(result.clone());

        let emoji = match result.polarity {
            rust_nlp_crypto::models::Polarity::Positive => "ğŸ˜Š",
            rust_nlp_crypto::models::Polarity::Negative => "ğŸ˜Ÿ",
            rust_nlp_crypto::models::Polarity::Neutral => "ğŸ˜",
        };

        let bar = create_score_bar(result.score);

        println!("{} {:+.2} {} \"{}\"", emoji, result.score, bar, truncate(text, 50));

        if !result.key_words.is_empty() {
            let keywords: Vec<String> = result
                .key_words
                .iter()
                .take(3)
                .map(|w| format!("{}({:+.1})", w.word, w.score))
                .collect();
            println!("         Keywords: {}", keywords.join(", "));
        }
        println!();
    }

    // ĞĞ³Ñ€ĞµĞ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹
    println!("ğŸ“Š Aggregated Results:");
    println!("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€");
    let aggregated = analyzer.aggregate(&results);
    println!("{}", aggregated);

    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // 6. ĞĞ°Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ‘Ğ°Ğ¹ĞµÑ
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    println!("\n6ï¸âƒ£  NAIVE BAYES CLASSIFIER");
    println!("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n");

    // Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
    let train_docs = vec![
        vec!["moon", "bullish", "pump", "gains"],
        vec!["great", "excellent", "profit", "winning"],
        vec!["crash", "dump", "scam", "rekt"],
        vec!["terrible", "horrible", "lost", "liquidated"],
        vec!["okay", "stable", "normal", "waiting"],
        vec!["uncertain", "maybe", "possibly", "neutral"],
    ]
    .iter()
    .map(|words| words.iter().map(|s| s.to_string()).collect())
    .collect::<Vec<Vec<String>>>();

    let train_labels = vec![
        "positive", "positive", "negative", "negative", "neutral", "neutral",
    ];

    // ĞĞ±ÑƒÑ‡Ğ°ĞµĞ¼ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€
    let mut classifier = NaiveBayesClassifier::new();
    classifier.fit(&train_docs, &train_labels.iter().map(|s| s.to_string()).collect::<Vec<_>>());

    println!("Classifier trained!");
    println!("  Classes: {:?}", classifier.classes());
    println!("  Vocabulary size: {}", classifier.vocab_size());

    // Ğ¢ĞµÑÑ‚Ğ¸Ñ€ÑƒĞµĞ¼
    println!("\nClassifying new texts:");

    let test_samples = vec![
        vec!["bullish", "moon", "gains"],
        vec!["crash", "dump", "terrible"],
        vec!["stable", "waiting", "normal"],
    ];

    for sample in &test_samples {
        let prediction = classifier.predict(sample);
        let probs = classifier.predict_proba(sample);

        println!("\n  Input: {:?}", sample);
        println!("  Prediction: {:?}", prediction);
        println!("  Probabilities:");
        for (class, prob) in &probs {
            let bar_len = (prob * 20.0) as usize;
            let bar = "â–ˆ".repeat(bar_len);
            println!("    {:10} {:.1}% {}", class, prob * 100.0, bar);
        }
    }

    println!("\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
    println!("   Demo Complete! ğŸ‰");
    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");
}

/// Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ğ¾Ğ»Ğ¾ÑĞºÑƒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸
fn create_score_bar(score: f64) -> String {
    let normalized = ((score + 1.0) / 2.0 * 20.0) as usize;
    let bar: String = (0..20)
        .map(|i| {
            if i < normalized {
                if score > 0.0 {
                    'â–ˆ'
                } else {
                    'â–“'
                }
            } else {
                'â–‘'
            }
        })
        .collect();

    format!("[{}]", bar)
}

/// ĞĞ±Ñ€ĞµĞ·Ğ°Ñ‚ÑŒ Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ¾ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹
fn truncate(s: &str, max_len: usize) -> String {
    if s.len() <= max_len {
        s.to_string()
    } else {
        format!("{}...", &s[..max_len - 3])
    }
}
