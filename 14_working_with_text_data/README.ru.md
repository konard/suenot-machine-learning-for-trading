# Текстовые данные для трейдинга: анализ настроений

Это первая из трёх глав, посвящённых извлечению торговых сигналов из текстовых данных с использованием обработки естественного языка (NLP) и машинного обучения.

Текстовые данные очень богаты по содержанию, но крайне неструктурированны, поэтому требуют более тщательной предобработки, чтобы алгоритм машинного обучения мог извлечь релевантную информацию. Ключевая задача состоит в преобразовании текста в числовой формат без потери смысла. Мы рассмотрим несколько техник, способных улавливать нюансы языка, чтобы их можно было использовать в качестве входных данных для алгоритмов машинного обучения.

В этой главе мы познакомимся с базовыми техниками извлечения признаков, которые фокусируются на отдельных семантических единицах, т.е. словах или коротких группах слов, называемых токенами. Мы покажем, как представить документы в виде векторов количества токенов, создав матрицу "документ-терм", а затем используем её как входные данные для классификации новостей и анализа настроений. Мы также познакомимся с алгоритмом наивного Байеса, который популярен для этих задач.

В следующих двух главах мы будем развивать эти техники и использовать алгоритмы машинного обучения, такие как тематическое моделирование и векторные представления слов (word embeddings), чтобы улавливать информацию, содержащуюся в более широком контексте.

## Содержание

1. [ML с текстовыми данными — от языка к признакам](#ml-с-текстовыми-данными--от-языка-к-признакам)
    * [Вызовы обработки естественного языка](#вызовы-обработки-естественного-языка)
    * [Примеры использования](#примеры-использования)
    * [Рабочий процесс NLP](#рабочий-процесс-nlp)
2. [От текста к токенам — конвейер NLP](#от-текста-к-токенам--конвейер-nlp)
    * [Пример кода: NLP-конвейер с spaCy и textacy](#пример-кода-nlp-конвейер-с-spacy-и-textacy)
        - [Данные](#данные)
    * [Пример кода: NLP с TextBlob](#пример-кода-nlp-с-textblob)
3. [Подсчёт токенов — матрица "документ-терм"](#подсчёт-токенов--матрица-документ-терм)
    * [Пример кода: матрица "документ-терм" с scikit-learn](#пример-кода-матрица-документ-терм-с-scikit-learn)
4. [NLP для трейдинга: классификация текста и анализ настроений](#nlp-для-трейдинга-классификация-текста-и-анализ-настроений)
    * [Наивный байесовский классификатор](#наивный-байесовский-классификатор)
    * [Пример кода: классификация новостных статей](#пример-кода-классификация-новостных-статей)
    * [Примеры кода: анализ настроений](#примеры-кода-анализ-настроений)
        - [Бинарная классификация: данные Twitter](#бинарная-классификация-данные-twitter)
        - [Сравнение различных алгоритмов ML на больших мультиклассовых данных Yelp](#сравнение-различных-алгоритмов-ml-на-больших-мультиклассовых-данных-yelp)

## ML с текстовыми данными — от языка к признакам

Текстовые данные могут быть чрезвычайно ценными, учитывая, сколько информации люди передают и хранят с помощью естественного языка. Разнообразный набор источников данных, релевантных для инвестирования, включает: от формальных документов (отчёты компаний, контракты, патенты) до новостей, мнений и аналитических исследований, а также различных типов публикаций и сообщений в социальных сетях.

Полезные ресурсы:

- [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf), Daniel Jurafsky & James H. Martin, 3-е издание, черновик, 2018
- [Statistical natural language processing and corpus-based computational linguistics](https://nlp.stanford.edu/links/statnlp.html), Аннотированный список ресурсов, Стэнфордский университет
- [NLP Data Sources](https://github.com/niderhoff/nlp-datasets)

### Вызовы обработки естественного языка

Преобразование неструктурированного текста в машиночитаемый формат требует тщательной предобработки для сохранения ценных семантических аспектов данных. То, как люди извлекают смысл и понимают содержание языка, до конца не изучено, и улучшение понимания языка машинами остаётся областью активных исследований.

NLP сложен, потому что эффективное использование текстовых данных для машинного обучения требует понимания внутренних механизмов языка, а также знаний о мире, к которому он относится. Ключевые проблемы включают:
- неоднозначность из-за полисемии, т.е. слово или фраза могут иметь разные значения в зависимости от контекста ('Выпускники местной школы сократились вдвое')
- нестандартное и развивающееся использование языка, особенно в социальных сетях
- идиомы: 'бросить полотенце' (сдаться)
- названия сущностей могут быть сложными: 'Где показывают "Приключения Флика"?'
- необходимость знаний о мире: 'Мария и Анна — сёстры' vs 'Мария и Анна — матери'

### Примеры использования

| Применение | Описание | Примеры |
|---|---|---|
| Чат-боты | Понимание естественного языка пользователя и генерация интеллектуальных ответов | [Api.ai](https://api.ai/) |
| Информационный поиск | Поиск релевантных и похожих результатов | [Google](https://www.google.com/) |
| Извлечение информации | Структурированная информация из неструктурированных документов | [События из Gmail](https://support.google.com/calendar/answer/6084018?hl=ru) |
| Машинный перевод | Перевод с одного языка на другой | [Google Переводчик](https://translate.google.com/) |
| Упрощение текста | Сохранение смысла текста при упрощении грамматики и словаря | [Rewordify](https://rewordify.com/), [Простая английская Википедия](https://simple.wikipedia.org/wiki/Main_Page) |
| Предиктивный ввод текста | Более быстрый или удобный набор текста | [Автодополнение фраз](https://justmarkham.shinyapps.io/textprediction/) |
| Анализ настроений | Определение отношения говорящего | [Hater News](https://medium.com/@KevinMcAlear/building-hater-news-62062c58325c) |
| Автоматическое резюмирование | Экстрактивное или абстрактивное резюмирование | [Алгоритм autotldr на Reddit](https://smmry.com/about) |
| Генерация естественного языка | Генерация текста из данных | [Как компьютер описывает спортивный матч](http://www.bbc.com/news/technology-34204052) |
| Распознавание и синтез речи | Речь в текст, текст в речь | [Google Web Speech API демо](https://www.google.com/intl/en/chrome/demos/speech.html) |
| Ответы на вопросы | Определение намерения вопроса, сопоставление запроса с базой знаний, оценка гипотез | [Как Watson победил чемпиона Jeopardy Кена Дженнингса?](http://blog.ted.com/how-did-supercomputer-watson-beat-jeopardy-champion-ken-jennings-experts-discuss/) |

### Рабочий процесс NLP

Ключевая цель использования машинного обучения на текстовых данных для алгоритмической торговли — извлечение сигналов из документов. Документ — это отдельный образец из релевантного источника текстовых данных, например отчёт компании, заголовок или новостная статья, или твит. Корпус, в свою очередь, — это коллекция документов.

Следующая схема показывает ключевые шаги преобразования документов в датасет, который можно использовать для обучения алгоритма машинного обучения с учителем, способного делать практически полезные прогнозы.

<p align="center">
<img src="https://i.imgur.com/LPxpc8D.png" width="90%">
</p>

## От текста к токенам — конвейер NLP

Следующая таблица обобщает ключевые задачи NLP-конвейера:

| Функция | Описание |
|-----------------------------|-------------------------------------------------------------------|
| Токенизация | Разбиение текста на слова, знаки препинания и т.д. |
| Разметка частей речи | Присвоение типов слов токенам, таких как глагол или существительное |
| Синтаксический анализ зависимостей | Разметка синтаксических зависимостей токенов, например подлежащее <=> дополнение |
| Стемминг и лемматизация | Приведение к базовым формам слов: "был" => "быть", "крысы" => "крыса" |
| Определение границ предложений | Поиск и сегментация отдельных предложений |
| Распознавание именованных сущностей | Разметка объектов "реального мира", таких как люди, компании или места |
| Сходство | Оценка сходства слов, текстовых фрагментов и документов |

### Пример кода: NLP-конвейер с spaCy и textacy

Ноутбук [nlp_pipeline_with_spaCy](01_nlp_pipeline_with_spaCy.ipynb) демонстрирует, как построить NLP-конвейер с использованием открытой библиотеки Python [spaCy](https://spacy.io/). Библиотека [textacy](https://chartbeat-labs.github.io/textacy/index.html) построена на spaCy и обеспечивает лёгкий доступ к атрибутам spaCy и дополнительной функциональности.

- spaCy [документация](https://spacy.io/) и [инструкции по установке](https://spacy.io/usage/#installation)
- textacy опирается на `spaCy` для решения дополнительных задач NLP — см. [документацию](https://chartbeat-labs.github.io/textacy/index.html)

#### Данные
- [BBC Articles](http://mlg.ucd.ie/datasets/bbc.html), используйте сырые текстовые файлы
- [TED2013](http://opus.nlpl.eu/TED2013.php), параллельный корпус субтитров TED-выступлений на 15 языках

### Пример кода: NLP с TextBlob

Библиотека `TextBlob` предоставляет упрощённый интерфейс для распространённых задач NLP, включая разметку частей речи, извлечение именных групп, анализ настроений, классификацию, перевод и другие.

Ноутбук [nlp_with_textblob](02_nlp_with_textblob.ipynb) иллюстрирует её функциональность.

- [Документация](https://textblob.readthedocs.io/en/dev/)
- [Анализ настроений](https://github.com/sloria/TextBlob/blob/dev/textblob/en/en-sentiment.xml)

Хорошей альтернативой является NLTK — ведущая платформа для создания программ на Python для работы с данными естественного языка. Она предоставляет удобные интерфейсы к более чем 50 корпусам и лексическим ресурсам, таким как WordNet, а также набор библиотек для обработки текста: классификации, токенизации, стемминга, разметки, парсинга и семантического анализа.

- Natural Language ToolKit (NLTK) [Документация](http://www.nltk.org/)

## Подсчёт токенов — матрица "документ-терм"

В этом разделе представлена модель "мешка слов" (bag-of-words), которая преобразует текстовые данные в числовое векторное пространство, позволяющее сравнивать документы по их расстоянию. Мы демонстрируем, как создать матрицу "документ-терм" с использованием библиотеки sklearn.

- [TF-IDF — о том, что важно](https://planspace.org/20150524-tfidf_is_about_what_matters/)

### Пример кода: матрица "документ-терм" с scikit-learn

Модуль предобработки scikit-learn предлагает два инструмента для создания матрицы "документ-терм":
1. [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) использует бинарные или абсолютные счётчики для измерения частоты терма tf(d, t) для каждого документа d и токена t.
2. [TfIDFVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html), напротив, взвешивает (абсолютную) частоту терма обратной частотой документа (idf). В результате терм, который встречается в большем количестве документов, получит меньший вес, чем токен с той же частотой для данного документа, но меньшей частотой по всем документам.

Ноутбук [document_term_matrix](03_document_term_matrix.ipynb) демонстрирует использование и настройку.

## NLP для трейдинга: классификация текста и анализ настроений

После преобразования текстовых данных в числовые признаки с помощью техник обработки естественного языка, рассмотренных в предыдущих разделах, классификация текста работает так же, как любая другая задача классификации.

В этом разделе мы применим эти техники предобработки к новостным статьям, отзывам о товарах и данным Twitter, и научим различные классификаторы предсказывать дискретные категории новостей, оценки отзывов и полярность настроений.

Сначала мы познакомимся с моделью наивного Байеса — вероятностным алгоритмом классификации, который хорошо работает с текстовыми признаками, полученными моделью "мешка слов".

- [Daily Market News Sentiment and Stock Prices](https://www.econstor.eu/handle/10419/125094), David E. Allen & Michael McAleer & Abhay K. Singh, 2015
- [Predicting Economic Indicators from Web Text Using Sentiment Composition](http://www.ijcce.org/index.php?m=content&c=index&a=show&catid=39&id=358), Abby Levenberg, et al, 2014
- [JP Morgan NLP research results](https://www.jpmorgan.com/global/research/machine-learning)

### Наивный байесовский классификатор

Алгоритм наивного Байеса очень популярен для классификации текста, потому что низкие вычислительные затраты и требования к памяти позволяют обучаться на очень больших, многомерных наборах данных. Его предсказательная способность может конкурировать с более сложными моделями, он обеспечивает хорошую базовую линию и наиболее известен успешным обнаружением спама.

Модель опирается на теорему Байеса и предположение о том, что различные признаки независимы друг от друга при заданном классе результата. Другими словами, для данного результата знание значения одного признака (например, наличие токена в документе) не даёт никакой информации о значении другого признака.

### Пример кода: классификация новостных статей

Мы начнём с иллюстрации модели наивного Байеса для классификации 2225 новостных статей BBC, которые, как мы знаем, относятся к пяти различным категориям.

Ноутбук [text_classification](04_news_text_classification.ipynb) содержит соответствующие примеры.

### Примеры кода: анализ настроений

Анализ настроений — одно из самых популярных применений обработки естественного языка и машинного обучения для трейдинга, потому что позитивные или негативные перспективы в отношении активов или других драйверов цены, вероятно, влияют на доходность.

Как правило, подходы к моделированию анализа настроений опираются на словари, как в библиотеке TextBlob, или на модели, обученные на результатах для конкретной предметной области. Последнее предпочтительнее, поскольку позволяет более целенаправленную разметку, например связывая текстовые признаки с последующими изменениями цены, а не с косвенными оценками настроений.

См. директорию [data](../data) для инструкций по получению данных.

#### Бинарная классификация: данные Twitter

Мы иллюстрируем машинное обучение для анализа настроений, используя [датасет Twitter](http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip) с бинарными метками полярности и большой датасет бизнес-отзывов Yelp с пятибалльной шкалой.

Ноутбук [sentiment_analysis_twitter](05_sentiment_analysis_twitter.ipynb) содержит соответствующий пример.

- [Cheng-Caverlee-Lee September 2009 - January 2010 Twitter Scrape](https://archive.org/details/twitter_cikm_2010)

#### Сравнение различных алгоритмов ML на больших мультиклассовых данных Yelp

Для демонстрации обработки и классификации текста в большом масштабе мы также используем [датасет Yelp](https://www.yelp.com/dataset).

Ноутбук [sentiment_analysis_yelp](06_sentiment_analysis_yelp.ipynb) содержит соответствующий пример.

- [Yelp Dataset Challenge](https://www.yelp.com/dataset/challenge)
