# Векторные представления слов (Word Embeddings) для анализа финансовых отчётов и документов SEC

Эта глава посвящена использованию нейронных сетей для обучения векторного представления отдельных семантических единиц — слов или абзацев. Такие векторы являются плотными (dense), в отличие от разреженных (sparse) векторов модели мешка слов (bag-of-words), и содержат несколько сотен действительных чисел вместо десятков тысяч бинарных или дискретных значений. Их называют эмбеддингами (embeddings), поскольку они задают каждой семантической единице позицию в непрерывном векторном пространстве.

Эмбеддинги получаются в результате обучения модели связывать токены с их контекстом, при этом схожее использование слов приводит к схожим векторам. В результате эмбеддинги кодируют семантические аспекты, такие как отношения между словами, через их относительное расположение в пространстве. Они являются мощными признаками для моделей глубокого обучения, которые мы рассмотрим в последующих главах.

## Содержание

1. [Как эмбеддинги слов кодируют семантику](#как-эмбеддинги-слов-кодируют-семантику)
    * [Как нейронные языковые модели изучают контекст использования](#как-нейронные-языковые-модели-изучают-контекст-использования)
    * [Модель word2vec: масштабируемые эмбеддинги слов и фраз](#модель-word2vec-масштабируемые-эмбеддинги-слов-и-фраз)
    * [Оценка эмбеддингов: векторная арифметика и аналогии](#оценка-эмбеддингов-векторная-арифметика-и-аналогии)
2. [Пример кода: работа с моделями эмбеддингов](#пример-кода-работа-с-моделями-эмбеддингов)
    * [Работа с GloVe (Global Vectors for Word Representation)](#работа-с-glove-global-vectors-for-word-representation)
    * [Оценка эмбеддингов с помощью аналогий](#оценка-эмбеддингов-с-помощью-аналогий)
3. [Пример кода: обучение доменно-специфичных эмбеддингов на финансовых новостях](#пример-кода-обучение-доменно-специфичных-эмбеддингов-на-финансовых-новостях)
    * [Предобработка финансовых новостей: определение предложений и n-граммы](#предобработка-финансовых-новостей-определение-предложений-и-n-граммы)
    * [Архитектура Skip-gram в TensorFlow 2 и визуализация в TensorBoard](#архитектура-skip-gram-в-tensorflow-2-и-визуализация-в-tensorboard)
    * [Как обучать эмбеддинги быстрее с Gensim](#как-обучать-эмбеддинги-быстрее-с-gensim)
4. [Пример кода: векторы слов из документов SEC с использованием gensim](#пример-кода-векторы-слов-из-документов-sec-с-использованием-gensim)
    * [Предобработка: выбор контента, определение предложений и n-граммы](#предобработка-выбор-контента-определение-предложений-и-n-граммы)
    * [Обучение и оценка модели](#обучение-и-оценка-модели)
5. [Пример кода: анализ тональности с Doc2Vec](#пример-кода-анализ-тональности-с-doc2vec)
6. [Новые горизонты: Attention, Трансформеры и предобучение](#новые-горизонты-attention-трансформеры-и-предобучение)
    * [Attention — это всё, что вам нужно: трансформация генерации естественного языка](#attention--это-всё-что-вам-нужно-трансформация-генерации-естественного-языка)
    * [BERT: к более универсальной предобученной языковой модели](#bert-к-более-универсальной-предобученной-языковой-модели)
    * [Использование предобученных современных моделей](#использование-предобученных-современных-моделей)
7. [Дополнительные ресурсы](#дополнительные-ресурсы)

## Как эмбеддинги слов кодируют семантику

Эмбеддинги слов представляют токены как векторы меньшей размерности, при этом их относительное расположение отражает взаимосвязь с точки зрения контекста использования. Они воплощают дистрибутивную гипотезу из лингвистики, которая утверждает, что слова лучше всего определяются через их окружение.

Векторы слов способны улавливать множество семантических аспектов; не только синонимы находятся близко друг к другу, но слова могут иметь несколько степеней сходства. Например, слово "водитель" может быть похоже как на "шофёр", так и на "фактор". Более того, эмбеддинги отражают отношения между парами слов, такие как аналогии (Токио относится к Японии так же, как Париж к Франции, или "пошёл" относится к "идти" так же, как "увидел" к "видеть").

### Как нейронные языковые модели изучают контекст использования

Эмбеддинги слов получаются в результате обучения неглубокой нейронной сети предсказывать слово по его контексту. В то время как традиционные языковые модели определяют контекст как слова, предшествующие целевому слову, модели эмбеддингов используют слова в симметричном окне вокруг целевого слова.

В отличие от этого, модель мешка слов использует весь документ как контекст и применяет (взвешенные) подсчёты для фиксации совместной встречаемости слов вместо предсказательных векторов.

### Модель word2vec: масштабируемые эмбеддинги слов и фраз

Модель word2vec — это двухслойная нейронная сеть, которая принимает текстовый корпус на вход и выдаёт набор векторов эмбеддингов для слов этого корпуса. Существуют две различные архитектуры для эффективного обучения векторов слов с использованием неглубоких нейронных сетей:

- Модель **непрерывного мешка слов (CBOW)** предсказывает целевое слово, используя усреднённый вектор контекстных слов на входе, так что их порядок не имеет значения. CBOW обучается быстрее и обычно немного точнее для частотных терминов, но уделяет меньше внимания редким словам.
- Модель **skip-gram (SG)**, напротив, использует целевое слово для предсказания слов из контекста. Она хорошо работает с небольшими наборами данных и находит хорошие представления даже для редких слов или фраз.

### Оценка эмбеддингов: векторная арифметика и аналогии

Измерения векторов слов и фраз не имеют явного смысла. Однако эмбеддинги кодируют схожее использование как близость в латентном пространстве таким образом, что это переносится на семантические отношения. В результате появляется интересное свойство: аналогии можно выражать через сложение и вычитание векторов слов.

Так же как слова могут использоваться в разных контекстах, они могут быть связаны с другими словами разными способами, и эти отношения соответствуют разным направлениям в латентном пространстве. Соответственно, существует несколько типов аналогий, которые эмбеддинги должны отражать, если обучающие данные это позволяют.

Авторы word2vec предоставляют список из нескольких тысяч отношений, охватывающих аспекты географии, грамматики и синтаксиса, а также семейных отношений для оценки качества векторов эмбеддингов (см. директорию [analogies](data/analogies)).

## Пример кода: работа с моделями эмбеддингов

Подобно другим методам обучения без учителя, цель обучения векторов эмбеддингов — создание признаков для других задач, таких как классификация текста или анализ тональности.

Существует несколько способов получить векторы эмбеддингов для заданного корпуса документов:
- Использовать эмбеддинги, обученные на большом общем корпусе, таком как Wikipedia или Google News
- Обучить собственную модель на документах, отражающих интересующую предметную область

### Работа с GloVe (Global Vectors for Word Representation)

GloVe — это алгоритм обучения без учителя, разработанный в лаборатории NLP Стэнфордского университета, который обучает векторные представления слов на основе агрегированной статистики совместной встречаемости слов (см. ссылки). Доступны предобученные векторы на следующих веб-масштабных источниках:
- Common Crawl с 42B или 840B токенов и словарём из 1.9M или 2.2M токенов
- Wikipedia 2014 + Gigaword 5 с 6B токенов и словарём из 400K токенов
- Twitter с использованием 2B твитов, 27B токенов и словарём из 1.2M токенов

Следующая таблица показывает точность на семантическом тесте word2vec, достигнутую векторами GloVE, обученными на Wikipedia:

| Категория                | Примеры | Точность | Категория             | Примеры | Точность |
|--------------------------|---------|----------|-----------------------|---------|----------|
| столицы-страны           | 506     | 94.86%   | сравнительная степень | 1332    | 88.21%   |
| столицы-мир              | 8372    | 96.46%   | превосходная степень  | 1056    | 74.62%   |
| город-в-штате            | 4242    | 60.00%   | причастие наст. вр.   | 1056    | 69.98%   |
| валюта                   | 752     | 17.42%   | национальность-прил.  | 1640    | 92.50%   |
| семья                    | 506     | 88.14%   | прошедшее время       | 1560    | 61.15%   |
| прилаг.-в-наречие        | 992     | 22.58%   | множ. число           | 1332    | 78.08%   |
| антонимы                 | 756     | 28.57%   | множ. число глаголов  | 870     | 58.51%   |

Существует несколько источников предобученных эмбеддингов слов. Популярные варианты включают GloVE от Стэнфорда и встроенные векторы spaCy.
- Ноутбук [using_trained_vectors](01_using_trained_vectors.ipynb) демонстрирует работу с предобученными векторами.

### Оценка эмбеддингов с помощью аналогий

Ноутбук [evaluating_embeddings](02_evaluating_embeddings.ipynb) показывает, как тестировать качество векторов слов с помощью аналогий и других семантических отношений между словами.

## Пример кода: обучение доменно-специфичных эмбеддингов на финансовых новостях

Многие задачи требуют эмбеддингов доменно-специфичной лексики, которую модели, предобученные на общем корпусе, могут не уловить. Стандартные модели word2vec не способны назначать векторы словам вне словаря и вместо этого используют вектор по умолчанию, что снижает их предсказательную ценность.

Например, при работе с отраслевыми документами лексика или её использование могут меняться со временем по мере появления новых технологий или продуктов. В результате эмбеддинги тоже должны развиваться. Кроме того, документы, такие как корпоративные отчёты о прибылях, используют нюансированный язык, который векторы GloVe, предобученные на статьях Wikipedia, вряд ли смогут правильно отразить.

См. директорию [data](../data) для инструкций по получению набора данных финансовых новостей.

### Предобработка финансовых новостей: определение предложений и n-граммы

Ноутбук [financial_news_preprocessing](03_financial_news_preprocessing.ipynb) демонстрирует подготовку исходных данных для нашей модели.

### Архитектура Skip-gram в TensorFlow 2 и визуализация в TensorBoard

Ноутбук [financial_news_word2vec_tensorflow](04_financial_news_word2vec_tensorflow.ipynb) показывает, как построить модель word2vec с использованием интерфейса Keras в TensorFlow 2, который мы подробнее рассмотрим в следующей главе.

### Как обучать эмбеддинги быстрее с Gensim

Реализация на TensorFlow очень прозрачна с точки зрения архитектуры, но не особенно быстра. Библиотека обработки естественного языка (NLP) [gensim](https://radimrehurek.com/gensim/), которую мы также использовали для тематического моделирования в прошлой главе, предлагает лучшую производительность и более близко напоминает оригинальную реализацию word2vec на C, предоставленную авторами.

Ноутбук [financial_news_word2vec_gensim](05_financial_news_word2vec_gensim.ipynb) показывает, как обучать векторы слов более эффективно.

## Пример кода: векторы слов из документов SEC с использованием gensim

В этом разделе мы обучим векторы слов и фраз на годовых отчётах SEC с использованием gensim, чтобы проиллюстрировать потенциальную ценность эмбеддингов слов для алгоритмической торговли. В следующих разделах мы объединим эти векторы как признаки с доходностью цен для обучения нейронных сетей предсказывать цены акций на основе содержания финансовых документов.

В частности, мы используем набор данных, содержащий более 22 000 годовых отчётов 10-K за период 2013-2016 годов, поданных публичными компаниями и содержащих как финансовую информацию, так и комментарии руководства (см. Главу 3 об [Альтернативных данных](../03_alternative_data)). Примерно для половины из 11K отчётов компаний у нас есть цены акций для разметки данных для предсказательного моделирования (см. ссылки на источники данных и ноутбуки в папке [sec-filings](sec-filings) для подробностей).

- [2013-2016 Cleaned/Parsed 10-K Filings with the SEC](https://data.world/jumpyaf/2013-2016-cleaned-parsed-10-k-filings-with-the-sec)
- [Stock Market Predictions with Natural Language Deep Learning](https://www.microsoft.com/developerblog/2017/12/04/predicting-stock-performance-deep-learning/)

### Предобработка: выбор контента, определение предложений и n-граммы

Ноутбук [sec_preprocessing](06_sec_preprocessing.ipynb) показывает, как парсить и токенизировать текст с использованием spaCy, аналогично подходу в Главе 14, [Текстовые данные для торговли: Анализ тональности](../14_working_with_text_data).

### Обучение и оценка модели

Ноутбук [sec_word2vec](07_sec_word2vec.ipynb) использует реализацию [word2vec](https://radimrehurek.com/gensim/models/word2vec.html) архитектуры skip-gram в gensim для обучения векторов слов на наборе данных документов SEC.

## Пример кода: анализ тональности с Doc2Vec

Классификация текста требует объединения нескольких эмбеддингов слов. Распространённый подход — усреднение векторов эмбеддингов для каждого слова в документе. Это использует информацию из всех эмбеддингов и фактически применяет векторное сложение для получения другой точки в пространстве эмбеддингов. Однако при этом теряется важная информация о порядке слов.

Напротив, современный подход к генерации эмбеддингов для фрагментов текста, таких как абзац или отзыв о продукте, — использование модели эмбеддингов документов doc2vec. Эта модель была разработана авторами word2vec вскоре после публикации их оригинальной работы. Как и word2vec, doc2vec также имеет два варианта:
- Модель **распределённого мешка слов (DBOW)** соответствует модели CBOW в Word2Vec. Векторы документов получаются из обучения сети на синтетической задаче предсказания целевого слова на основе как векторов контекстных слов, так и вектора документа.
- Модель **распределённой памяти (DM)** соответствует архитектуре skipgram в word2vec. Векторы документов получаются из обучения нейросети предсказывать целевое слово, используя полный вектор документа.

Ноутбук [doc2vec_yelp_sentiment](08_doc2vec_yelp_sentiment.ipynb) применяет doc2vec к случайной выборке из 1 млн отзывов Yelp с их рейтингами.

## Новые горизонты: Attention, Трансформеры и предобучение

Эмбеддинги Word2vec и GloVe захватывают больше семантической информации, чем подход мешка слов, но позволяют получить только одно фиксированное представление для каждого токена, которое не различает контекстно-зависимое использование. Для решения нерешённых проблем, таких как множественные значения одного слова (полисемия), появилось несколько новых моделей, основанных на механизме внимания (attention), предназначенных для обучения более контекстуализированных эмбеддингов слов ([Vaswani et al. 2017](https://arxiv.org/abs/1706.03762)). Ключевые характеристики этих моделей:
- использование двунаправленных языковых моделей, которые обрабатывают текст как слева направо, так и справа налево для более богатого представления контекста, и
- использование полуконтролируемого предобучения на большом общем корпусе для изучения универсальных языковых аспектов в форме эмбеддингов и весов сети, которые можно использовать и дообучить для конкретных задач

### Attention — это всё, что вам нужно: трансформация генерации естественного языка

В 2018 году Google выпустила модель BERT, что означает Bidirectional Encoder Representations from Transformers ([Devlin et al. 2019](https://arxiv.org/abs/1810.04805)). Это стало крупным прорывом в исследованиях NLP — модель достигла революционных результатов в одиннадцати задачах понимания естественного языка, от ответов на вопросы и распознавания именованных сущностей до перефразирования и анализа тональности, измеренных по бенчмарку General Language Understanding Evaluation (GLUE) [benchmark](https://gluebenchmark.com/).

- [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)
- [Визуализация модели нейронного машинного перевода (Механика моделей Seq2seq с Attention)](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)

### BERT: к более универсальной предобученной языковой модели

Модель BERT основана на двух ключевых идеях: архитектуре трансформера, описанной в предыдущем разделе, и неконтролируемом предобучении, так что её не нужно обучать с нуля для каждой новой задачи; вместо этого её веса дообучаются (fine-tuning).
- BERT выводит механизм внимания на новый (более глубокий) уровень, используя 12 или 24 слоя в зависимости от архитектуры, каждый с 12 или 16 головами внимания, что приводит к 24 x 16 = 384 механизмам внимания для изучения контекстно-специфичных эмбеддингов.
- BERT использует неконтролируемое двунаправленное предобучение для изучения своих весов заранее на двух задачах: маскированное языковое моделирование (предсказание пропущенного слова по левому и правому контексту) и предсказание следующего предложения (предсказание, следует ли одно предложение за другим).

- [The Illustrated BERT, ELMo, and co. (Как NLP освоил Transfer Learning)](https://jalammar.github.io/illustrated-bert/)
- [Бенчмарк General Language Understanding Evaluation (GLUE)](https://gluebenchmark.com/leaderboard)
- [Финансовый NLP в S&P Global](https://www.youtube.com/watch?v=rdmaR4WRYEM&list=PLBmcuObd5An4UC6jvK_-eSl6jCvP1gwXc&index=9)

### Использование предобученных современных моделей

- [Huggingface Transformers](https://github.com/huggingface/transformers)
    - Transformers (ранее известный как pytorch-transformers и pytorch-pretrained-bert) предоставляет современные архитектуры общего назначения (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet, T5, CTRL...) для понимания естественного языка (NLU) и генерации естественного языка (NLG) с более чем тысячами предобученных моделей на 100+ языках и глубокой совместимостью между PyTorch и TensorFlow 2.0.
- [spacy-transformers](https://github.com/explosion/spacy-transformers)
    - Этот пакет (ранее spacy-pytorch-transformers) предоставляет пайплайны моделей spaCy, которые оборачивают пакет transformers от Hugging Face, так что вы можете использовать их в spaCy. Результат — удобный доступ к современным архитектурам трансформеров, таким как BERT, GPT-2, XLNet и т.д.
- [Allen NLP](https://allennlp.org/)
    - Глубокое обучение для NLP: AllenNLP упрощает проектирование и оценку новых моделей глубокого обучения практически для любой задачи NLP, а также инфраструктуру для их запуска в облаке или на вашем ноутбуке.
    - Современные модели: AllenNLP включает эталонные реализации высококачественных моделей как для базовых задач NLP (например, семантическая ролевая разметка), так и для приложений NLP (например, текстуальный вывод).
- [Sentence Transformers: Многоязычные эмбеддинги предложений с использованием BERT / RoBERTa / XLM-RoBERTa и др. с PyTorch]
    - BERT / RoBERTa / XLM-RoBERTa из коробки производят довольно плохие эмбеддинги предложений. Этот репозиторий дообучает BERT / RoBERTa / DistilBERT / ALBERT / XLNet с сиамской или триплетной сетевой структурой для создания семантически значимых эмбеддингов предложений, которые можно использовать в неконтролируемых сценариях: семантическое текстовое сходство через косинусное сходство, кластеризация, семантический поиск.

## Дополнительные ресурсы

- [GloVe: Global Vectors for Word Representation](https://github.com/stanfordnlp/GloVe)
- [Данные Common Crawl](http://commoncrawl.org/the-data/)
- [Примеры аналогий word2vec](https://github.com/nicholas-leonard/word2vec/blob/master/questions-words.txt)
- [Векторы слов и семантическое сходство в spaCy](https://spacy.io/usage/vectors-similarity)
- [2013-2016 Cleaned/Parsed 10-K Filings with the SEC](https://data.world/jumpyaf/2013-2016-cleaned-parsed-10-k-filings-with-the-sec)
- [Stanford Sentiment Tree Bank](https://nlp.stanford.edu/sentiment/treebank.html)
- [Эмбеддинги слов | TensorFlow Core](https://www.tensorflow.org/tutorials/text/word_embeddings)
- [Визуализация данных с помощью Embedding Projector в TensorBoard](https://www.tensorflow.org/tensorboard/tensorboard_projector_plugin)
